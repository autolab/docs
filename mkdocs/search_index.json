{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to the Autolab Docs\n\n\nAutolab is a course management and homework autograding platform that enables instructors to offer programming labs to their students. It includes gradebooks, rosters, handins/handouts, lab writeups, code annotation, manual grading, late penalties, grace days, cheat checking, meetings, partners, and bulk emails.\n\n\nCommands\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nProject layout\n\n\n1\n2\n3\n4\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-the-autolab-docs", 
            "text": "Autolab is a course management and homework autograding platform that enables instructors to offer programming labs to their students. It includes gradebooks, rosters, handins/handouts, lab writeups, code annotation, manual grading, late penalties, grace days, cheat checking, meetings, partners, and bulk emails.", 
            "title": "Welcome to the Autolab Docs"
        }, 
        {
            "location": "/#commands", 
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.", 
            "title": "Commands"
        }, 
        {
            "location": "/#project-layout", 
            "text": "1\n2\n3\n4 mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Project layout"
        }, 
        {
            "location": "/instructors/", 
            "text": "Instructor's Guide\n\n\nThis document provides instructors with a brief overview of the basic ideas and capabilities of the Autolab system. It's meant to be read from beginning to end the first time. \n\n\nUsers\n\n\nUsers\n are either \ninstructors\n, \ncourse assistants\n, or \nstudents\n. Instructors have full permissions. Course assistants are only allowed to enter grades. Students see only their own work. Each user is uniquely identified by their email address. You can change the permissions for a particular user at any time. Note that some instructors opt to give some or all of their TAs instructor status.\n\n\nRoster\n\n\nThe \nroster\n holds the list of users. You can add and remove users one at a time, or in bulk by uploading a CSV file in the general Autolab format:\n\n\nSemester,email,last_name,first_name,school,major,year,grading_policy,courseNumber,courseLecture,section\n\n\nor in the format that is exported by the CMU S3 service:\n\n\n\"Semester\",\"Course\",\"Section\",\"Lecture\",\"Mini\",\"Last Name\",\"First Name\",\"MI\",\"AndrewID\",\"Email\",\"College\",\"Department\",...\n\n\n\n\n\nAttention CMU Instructors:\n\n\nS3 lists each student twice: once in a lecture roster, which lists the lecture number (e.g., 1, 2,...) in the section field, and once in a section roster, which lists the section letter (e.g., A, B,...) in the section field. Be careful not to import the lecture roster. Instead, export and upload each section individually. Or you can export everything from S3 with a single action, edit out the roster entries for the lecture(s), and then upload a single file to Autolab with all of the sections.\n\n\n\n\nFor the bulk upload, you can choose to either: \n\n\n\n \nadd\n any new students in the roster file to the Autolab roster, or to \n\n\n \nupdate\n the Autolab roster by marking students missing from roster files as \ndropped\n.\n \n\n\n\nInstructors and course assistants are never marked as dropped. User accounts are never deleted. Students marked as dropped can still see their work, but cannot submit new work and do not appear on the instructor gradebook. Instructors can change the dropped status of a student at any time. \n\n\nOnce a student is added to the roster for a course, then that course becomes visible to the student when they visit the Autolab site. A student can be enrolled in an arbitrary number of Autolab courses.\n\n\nLabs (Assessments)\n\n\nA \nlab\n (or \nassessment\n) \nis broadly defined as a submission set; it is anything that\nyour students make submissions (handins) for. This could be a programming assignment, a\ntyped homework, or even an in-class exam. You can create labs from scratch, or reuse them from previous semesters.\nSee the companion [[Guide for Lab Authors]] for info on writing and installing labs. \n\n\nAssessment Categories\n\n\nYou can tag each assessment with an arbitrary user-defined \ncategory\n, e.g., \"Lab\", \"Exam\", \"Homework\".\n\n\nAutograders and Scoreboards\n\n\nLabs can be \nautograded\n or not, at your disrcretion. When a student submits to an autograded lab, Autolab runs an instructor-supplied \nautograder\n program that assigns scores to one or more problems associated with the lab. Autograded labs can have an optional \nscoreboard\n that shows (anonymized) results in real-time. See the companion [[Guide for Lab Authors]] for details on writing autograded labs with scoreboards.\n\n\nImportant Dates\n\n\nA lab has a \nstart date\n, \ndue date\n, \nend date\n and \ngrading deadline\n. The link to a lab becomes visible to students after the start date (it's always visible to instructors). Students can submit until the due date without penalty or consuming grace days. Submission is turned off after the end date. Grades are included in the gradebook's category and course averages only after the grading deadline.\n\n\nHandins\n\n\nOnce an assessment is live (past the start date), students can begin submitting handins, where each handin is a single file, which can be either a text file or an archive file (e.g., \nmm.c\n, \nhandin.tar\n).\n\n\nPenalties and Extensions\n\n\nYou can set penalties for late handins, set hard limits on the number of handins, or set soft limits that penalize excessive handins on a sliding scale. You can also give a student an \nextension\n that\nextends the due dates and end dates for that student.\n\n\nGrace Days\n\n\nAutolab provides support for a late handin policy based on \ngrace days\n. Each\nstudent has a semester-long budget of grace days that are automatically applied if they handin after the due date.\nEach late day consumes one of the budgeted grace days. The Autolab system keeps track of the number of grace days that have been used by each student to date. If students run out of grace days and handin late, then there\nis a fixed late penalty (possibly zero) that can be set by the instructor.\n\n\nProblems\n\n\nEach lab contains at least one \nproblem\n, defined by the instructor, with some point value. Each problem has a name (e.g., \"Prob1\", \"Style\") that is unique for the lab (although different labs can have the same problem names).\n\n\nSubmissions\n\n\nOnce an assessment is live (past the start date), students can begin making submissions (handins),  where each submission is a single file.\n\n\nGrades\n\n\nGrades\n come in a number of different forms:\n\n\n\nProblem scores:\n These are scalar values (possibly negative) assigned per problem per submission, either manually by a human grader after the end date, or automatically by an autograder after each submission. Problem scores can also be uploaded (imported) in bulk from a CSV file. \n\n\nAssessment raw score:\n By default, the raw score is the sum of the individual problem scores, before \nany penalties are applied. You can override the default raw score calculation. See below.\n\n\n\nAssessment total score:\n The total score is the raw score, plus any late penalties, plus any instructor \ntweaks\n.\n\n\n\nCategory averages:\n This is the average for a particular student over all\nassessments in a specific instructor-defined category such as \"Labs, or \"Exams\".\nBy default the category average is the arithmetic mean of all assessment total scores, but it can be overwridden.\nSee below.\n\n\n\nCourse Average:\n By default, the course average is average of all category averages, but can be overidden.\nSee below.\n\n\n\n\n\n\nSubmissions can be\nclassified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No\nGrade\" submission will show up in the gradebook as NG and a zero will be used\nwhen calculating averages. An \"Excused\" submission will show up in the\ngradebook as EXC and will not be used when calculating averages.\n\n\nOverriding Raw Score Calculations\n\n\nAutolab computes raw scores for a lab with a Ruby function called \nraw_score\n. The default is the sum of the individual problem scores. But you can change this by providing your own \nraw_score\n function in \nlabname\n.rb\n file. For example, to override the raw_score calculation for a lab called \nmalloclab\n, you might add the following \nraw_score\n function to \nmalloclab/malloclab.rb\n:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n  \n# In malloclab/malloclab.rb file\n\n  \ndef\n \nraw_score\n(\nscore\n)\n\n    \nperfindex\n \n=\n \nscore\n[\nAutograded Score\n].\nto_f\n()\n\n    \nheap\n \n=\n \nscore\n[\nHeap Checker\n].\nto_f\n()\n\n    \nstyle\n \n=\n \nscore\n[\nStyle\n].\nto_f\n()\n\n    \ndeduct\n \n=\n \nscore\n[\nCorrectnessDeductions\n].\nto_f\n()\n\n    \nperfpoints\n \n=\n \nperfindex\n\n\n    \n# perfindex below 50 gets autograded score of 0. \n\n    \nif\n \nperfindex\n \n \n50\n.\n0\n \nthen\n\n      \nperfpoints\n \n=\n \n0\n\n    \nelse\n\n      \nperfpoints\n \n=\n \nperfindex\n\n    \nend\n\n\n    \nreturn\n \nperfpoints\n \n+\n \nheap\n \n+\n \nstyle\n \n+\n \ndeduct\n\n  \nend\n\n\n\n\n\n\n\nThis particular lab has four problems called \"Autograded Score\",  \"Heap Checker\", \"Style\", and \"CorrectnessDeductions\". An \"Autograded Score\" less than 50 is set to zero when the raw score is calculated. \n\n\nNote: To make this change live, you must select the \"Reload config file\" option on the \nmalloclab\n page.\n\n\nOverriding Category and Course Averages\n\n\nThe average for a category \nfoo\n is calculated by a default Ruby function called \nfooAverage\n, which you can override in the \ncourse.rb\n file. For example, in our course, we prefer to report the \"average\" as the total number of normalized points (out of 100) that the student has accrued so far. This helps them understand where they stand in the class, e.g., \"Going into the final exam (worth 30 normalized points), I have 60 normalized points, so the only way to get an A is to get 100% on the final.\" Here's the Ruby function for category \"Lab\":\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n# In course.rb file\n\n\ndef\n \nLabAverage\n(\nuser\n)\n\n    \npts\n \n=\n \n(\nuser\n[\ndatalab\n].\nto_f\n()\n \n/\n \n63\n.\n0\n)\n \n*\n \n6\n.\n0\n \n+\n\n      \n(\nuser\n[\nbomblab\n].\nto_f\n()\n \n/\n \n70\n.\n0\n)\n \n*\n \n5\n.\n0\n \n+\n \n      \n(\nuser\n[\nattacklab\n].\nto_f\n()\n \n/\n \n100\n.\n0\n)\n \n*\n \n4\n.\n0\n \n+\n\n      \n(\nuser\n[\ncachelab\n].\nto_f\n()\n \n/\n \n60\n.\n0\n)\n \n*\n \n7\n.\n0\n \n+\n\n      \n(\nuser\n[\ntshlab\n].\nto_f\n()\n \n/\n \n110\n.\n0\n)\n \n*\n \n8\n.\n0\n \n+\n\n      \n(\nuser\n[\nmalloclab\n].\nto_f\n()\n \n/\n \n120\n.\n0\n)\n \n*\n \n12\n.\n0\n \n+\n\n      \n(\nuser\n[\nproxylab\n].\nto_f\n()\n \n/\n \n100\n.\n0\n)\n \n*\n \n8\n.\n0\n \n    \nreturn\n \npts\n.\nto_f\n()\n.\nround\n(\n2\n)\n\n\nend\n\n\n\n\n\n\n\nIn this case, labs are worth a total of 50/100 normalized points. The assessment called \ndatalab\n is graded out of a total of 63 points and is worth 6/50 normalized points.\n\n\nHere is the Ruby function for category \"Exam\":\n\n\n1\n2\n3\n4\n5\n6\n# In course.rb file\n\n\ndef\n \nExamAverage\n(\nuser\n)\n \n    \npts\n \n=\n \n((\nuser\n[\nmidterm\n].\nto_f\n()\n/\n60\n.\n0\n)\n \n*\n \n20\n.\n0\n)\n \n+\n\n          \n((\nuser\n[\nfinal\n].\nto_f\n()\n/\n80\n.\n0\n)\n*\n \n30\n.\n0\n)\n \n    \nreturn\n \npts\n.\nto_f\n()\n.\nround\n(\n2\n)\n \n\nend\n\n\n\n\n\n\n\nIn this case, exams are worth 50/100 normalized points. The assessment called \nmidterm\n is graded out of total of 60 points and is worth 20/50 normalized points.\n\n\nThe course average is computed by a default Ruby function called \ncourseAverage\n, which can be overridden by the \ncourse.rb\n file in the course directory. Here is the function for our running example:\n\n\n1\n2\n3\n4\n5\n# In course.rb file\n\n\ndef\n \ncourseAverage\n(\nuser\n)\n\n    \npts\n \n=\n \nuser\n[\ncatLab\n].\nto_f\n()\n \n+\n \nuser\n[\ncatExam\n].\nto_f\n()\n\n    \nreturn\n \npts\n.\nto_f\n()\n.\nround\n(\n2\n)\n\n\nend\n\n\n\n\n\n\n\nIn this course, the course average is the sum of the category averages for \"Lab\" and \"Exam\".\n\n\nNote: To make these changes live, you must select \"Reload course config file\" on the \"Manage course\" page.\n\n\nHandin History\n\n\nFor each lab, students can view all of their submissions, including any source code, and the problem scores, penalties, and total scores associated with those submissions, via the \nhandin history\n page.\n\n\nGradesheet\n\n\nThe \ngradesheet\n (not to be confused with the \ngradebook\n) is the workhorse grading tool. Each assessment has a separate gradesheet with the following features:\n\n\n\nProvides an interface for manually entering problem scores (and problem feedback) for the most recent submmission from each student. \n\n\nProvides an interface for viewing and annotating the submitted code.\n\n\nDisplays the problem scores for the most recent submission for each student, summarizes any late penalties, and computes the total score.\n\n\nProvides a link to each student's handin history.\n\n\n\n\nGradebook\n\n\nThe \ngradebook\n comes in two forms. The \nstudent gradebook\n displays the \ngrades for a particular student, including total scores for each assessment, category averages, and the course average. The \ninstructor gradebook\n is a table that displays the grades for the most recent submission of each student, including assessment total scores, category averages and course average. \n\n\nFor the gradebook calculations, submissions are \nclassified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No\nGrade\" submission will show up in the gradebook as NG and a zero will be used\nwhen calculating averages. An \"Excused\" submission will show up in the\ngradebook as EXC and will not be used when calculating averages.\n\n\nReleasing Grades\n\n\nManually assigned grades are by default not released, and therefore not visible to\nstudents. You can release grades on an individual basis while grading, or\nrelease all available grades in bulk by using the \"Release all grades\" option. You can also reverse this\nprocess using the \"Withdraw all grades\" option. (The word \"withdraw\" is perhaps unfortunate. No grades are ever deleted. They are simply withdrawn from the student's view.)", 
            "title": "Guide for Instructors"
        }, 
        {
            "location": "/instructors/#instructors-guide", 
            "text": "This document provides instructors with a brief overview of the basic ideas and capabilities of the Autolab system. It's meant to be read from beginning to end the first time.", 
            "title": "Instructor's Guide"
        }, 
        {
            "location": "/instructors/#users", 
            "text": "Users  are either  instructors ,  course assistants , or  students . Instructors have full permissions. Course assistants are only allowed to enter grades. Students see only their own work. Each user is uniquely identified by their email address. You can change the permissions for a particular user at any time. Note that some instructors opt to give some or all of their TAs instructor status.", 
            "title": "Users"
        }, 
        {
            "location": "/instructors/#roster", 
            "text": "The  roster  holds the list of users. You can add and remove users one at a time, or in bulk by uploading a CSV file in the general Autolab format: \nSemester,email,last_name,first_name,school,major,year,grading_policy,courseNumber,courseLecture,section \nor in the format that is exported by the CMU S3 service: \n\"Semester\",\"Course\",\"Section\",\"Lecture\",\"Mini\",\"Last Name\",\"First Name\",\"MI\",\"AndrewID\",\"Email\",\"College\",\"Department\",...   Attention CMU Instructors:  S3 lists each student twice: once in a lecture roster, which lists the lecture number (e.g., 1, 2,...) in the section field, and once in a section roster, which lists the section letter (e.g., A, B,...) in the section field. Be careful not to import the lecture roster. Instead, export and upload each section individually. Or you can export everything from S3 with a single action, edit out the roster entries for the lecture(s), and then upload a single file to Autolab with all of the sections.   For the bulk upload, you can choose to either:     add  any new students in the roster file to the Autolab roster, or to     update  the Autolab roster by marking students missing from roster files as  dropped .    Instructors and course assistants are never marked as dropped. User accounts are never deleted. Students marked as dropped can still see their work, but cannot submit new work and do not appear on the instructor gradebook. Instructors can change the dropped status of a student at any time.   Once a student is added to the roster for a course, then that course becomes visible to the student when they visit the Autolab site. A student can be enrolled in an arbitrary number of Autolab courses.", 
            "title": "Roster"
        }, 
        {
            "location": "/instructors/#labs-assessments", 
            "text": "A  lab  (or  assessment ) \nis broadly defined as a submission set; it is anything that\nyour students make submissions (handins) for. This could be a programming assignment, a\ntyped homework, or even an in-class exam. You can create labs from scratch, or reuse them from previous semesters.\nSee the companion [[Guide for Lab Authors]] for info on writing and installing labs.", 
            "title": "Labs (Assessments)"
        }, 
        {
            "location": "/instructors/#assessment-categories", 
            "text": "You can tag each assessment with an arbitrary user-defined  category , e.g., \"Lab\", \"Exam\", \"Homework\".", 
            "title": "Assessment Categories"
        }, 
        {
            "location": "/instructors/#autograders-and-scoreboards", 
            "text": "Labs can be  autograded  or not, at your disrcretion. When a student submits to an autograded lab, Autolab runs an instructor-supplied  autograder  program that assigns scores to one or more problems associated with the lab. Autograded labs can have an optional  scoreboard  that shows (anonymized) results in real-time. See the companion [[Guide for Lab Authors]] for details on writing autograded labs with scoreboards.", 
            "title": "Autograders and Scoreboards"
        }, 
        {
            "location": "/instructors/#important-dates", 
            "text": "A lab has a  start date ,  due date ,  end date  and  grading deadline . The link to a lab becomes visible to students after the start date (it's always visible to instructors). Students can submit until the due date without penalty or consuming grace days. Submission is turned off after the end date. Grades are included in the gradebook's category and course averages only after the grading deadline.", 
            "title": "Important Dates"
        }, 
        {
            "location": "/instructors/#handins", 
            "text": "Once an assessment is live (past the start date), students can begin submitting handins, where each handin is a single file, which can be either a text file or an archive file (e.g.,  mm.c ,  handin.tar ).", 
            "title": "Handins"
        }, 
        {
            "location": "/instructors/#penalties-and-extensions", 
            "text": "You can set penalties for late handins, set hard limits on the number of handins, or set soft limits that penalize excessive handins on a sliding scale. You can also give a student an  extension  that\nextends the due dates and end dates for that student.", 
            "title": "Penalties and Extensions"
        }, 
        {
            "location": "/instructors/#grace-days", 
            "text": "Autolab provides support for a late handin policy based on  grace days . Each\nstudent has a semester-long budget of grace days that are automatically applied if they handin after the due date.\nEach late day consumes one of the budgeted grace days. The Autolab system keeps track of the number of grace days that have been used by each student to date. If students run out of grace days and handin late, then there\nis a fixed late penalty (possibly zero) that can be set by the instructor.", 
            "title": "Grace Days"
        }, 
        {
            "location": "/instructors/#problems", 
            "text": "Each lab contains at least one  problem , defined by the instructor, with some point value. Each problem has a name (e.g., \"Prob1\", \"Style\") that is unique for the lab (although different labs can have the same problem names).", 
            "title": "Problems"
        }, 
        {
            "location": "/instructors/#submissions", 
            "text": "Once an assessment is live (past the start date), students can begin making submissions (handins),  where each submission is a single file.", 
            "title": "Submissions"
        }, 
        {
            "location": "/instructors/#grades", 
            "text": "Grades  come in a number of different forms:  Problem scores:  These are scalar values (possibly negative) assigned per problem per submission, either manually by a human grader after the end date, or automatically by an autograder after each submission. Problem scores can also be uploaded (imported) in bulk from a CSV file.   Assessment raw score:  By default, the raw score is the sum of the individual problem scores, before \nany penalties are applied. You can override the default raw score calculation. See below.  Assessment total score:  The total score is the raw score, plus any late penalties, plus any instructor  tweaks .  Category averages:  This is the average for a particular student over all\nassessments in a specific instructor-defined category such as \"Labs, or \"Exams\".\nBy default the category average is the arithmetic mean of all assessment total scores, but it can be overwridden.\nSee below.  Course Average:  By default, the course average is average of all category averages, but can be overidden.\nSee below.   Submissions can be\nclassified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No\nGrade\" submission will show up in the gradebook as NG and a zero will be used\nwhen calculating averages. An \"Excused\" submission will show up in the\ngradebook as EXC and will not be used when calculating averages.", 
            "title": "Grades"
        }, 
        {
            "location": "/instructors/#overriding-raw-score-calculations", 
            "text": "Autolab computes raw scores for a lab with a Ruby function called  raw_score . The default is the sum of the individual problem scores. But you can change this by providing your own  raw_score  function in  labname .rb  file. For example, to override the raw_score calculation for a lab called  malloclab , you might add the following  raw_score  function to  malloclab/malloclab.rb :   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17    # In malloclab/malloclab.rb file \n   def   raw_score ( score ) \n     perfindex   =   score [ Autograded Score ]. to_f () \n     heap   =   score [ Heap Checker ]. to_f () \n     style   =   score [ Style ]. to_f () \n     deduct   =   score [ CorrectnessDeductions ]. to_f () \n     perfpoints   =   perfindex \n\n     # perfindex below 50 gets autograded score of 0.  \n     if   perfindex     50 . 0   then \n       perfpoints   =   0 \n     else \n       perfpoints   =   perfindex \n     end \n\n     return   perfpoints   +   heap   +   style   +   deduct \n   end    This particular lab has four problems called \"Autograded Score\",  \"Heap Checker\", \"Style\", and \"CorrectnessDeductions\". An \"Autograded Score\" less than 50 is set to zero when the raw score is calculated.   Note: To make this change live, you must select the \"Reload config file\" option on the  malloclab  page.", 
            "title": "Overriding Raw Score Calculations"
        }, 
        {
            "location": "/instructors/#overriding-category-and-course-averages", 
            "text": "The average for a category  foo  is calculated by a default Ruby function called  fooAverage , which you can override in the  course.rb  file. For example, in our course, we prefer to report the \"average\" as the total number of normalized points (out of 100) that the student has accrued so far. This helps them understand where they stand in the class, e.g., \"Going into the final exam (worth 30 normalized points), I have 60 normalized points, so the only way to get an A is to get 100% on the final.\" Here's the Ruby function for category \"Lab\":   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 # In course.rb file  def   LabAverage ( user ) \n     pts   =   ( user [ datalab ]. to_f ()   /   63 . 0 )   *   6 . 0   + \n       ( user [ bomblab ]. to_f ()   /   70 . 0 )   *   5 . 0   +  \n       ( user [ attacklab ]. to_f ()   /   100 . 0 )   *   4 . 0   + \n       ( user [ cachelab ]. to_f ()   /   60 . 0 )   *   7 . 0   + \n       ( user [ tshlab ]. to_f ()   /   110 . 0 )   *   8 . 0   + \n       ( user [ malloclab ]. to_f ()   /   120 . 0 )   *   12 . 0   + \n       ( user [ proxylab ]. to_f ()   /   100 . 0 )   *   8 . 0  \n     return   pts . to_f () . round ( 2 )  end    In this case, labs are worth a total of 50/100 normalized points. The assessment called  datalab  is graded out of a total of 63 points and is worth 6/50 normalized points.  Here is the Ruby function for category \"Exam\":  1\n2\n3\n4\n5\n6 # In course.rb file  def   ExamAverage ( user )  \n     pts   =   (( user [ midterm ]. to_f () / 60 . 0 )   *   20 . 0 )   + \n           (( user [ final ]. to_f () / 80 . 0 ) *   30 . 0 )  \n     return   pts . to_f () . round ( 2 )   end    In this case, exams are worth 50/100 normalized points. The assessment called  midterm  is graded out of total of 60 points and is worth 20/50 normalized points.  The course average is computed by a default Ruby function called  courseAverage , which can be overridden by the  course.rb  file in the course directory. Here is the function for our running example:  1\n2\n3\n4\n5 # In course.rb file  def   courseAverage ( user ) \n     pts   =   user [ catLab ]. to_f ()   +   user [ catExam ]. to_f () \n     return   pts . to_f () . round ( 2 )  end    In this course, the course average is the sum of the category averages for \"Lab\" and \"Exam\".  Note: To make these changes live, you must select \"Reload course config file\" on the \"Manage course\" page.", 
            "title": "Overriding Category and Course Averages"
        }, 
        {
            "location": "/instructors/#handin-history", 
            "text": "For each lab, students can view all of their submissions, including any source code, and the problem scores, penalties, and total scores associated with those submissions, via the  handin history  page.", 
            "title": "Handin History"
        }, 
        {
            "location": "/instructors/#gradesheet", 
            "text": "The  gradesheet  (not to be confused with the  gradebook ) is the workhorse grading tool. Each assessment has a separate gradesheet with the following features:  Provides an interface for manually entering problem scores (and problem feedback) for the most recent submmission from each student.   Provides an interface for viewing and annotating the submitted code.  Displays the problem scores for the most recent submission for each student, summarizes any late penalties, and computes the total score.  Provides a link to each student's handin history.", 
            "title": "Gradesheet"
        }, 
        {
            "location": "/instructors/#gradebook", 
            "text": "The  gradebook  comes in two forms. The  student gradebook  displays the \ngrades for a particular student, including total scores for each assessment, category averages, and the course average. The  instructor gradebook  is a table that displays the grades for the most recent submission of each student, including assessment total scores, category averages and course average.   For the gradebook calculations, submissions are \nclassified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No\nGrade\" submission will show up in the gradebook as NG and a zero will be used\nwhen calculating averages. An \"Excused\" submission will show up in the\ngradebook as EXC and will not be used when calculating averages.", 
            "title": "Gradebook"
        }, 
        {
            "location": "/instructors/#releasing-grades", 
            "text": "Manually assigned grades are by default not released, and therefore not visible to\nstudents. You can release grades on an individual basis while grading, or\nrelease all available grades in bulk by using the \"Release all grades\" option. You can also reverse this\nprocess using the \"Withdraw all grades\" option. (The word \"withdraw\" is perhaps unfortunate. No grades are ever deleted. They are simply withdrawn from the student's view.)", 
            "title": "Releasing Grades"
        }
    ]
}