{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to the Autolab Docs\n\n\nAutolab is a course management platform that enables instructors to offer autograded programming assignments to their students. The two key  ideas in Autolab are \nautograding\n that is, programs evaluating other programs, and \nscoreboards\n that display the latest autograded scores for each student. Autolab also provides gradebooks, rosters, handins/handouts, lab writeups, code annotation, manual grading, late penalties, grace days, cheat checking, meetings, partners, and bulk emails.\n\n\nFor information on how to use Autolab for your course see the \nGuide for Instructors\n. To learn how to write an autograded lab see the \nGuide for Lab Authors\n.\n\n\nGetting Started\n\n\nAutolab consists of two services: (1) the Ruby on Rails frontend, and (2) \nTango\n, the RESTful Python autograding server. Either service can run independently without the other. But in order to use all features of Autolab, we highly recommend installing both services.\n\n\nCurrently, we have support for installing Autolab on \nUbuntu 14.04+\n and \nMac OSX\n.\n\n\nUbuntu 14.04+\n\n\nThere are two ways to install Autolab on Ubuntu.\n\n\nOption 1\n \n\n\nThe recommended way is to use the \nOneClick option\n. This option uses Docker to provide a complete installation of the Autolab frontend and Tango, for either development or production. Because it provides things like integration with SSL certificates and mail services, this option is specially useful for installing on external services like Heroku, EC2, DigitalOcean, or other Ubuntu VPS providers.\n\n\nOption 2\n  \n\n\nAnother option is to install the frontend and Tango manually, without using Docker. This gives you more control over the installation, but is only appropriate for advanced users with knowledge of the Unix command line, Rails, and Ruby Gems. To install the Autolab frontend in developer mode, run the following script:\n\n\nAUTOLAB_SCRIPT=`mktemp` \n \\curl -sSL https://raw.githubusercontent.com/autolab/Autolab/master/bin/setup.sh \n $AUTOLAB_SCRIPT \n \\bash $AUTOLAB_SCRIPT\n\n\n\n\nWhen the script runs, you will be prompted for the \nsudo\n password and other confirmations. You can see the details of the script \nhere\n. Once finished, \ninstall Tango\n.\n\n\nMac OSX 10.11+\n\n\nFollow the step-by-step instructions below:\n\n\n\n\n\n\nInstall \nrbenv\n (use the Basic GitHub Checkout method)\n\n\n\n\n\n\nInstall \nruby-build\n as an rbenv plugin:\n\n\ngit clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build\n\n\n\n\nRestart your shell at this point in order to start using your newly installed rbenv\n\n\n\n\n\n\nClone the Autolab repo into home directory and enter it:\n\n\ncd\ngit clone https://github.com/autolab/Autolab.git \n cd Autolab\n\n\n\n\n\n\n\n\nInstall the correct version of ruby:\n\n\nrbenv install $(cat .ruby-version)\n\n\n\n\nAt this point, confirm that \nrbenv\n is working (you might need to restart your shell):\n\n\n$ which ruby\n~/.rbenv/shims/ruby\n\n$ which rake\n~/.rbenv/shims/rake\n\n\n\n\n\n\n\n\nInstall \nbundler\n:\n\n\ngem install bundler\nrbenv rehash\n\n\n\n\n\n\n\n\nInstall the required gems (run the following commands in the cloned Autolab repo):\n\n\ncd bin\nbundle install\n\n\n\n\nRefer to the \nFAQ\n for issues installing gems\n\n\n\n\n\n\nInstall one of two database options\n\n\n\n\nSQLite\n should \nonly\n be used in development\n\n\nMySQL\n can be used in development or production\n\n\n\n\n\n\n\n\nConfigure your database:\n\n\ncp config/database.yml.template config/database.yml\n\n\n\n\nEdit \ndatabase.yml\n with the correct credentials for your chosen database. Refer to the \nFAQ\n for any issues.\n\n\n\n\n\n\nConfigure school/organization specific information (new feature):\n\n\ncp config/school.yml.template config/school.yml\n\n\n\n\nEdit \nschool.yml\n with your school/organization specific names and emails\n\n\n\n\n\n\nConfigure the Devise Auth System with a unique key (run these commands exactly):\n\n\ncp config/initializers/devise.rb.template config/initializers/devise.rb\nsed -i \ns/\nYOUR-SECRET-KEY\n/`bundle exec rake secret`/g\n initializers/devise.rb\n\n\n\n\nFill in \nYOUR_WEBSITE\n in \nconfig/initializers/devise.rb\n file. To skip this step for now, fill with \nfoo.bar\n.\n\n\n\n\n\n\nCreate and initialize the database tables:\n\n\nbundle exec rake db:create\n\n\n\n\nbundle exec rake db:migrate\nDo not forget to use \nbundle exec\n in front of every rake/rails command.\n\n\n\n\n\n\nPopulate dummy data (development only):\n\n\nbundle exec rake autolab:populate\n\n\n\n\n\n\n\n\nStart the rails server:\n\n\nbundle exec rails s -p 3000\n\n\n\n\n\n\n\n\nGo to localhost:3000 and login with \nDeveloper Login\n:\n\n\nEmail: \nadmin@foo.bar\n.\n\n\n\n\n\n\n\n\nInstall \nTango\n, the backend autograding service.\n\n\n\n\n\n\nNow you are all set to start using Autolab! Visit the \nGuide for Instructors\n and \nGuide for Lab Authors\n pages for more info.\n\n\n\n\n\n\nFAQ\n\n\nThis is a general list of questions that we get often. If you find a solution to an issue not mentioned here,\nplease contact us at \n\n\nUbuntu Script Bugs\n\n\nIf you get the following error\n\n\nFailed to fetch http://dl.google.com/linux/chrome/deb/dists/stable/Release  \nUnable to find expected entry 'main/binary-i386/Packages' in Release file (Wrong sources.list entry or malformed file)\n\n\n\n\nthen follow the solution in \nthis post\n. \n\n\nWhere do I find the MySQL username and password?\n\n\nIf this is your first time logging into MySQL, your username is 'root'. You may also need to set the root password:\n\n\nStart the server:\n\n\nsudo /usr/local/mysql/support-files/mysql.server start\n\n\n\n\nSet the password:\n\n\nmysqladmin -u root password \n[New_Password]\n\n\n\n\nIf you lost your root password, refer to the \nMySQL wiki\n\n\nBundle Install Errors\n\n\nThis happens as gems get updated. These fixes are gem-specific, but two common ones are\n\n\neventmachine\n\n\nbundle config build.eventmachine --with-cppflags=-I/usr/local/opt/openssl/include\n\n\n\n\nlibv8\n\n\nbundle config build.libv8 --with-system-v8\n\n\n\n\nRun \nbundle install\n again\n\n\nIf this still does not work, try exploring \nthis StackOverflow link\n\n\nCan't connect to local MySQL server through socket\n\n\nMake sure you've started the MySQL server and double-check the socket in \nconfig/database.yml\n\n\nThe default socket location is \n/tmp/mysql.sock\n.\n\n\nI forgot my MySQL root password\n\n\nYou can reset it following the instructions on \nthis Stack Overflow post\n\n\nIf \nmysql\n complains that the password is expired, follow the instructions on the second answer on \nthis post", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#welcome-to-the-autolab-docs", 
            "text": "Autolab is a course management platform that enables instructors to offer autograded programming assignments to their students. The two key  ideas in Autolab are  autograding  that is, programs evaluating other programs, and  scoreboards  that display the latest autograded scores for each student. Autolab also provides gradebooks, rosters, handins/handouts, lab writeups, code annotation, manual grading, late penalties, grace days, cheat checking, meetings, partners, and bulk emails.  For information on how to use Autolab for your course see the  Guide for Instructors . To learn how to write an autograded lab see the  Guide for Lab Authors .", 
            "title": "Welcome to the Autolab Docs"
        }, 
        {
            "location": "/#getting-started", 
            "text": "Autolab consists of two services: (1) the Ruby on Rails frontend, and (2)  Tango , the RESTful Python autograding server. Either service can run independently without the other. But in order to use all features of Autolab, we highly recommend installing both services.  Currently, we have support for installing Autolab on  Ubuntu 14.04+  and  Mac OSX .", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#ubuntu-1404", 
            "text": "There are two ways to install Autolab on Ubuntu.  Option 1    The recommended way is to use the  OneClick option . This option uses Docker to provide a complete installation of the Autolab frontend and Tango, for either development or production. Because it provides things like integration with SSL certificates and mail services, this option is specially useful for installing on external services like Heroku, EC2, DigitalOcean, or other Ubuntu VPS providers.  Option 2     Another option is to install the frontend and Tango manually, without using Docker. This gives you more control over the installation, but is only appropriate for advanced users with knowledge of the Unix command line, Rails, and Ruby Gems. To install the Autolab frontend in developer mode, run the following script:  AUTOLAB_SCRIPT=`mktemp`   \\curl -sSL https://raw.githubusercontent.com/autolab/Autolab/master/bin/setup.sh   $AUTOLAB_SCRIPT   \\bash $AUTOLAB_SCRIPT  When the script runs, you will be prompted for the  sudo  password and other confirmations. You can see the details of the script  here . Once finished,  install Tango .", 
            "title": "Ubuntu 14.04+"
        }, 
        {
            "location": "/#mac-osx-1011", 
            "text": "Follow the step-by-step instructions below:    Install  rbenv  (use the Basic GitHub Checkout method)    Install  ruby-build  as an rbenv plugin:  git clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build  Restart your shell at this point in order to start using your newly installed rbenv    Clone the Autolab repo into home directory and enter it:  cd\ngit clone https://github.com/autolab/Autolab.git   cd Autolab    Install the correct version of ruby:  rbenv install $(cat .ruby-version)  At this point, confirm that  rbenv  is working (you might need to restart your shell):  $ which ruby\n~/.rbenv/shims/ruby\n\n$ which rake\n~/.rbenv/shims/rake    Install  bundler :  gem install bundler\nrbenv rehash    Install the required gems (run the following commands in the cloned Autolab repo):  cd bin\nbundle install  Refer to the  FAQ  for issues installing gems    Install one of two database options   SQLite  should  only  be used in development  MySQL  can be used in development or production     Configure your database:  cp config/database.yml.template config/database.yml  Edit  database.yml  with the correct credentials for your chosen database. Refer to the  FAQ  for any issues.    Configure school/organization specific information (new feature):  cp config/school.yml.template config/school.yml  Edit  school.yml  with your school/organization specific names and emails    Configure the Devise Auth System with a unique key (run these commands exactly):  cp config/initializers/devise.rb.template config/initializers/devise.rb\nsed -i  s/ YOUR-SECRET-KEY /`bundle exec rake secret`/g  initializers/devise.rb  Fill in  YOUR_WEBSITE  in  config/initializers/devise.rb  file. To skip this step for now, fill with  foo.bar .    Create and initialize the database tables:  bundle exec rake db:create  bundle exec rake db:migrate\nDo not forget to use  bundle exec  in front of every rake/rails command.    Populate dummy data (development only):  bundle exec rake autolab:populate    Start the rails server:  bundle exec rails s -p 3000    Go to localhost:3000 and login with  Developer Login :  Email:  admin@foo.bar .    Install  Tango , the backend autograding service.    Now you are all set to start using Autolab! Visit the  Guide for Instructors  and  Guide for Lab Authors  pages for more info.", 
            "title": "Mac OSX 10.11+"
        }, 
        {
            "location": "/#faq", 
            "text": "This is a general list of questions that we get often. If you find a solution to an issue not mentioned here,\nplease contact us at", 
            "title": "FAQ"
        }, 
        {
            "location": "/#ubuntu-script-bugs", 
            "text": "If you get the following error  Failed to fetch http://dl.google.com/linux/chrome/deb/dists/stable/Release  \nUnable to find expected entry 'main/binary-i386/Packages' in Release file (Wrong sources.list entry or malformed file)  then follow the solution in  this post .", 
            "title": "Ubuntu Script Bugs"
        }, 
        {
            "location": "/#where-do-i-find-the-mysql-username-and-password", 
            "text": "If this is your first time logging into MySQL, your username is 'root'. You may also need to set the root password:  Start the server:  sudo /usr/local/mysql/support-files/mysql.server start  Set the password:  mysqladmin -u root password  [New_Password]  If you lost your root password, refer to the  MySQL wiki", 
            "title": "Where do I find the MySQL username and password?"
        }, 
        {
            "location": "/#bundle-install-errors", 
            "text": "This happens as gems get updated. These fixes are gem-specific, but two common ones are  eventmachine  bundle config build.eventmachine --with-cppflags=-I/usr/local/opt/openssl/include  libv8  bundle config build.libv8 --with-system-v8  Run  bundle install  again  If this still does not work, try exploring  this StackOverflow link", 
            "title": "Bundle Install Errors"
        }, 
        {
            "location": "/#cant-connect-to-local-mysql-server-through-socket", 
            "text": "Make sure you've started the MySQL server and double-check the socket in  config/database.yml  The default socket location is  /tmp/mysql.sock .", 
            "title": "Can't connect to local MySQL server through socket"
        }, 
        {
            "location": "/#i-forgot-my-mysql-root-password", 
            "text": "You can reset it following the instructions on  this Stack Overflow post  If  mysql  complains that the password is expired, follow the instructions on the second answer on  this post", 
            "title": "I forgot my MySQL root password"
        }, 
        {
            "location": "/instructors/", 
            "text": "Guide for Instructors\n\n\nThis document provides instructors with a brief overview of the basic ideas and capabilities of the Autolab system. It's meant to be read from beginning to end the first time. \n\n\nUsers\n\n\nUsers\n are either \ninstructors\n, \ncourse assistants\n, or \nstudents\n. Instructors have full permissions. Course assistants are only allowed to enter grades. Students see only their own work. Each user is uniquely identified by their email address. You can change the permissions for a particular user at any time. Note that some instructors opt to give some or all of their TAs instructor status.\n\n\nRoster\n\n\nThe \nroster\n holds the list of users. You can add and remove users one at a time, or in bulk by uploading a CSV file in the general Autolab format:\n\n\nSemester,email,last_name,first_name,school,major,year,grading_policy,courseNumber,courseLecture,section\n\n\nor in the format that is exported by the CMU S3 service:\n\n\n\"Semester\",\"Course\",\"Section\",\"Lecture\",\"Mini\",\"Last Name\",\"First Name\",\"MI\",\"AndrewID\",\"Email\",\"College\",\"Department\",...\n\n\n\n\n\nAttention CMU Instructors:\n\n\nS3 lists each student twice: once in a lecture roster, which lists the lecture number (e.g., 1, 2,...) in the section field, and once in a section roster, which lists the section letter (e.g., A, B,...) in the section field. Be careful not to import the lecture roster. Instead, export and upload each section individually. Or you can export everything from S3 with a single action, edit out the roster entries for the lecture(s), and then upload a single file to Autolab with all of the sections.\n\n\n\n\nFor the bulk upload, you can choose to either: \n\n\n\n \nadd\n any new students in the roster file to the Autolab roster, or to \n\n\n \nupdate\n the Autolab roster by marking students missing from roster files as \ndropped\n.\n \n\n\n\nInstructors and course assistants are never marked as dropped. User accounts are never deleted. Students marked as dropped can still see their work, but cannot submit new work and do not appear on the instructor gradebook. Instructors can change the dropped status of a student at any time. \n\n\nOnce a student is added to the roster for a course, then that course becomes visible to the student when they visit the Autolab site. A student can be enrolled in an arbitrary number of Autolab courses.\n\n\nLabs (Assessments)\n\n\nA \nlab\n (or \nassessment\n) \nis broadly defined as a submission set; it is anything that\nyour students make submissions (handins) for. This could be a programming assignment, a\ntyped homework, or even an in-class exam. You can create labs from scratch, or reuse them from previous semesters.\nSee the companion \nGuide For Lab Authors\n for info on writing and installing labs. \n\n\nAssessment Categories\n\n\nYou can tag each assessment with an arbitrary user-defined \ncategory\n, e.g., \"Lab\", \"Exam\", \"Homework\".\n\n\nAutograders and Scoreboards\n\n\nLabs can be \nautograded\n or not, at your disrcretion. When a student submits to an autograded lab, Autolab runs an instructor-supplied \nautograder\n program that assigns scores to one or more problems associated with the lab. Autograded labs can have an optional \nscoreboard\n that shows (anonymized) results in real-time. See the companion \nGuide For Lab Authors\n for details on writing autograded labs with scoreboards.\n\n\nImportant Dates\n\n\nA lab has a \nstart date\n, \ndue date\n, \nend date\n and \ngrading deadline\n. The link to a lab becomes visible to students after the start date (it's always visible to instructors). Students can submit until the due date without penalty or consuming grace days. Submission is turned off after the end date. Grades are included in the gradebook's category and course averages only after the grading deadline.\n\n\nHandins\n\n\nOnce an assessment is live (past the start date), students can begin submitting handins, where each handin is a single file, which can be either a text file or an archive file (e.g., \nmm.c\n, \nhandin.tar\n).\n\n\nPenalties and Extensions\n\n\nYou can set penalties for late handins, set hard limits on the number of handins, or set soft limits that penalize excessive handins on a sliding scale. You can also give a student an \nextension\n that\nextends the due dates and end dates for that student.\n\n\nGrace Days\n\n\nAutolab provides support for a late handin policy based on \ngrace days\n. Each\nstudent has a semester-long budget of grace days that are automatically applied if they handin after the due date.\nEach late day consumes one of the budgeted grace days. The Autolab system keeps track of the number of grace days that have been used by each student to date. If students run out of grace days and handin late, then there\nis a fixed late penalty (possibly zero) that can be set by the instructor.\n\n\nProblems\n\n\nEach lab contains at least one \nproblem\n, defined by the instructor, with some point value. Each problem has a name (e.g., \"Prob1\", \"Style\") that is unique for the lab (although different labs can have the same problem names).\n\n\nSubmissions\n\n\nOnce an assessment is live (past the start date), students can begin making submissions (handins),  where each submission is a single file.\n\n\nGrades\n\n\nGrades\n come in a number of different forms:\n\n\n\nProblem scores:\n These are scalar values (possibly negative) assigned per problem per submission, either manually by a human grader after the end date, or automatically by an autograder after each submission. Problem scores can also be uploaded (imported) in bulk from a CSV file. \n\n\nAssessment raw score:\n By default, the raw score is the sum of the individual problem scores, before \nany penalties are applied. You can override the default raw score calculation. See below.\n\n\n\nAssessment total score:\n The total score is the raw score, plus any late penalties, plus any instructor \ntweaks\n.\n\n\n\nCategory averages:\n This is the average for a particular student over all\nassessments in a specific instructor-defined category such as \"Labs, or \"Exams\".\nBy default the category average is the arithmetic mean of all assessment total scores, but it can be overwridden.\nSee below.\n\n\n\nCourse Average:\n By default, the course average is average of all category averages, but can be overidden.\nSee below.\n\n\n\n\n\n\nSubmissions can be\nclassified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No\nGrade\" submission will show up in the gradebook as NG and a zero will be used\nwhen calculating averages. An \"Excused\" submission will show up in the\ngradebook as EXC and will not be used when calculating averages.\n\n\nOverriding Raw Score Calculations\n\n\nAutolab computes raw scores for a lab with a Ruby function called \nraw_score\n. The default is the sum of the individual problem scores. But you can change this by providing your own \nraw_score\n function in \nlabname\n.rb\n file. For example, to override the raw_score calculation for a lab called \nmalloclab\n, you might add the following \nraw_score\n function to \nmalloclab/malloclab.rb\n:\n\n\n  # In malloclab/malloclab.rb file\n  def raw_score(score)\n    perfindex = score[\nAutograded Score\n].to_f()\n    heap = score[\nHeap Checker\n].to_f()\n    style = score[\nStyle\n].to_f()\n    deduct = score[\nCorrectnessDeductions\n].to_f()\n    perfpoints = perfindex\n\n    # perfindex below 50 gets autograded score of 0. \n    if perfindex \n 50.0 then\n      perfpoints = 0\n    else\n      perfpoints = perfindex\n    end\n\n    return perfpoints + heap + style + deduct\n  end\n\n\n\n\nThis particular lab has four problems called \"Autograded Score\",  \"Heap Checker\", \"Style\", and \"CorrectnessDeductions\". An \"Autograded Score\" less than 50 is set to zero when the raw score is calculated. \n\n\nNote: To make this change live, you must select the \"Reload config file\" option on the \nmalloclab\n page.\n\n\nOverriding Category and Course Averages\n\n\nThe average for a category \nfoo\n is calculated by a default Ruby function called \nfooAverage\n, which you can override in the \ncourse.rb\n file. For example, in our course, we prefer to report the \"average\" as the total number of normalized points (out of 100) that the student has accrued so far. This helps them understand where they stand in the class, e.g., \"Going into the final exam (worth 30 normalized points), I have 60 normalized points, so the only way to get an A is to get 100% on the final.\" Here's the Ruby function for category \"Lab\":\n\n\n# In course.rb file\ndef LabAverage(user)\n    pts = (user['datalab'].to_f() / 63.0) * 6.0 +\n      (user['bomblab'].to_f() / 70.0) * 5.0 + \n      (user['attacklab'].to_f() / 100.0) * 4.0 +\n      (user['cachelab'].to_f() / 60.0) * 7.0 +\n      (user['tshlab'].to_f() / 110.0) * 8.0 +\n      (user['malloclab'].to_f() / 120.0) * 12.0 +\n      (user['proxylab'].to_f() / 100.0) * 8.0 \n    return pts.to_f().round(2)\nend\n\n\n\n\nIn this case, labs are worth a total of 50/100 normalized points. The assessment called \ndatalab\n is graded out of a total of 63 points and is worth 6/50 normalized points.\n\n\nHere is the Ruby function for category \"Exam\":\n\n\n# In course.rb file\ndef ExamAverage(user) \n    pts = ((user['midterm'].to_f()/60.0) * 20.0) +\n          ((user['final'].to_f()/80.0)* 30.0) \n    return pts.to_f().round(2) \nend\n\n\n\n\nIn this case, exams are worth 50/100 normalized points. The assessment called \nmidterm\n is graded out of total of 60 points and is worth 20/50 normalized points.\n\n\nThe course average is computed by a default Ruby function called \ncourseAverage\n, which can be overridden by the \ncourse.rb\n file in the course directory. Here is the function for our running example:\n\n\n# In course.rb file\ndef courseAverage(user)\n    pts = user['catLab'].to_f() + user['catExam'].to_f()\n    return pts.to_f().round(2)\nend\n\n\n\n\nIn this course, the course average is the sum of the category averages for \"Lab\" and \"Exam\".\n\n\nNote: To make these changes live, you must select \"Reload course config file\" on the \"Manage course\" page.\n\n\nHandin History\n\n\nFor each lab, students can view all of their submissions, including any source code, and the problem scores, penalties, and total scores associated with those submissions, via the \nhandin history\n page.\n\n\nGradesheet\n\n\nThe \ngradesheet\n (not to be confused with the \ngradebook\n) is the workhorse grading tool. Each assessment has a separate gradesheet with the following features:\n\n\n\nProvides an interface for manually entering problem scores (and problem feedback) for the most recent submmission from each student. \n\n\nProvides an interface for viewing and annotating the submitted code.\n\n\nDisplays the problem scores for the most recent submission for each student, summarizes any late penalties, and computes the total score.\n\n\nProvides a link to each student's handin history.\n\n\n\n\nGradebook\n\n\nThe \ngradebook\n comes in two forms. The \nstudent gradebook\n displays the \ngrades for a particular student, including total scores for each assessment, category averages, and the course average. The \ninstructor gradebook\n is a table that displays the grades for the most recent submission of each student, including assessment total scores, category averages and course average. \n\n\nFor the gradebook calculations, submissions are \nclassified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No\nGrade\" submission will show up in the gradebook as NG and a zero will be used\nwhen calculating averages. An \"Excused\" submission will show up in the\ngradebook as EXC and will not be used when calculating averages.\n\n\nReleasing Grades\n\n\nManually assigned grades are by default not released, and therefore not visible to\nstudents. You can release grades on an individual basis while grading, or\nrelease all available grades in bulk by using the \"Release all grades\" option. You can also reverse this\nprocess using the \"Withdraw all grades\" option. (The word \"withdraw\" is perhaps unfortunate. No grades are ever deleted. They are simply withdrawn from the student's view.)", 
            "title": "Guide for Instructors"
        }, 
        {
            "location": "/instructors/#guide-for-instructors", 
            "text": "This document provides instructors with a brief overview of the basic ideas and capabilities of the Autolab system. It's meant to be read from beginning to end the first time.", 
            "title": "Guide for Instructors"
        }, 
        {
            "location": "/instructors/#users", 
            "text": "Users  are either  instructors ,  course assistants , or  students . Instructors have full permissions. Course assistants are only allowed to enter grades. Students see only their own work. Each user is uniquely identified by their email address. You can change the permissions for a particular user at any time. Note that some instructors opt to give some or all of their TAs instructor status.", 
            "title": "Users"
        }, 
        {
            "location": "/instructors/#roster", 
            "text": "The  roster  holds the list of users. You can add and remove users one at a time, or in bulk by uploading a CSV file in the general Autolab format: \nSemester,email,last_name,first_name,school,major,year,grading_policy,courseNumber,courseLecture,section \nor in the format that is exported by the CMU S3 service: \n\"Semester\",\"Course\",\"Section\",\"Lecture\",\"Mini\",\"Last Name\",\"First Name\",\"MI\",\"AndrewID\",\"Email\",\"College\",\"Department\",...   Attention CMU Instructors:  S3 lists each student twice: once in a lecture roster, which lists the lecture number (e.g., 1, 2,...) in the section field, and once in a section roster, which lists the section letter (e.g., A, B,...) in the section field. Be careful not to import the lecture roster. Instead, export and upload each section individually. Or you can export everything from S3 with a single action, edit out the roster entries for the lecture(s), and then upload a single file to Autolab with all of the sections.   For the bulk upload, you can choose to either:     add  any new students in the roster file to the Autolab roster, or to     update  the Autolab roster by marking students missing from roster files as  dropped .    Instructors and course assistants are never marked as dropped. User accounts are never deleted. Students marked as dropped can still see their work, but cannot submit new work and do not appear on the instructor gradebook. Instructors can change the dropped status of a student at any time.   Once a student is added to the roster for a course, then that course becomes visible to the student when they visit the Autolab site. A student can be enrolled in an arbitrary number of Autolab courses.", 
            "title": "Roster"
        }, 
        {
            "location": "/instructors/#labs-assessments", 
            "text": "A  lab  (or  assessment ) \nis broadly defined as a submission set; it is anything that\nyour students make submissions (handins) for. This could be a programming assignment, a\ntyped homework, or even an in-class exam. You can create labs from scratch, or reuse them from previous semesters.\nSee the companion  Guide For Lab Authors  for info on writing and installing labs.", 
            "title": "Labs (Assessments)"
        }, 
        {
            "location": "/instructors/#assessment-categories", 
            "text": "You can tag each assessment with an arbitrary user-defined  category , e.g., \"Lab\", \"Exam\", \"Homework\".", 
            "title": "Assessment Categories"
        }, 
        {
            "location": "/instructors/#autograders-and-scoreboards", 
            "text": "Labs can be  autograded  or not, at your disrcretion. When a student submits to an autograded lab, Autolab runs an instructor-supplied  autograder  program that assigns scores to one or more problems associated with the lab. Autograded labs can have an optional  scoreboard  that shows (anonymized) results in real-time. See the companion  Guide For Lab Authors  for details on writing autograded labs with scoreboards.", 
            "title": "Autograders and Scoreboards"
        }, 
        {
            "location": "/instructors/#important-dates", 
            "text": "A lab has a  start date ,  due date ,  end date  and  grading deadline . The link to a lab becomes visible to students after the start date (it's always visible to instructors). Students can submit until the due date without penalty or consuming grace days. Submission is turned off after the end date. Grades are included in the gradebook's category and course averages only after the grading deadline.", 
            "title": "Important Dates"
        }, 
        {
            "location": "/instructors/#handins", 
            "text": "Once an assessment is live (past the start date), students can begin submitting handins, where each handin is a single file, which can be either a text file or an archive file (e.g.,  mm.c ,  handin.tar ).", 
            "title": "Handins"
        }, 
        {
            "location": "/instructors/#penalties-and-extensions", 
            "text": "You can set penalties for late handins, set hard limits on the number of handins, or set soft limits that penalize excessive handins on a sliding scale. You can also give a student an  extension  that\nextends the due dates and end dates for that student.", 
            "title": "Penalties and Extensions"
        }, 
        {
            "location": "/instructors/#grace-days", 
            "text": "Autolab provides support for a late handin policy based on  grace days . Each\nstudent has a semester-long budget of grace days that are automatically applied if they handin after the due date.\nEach late day consumes one of the budgeted grace days. The Autolab system keeps track of the number of grace days that have been used by each student to date. If students run out of grace days and handin late, then there\nis a fixed late penalty (possibly zero) that can be set by the instructor.", 
            "title": "Grace Days"
        }, 
        {
            "location": "/instructors/#problems", 
            "text": "Each lab contains at least one  problem , defined by the instructor, with some point value. Each problem has a name (e.g., \"Prob1\", \"Style\") that is unique for the lab (although different labs can have the same problem names).", 
            "title": "Problems"
        }, 
        {
            "location": "/instructors/#submissions", 
            "text": "Once an assessment is live (past the start date), students can begin making submissions (handins),  where each submission is a single file.", 
            "title": "Submissions"
        }, 
        {
            "location": "/instructors/#grades", 
            "text": "Grades  come in a number of different forms:  Problem scores:  These are scalar values (possibly negative) assigned per problem per submission, either manually by a human grader after the end date, or automatically by an autograder after each submission. Problem scores can also be uploaded (imported) in bulk from a CSV file.   Assessment raw score:  By default, the raw score is the sum of the individual problem scores, before \nany penalties are applied. You can override the default raw score calculation. See below.  Assessment total score:  The total score is the raw score, plus any late penalties, plus any instructor  tweaks .  Category averages:  This is the average for a particular student over all\nassessments in a specific instructor-defined category such as \"Labs, or \"Exams\".\nBy default the category average is the arithmetic mean of all assessment total scores, but it can be overwridden.\nSee below.  Course Average:  By default, the course average is average of all category averages, but can be overidden.\nSee below.   Submissions can be\nclassified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No\nGrade\" submission will show up in the gradebook as NG and a zero will be used\nwhen calculating averages. An \"Excused\" submission will show up in the\ngradebook as EXC and will not be used when calculating averages.", 
            "title": "Grades"
        }, 
        {
            "location": "/instructors/#overriding-raw-score-calculations", 
            "text": "Autolab computes raw scores for a lab with a Ruby function called  raw_score . The default is the sum of the individual problem scores. But you can change this by providing your own  raw_score  function in  labname .rb  file. For example, to override the raw_score calculation for a lab called  malloclab , you might add the following  raw_score  function to  malloclab/malloclab.rb :    # In malloclab/malloclab.rb file\n  def raw_score(score)\n    perfindex = score[ Autograded Score ].to_f()\n    heap = score[ Heap Checker ].to_f()\n    style = score[ Style ].to_f()\n    deduct = score[ CorrectnessDeductions ].to_f()\n    perfpoints = perfindex\n\n    # perfindex below 50 gets autograded score of 0. \n    if perfindex   50.0 then\n      perfpoints = 0\n    else\n      perfpoints = perfindex\n    end\n\n    return perfpoints + heap + style + deduct\n  end  This particular lab has four problems called \"Autograded Score\",  \"Heap Checker\", \"Style\", and \"CorrectnessDeductions\". An \"Autograded Score\" less than 50 is set to zero when the raw score is calculated.   Note: To make this change live, you must select the \"Reload config file\" option on the  malloclab  page.", 
            "title": "Overriding Raw Score Calculations"
        }, 
        {
            "location": "/instructors/#overriding-category-and-course-averages", 
            "text": "The average for a category  foo  is calculated by a default Ruby function called  fooAverage , which you can override in the  course.rb  file. For example, in our course, we prefer to report the \"average\" as the total number of normalized points (out of 100) that the student has accrued so far. This helps them understand where they stand in the class, e.g., \"Going into the final exam (worth 30 normalized points), I have 60 normalized points, so the only way to get an A is to get 100% on the final.\" Here's the Ruby function for category \"Lab\":  # In course.rb file\ndef LabAverage(user)\n    pts = (user['datalab'].to_f() / 63.0) * 6.0 +\n      (user['bomblab'].to_f() / 70.0) * 5.0 + \n      (user['attacklab'].to_f() / 100.0) * 4.0 +\n      (user['cachelab'].to_f() / 60.0) * 7.0 +\n      (user['tshlab'].to_f() / 110.0) * 8.0 +\n      (user['malloclab'].to_f() / 120.0) * 12.0 +\n      (user['proxylab'].to_f() / 100.0) * 8.0 \n    return pts.to_f().round(2)\nend  In this case, labs are worth a total of 50/100 normalized points. The assessment called  datalab  is graded out of a total of 63 points and is worth 6/50 normalized points.  Here is the Ruby function for category \"Exam\":  # In course.rb file\ndef ExamAverage(user) \n    pts = ((user['midterm'].to_f()/60.0) * 20.0) +\n          ((user['final'].to_f()/80.0)* 30.0) \n    return pts.to_f().round(2) \nend  In this case, exams are worth 50/100 normalized points. The assessment called  midterm  is graded out of total of 60 points and is worth 20/50 normalized points.  The course average is computed by a default Ruby function called  courseAverage , which can be overridden by the  course.rb  file in the course directory. Here is the function for our running example:  # In course.rb file\ndef courseAverage(user)\n    pts = user['catLab'].to_f() + user['catExam'].to_f()\n    return pts.to_f().round(2)\nend  In this course, the course average is the sum of the category averages for \"Lab\" and \"Exam\".  Note: To make these changes live, you must select \"Reload course config file\" on the \"Manage course\" page.", 
            "title": "Overriding Category and Course Averages"
        }, 
        {
            "location": "/instructors/#handin-history", 
            "text": "For each lab, students can view all of their submissions, including any source code, and the problem scores, penalties, and total scores associated with those submissions, via the  handin history  page.", 
            "title": "Handin History"
        }, 
        {
            "location": "/instructors/#gradesheet", 
            "text": "The  gradesheet  (not to be confused with the  gradebook ) is the workhorse grading tool. Each assessment has a separate gradesheet with the following features:  Provides an interface for manually entering problem scores (and problem feedback) for the most recent submmission from each student.   Provides an interface for viewing and annotating the submitted code.  Displays the problem scores for the most recent submission for each student, summarizes any late penalties, and computes the total score.  Provides a link to each student's handin history.", 
            "title": "Gradesheet"
        }, 
        {
            "location": "/instructors/#gradebook", 
            "text": "The  gradebook  comes in two forms. The  student gradebook  displays the \ngrades for a particular student, including total scores for each assessment, category averages, and the course average. The  instructor gradebook  is a table that displays the grades for the most recent submission of each student, including assessment total scores, category averages and course average.   For the gradebook calculations, submissions are \nclassified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No\nGrade\" submission will show up in the gradebook as NG and a zero will be used\nwhen calculating averages. An \"Excused\" submission will show up in the\ngradebook as EXC and will not be used when calculating averages.", 
            "title": "Gradebook"
        }, 
        {
            "location": "/instructors/#releasing-grades", 
            "text": "Manually assigned grades are by default not released, and therefore not visible to\nstudents. You can release grades on an individual basis while grading, or\nrelease all available grades in bulk by using the \"Release all grades\" option. You can also reverse this\nprocess using the \"Withdraw all grades\" option. (The word \"withdraw\" is perhaps unfortunate. No grades are ever deleted. They are simply withdrawn from the student's view.)", 
            "title": "Releasing Grades"
        }, 
        {
            "location": "/lab/", 
            "text": "Guide for Lab Authors\n\n\nThis guide explains how to create autograded programming assignments (labs) for the Autolab system. \n\n\nWriting Autograders\n\n\nAn \nautograder\n is a program that takes a student's work as input, and generates some quantitative evaluation of that work as output. The student's work consists of one or more source files written in an arbitrary programming language. \nThe autograder processes these files and generates arbitrary text lines on stdout. The last text line on stdout must be a JSON string, called an \nautoresult\n, that assigns an autograded score to one or more problems, and optionally, generates the scoreboard entries for this submission.\n\n\nThe JSON autoresult is a \"scores\" hash that assigns a numerical score to one or more problems, and an optional \"scoreboard\" array that provides the scoreboard entries for this submission. For example, \n\n\n{\nscores\n: {\nProb1\n: 10, \nProb2\n: 5}}\n\n\n\n\nassigns 10 points to \"Prob1\" and 5 points to \"Prob2\" for this submission. The names of the problems must exactly match the names of the problems for this lab on the Autolab web site. Not all problems need to be autograded. For example, there might be a problem for this assessment called \"Style\" that you grade manually after the due date. \n\n\nIf you used the Autolab web site to configure a scoreboard for this lab with three columns called \"Prob1\", \"Prob2\", and \"Total\", then the autoresult might be: \n\n\n{\nscores\n: {\nProb1\n: 10, \nProb2\n: 5}, \nscoreboard\n: [10, 5, 15]}\n\n\n\n\nBy convention, an autograder accepts an optional \n-A\n command line argument that tells it to emit the JSON autoresult. So if you run the autograder outside of the context of Autolab, you can suppress the autoresult line by calling the autograder without the \n-A\n argument.\n\n\nOne of the nice properties of Autolab autograders is that they can be written and tested offline, without requiring any interaction with Autolab. Writing autograders is not easy, but the fact that they can be developed offline allows you to develop and test them in your own familiar computing environment. \n\n\nInstalling Autograded Labs\n\n\nAfter you've written and tested the autograder, you then use the Autolab web site to create the autograded lab. Autolab supports creating new labs from scratch, or reusing labs from previous semesters. We'll describe each of these in turn.\n\n\nCreating an Autograded Lab from Scratch\n\n\nStep 1: Create the new lab.\n\n\nCreate a new lab by clicking the \"Install Assessment\" button and choosing \"Option 1: Create a New Assessment from Scratch.\" For course \ncourse\n and lab \nlab\n,  this will create a \nlab directory\n in the Autolab file hierarchy called \ncourses/\ncourse\n/\nlab\n. This initial directory contains a couple of config files and a directory called \nlab\n/handin\n that will contain all of the student handin files. In general, you should never modify any of these. \n\n\n\n\nAttention CMU Lab Authors\n\n\nAt CMU, the lab directory is called \n/afs/cs/academic/class/\ncourse\n/autolab/\nlab\n. For example: \n/afs/cs/academic/class/15213-f16/autolab/foo\n is the lab directory for the lab named \nfoo\n for the Fall 2016 instance of 15-213. All lab-related files must go in this \nautolab\n directory to avoid permissions issues.\n\n\n\n\nStep 2: Configure the lab for autograding.\n\n\nUsing the \"Edit Assessment\" page, turn on autograding for this lab by selecting \"Add Autograder.\" You will be asked for the name of the image to be used for autograding this lab. The default image distributed with Autolab is an Ubuntu image called \nautograding_image\n. If your class needs different software, then you or your facilities staff will need to update the default image or create a new one. \n\n\n\n\nAttention CMU Lab Authors\n\n\nThe default autograding image at CMU is called \nrhel.img\n and is a copy of the software on the CMU Andrew machines (\nlinux.andrew.cmu.edu\n). If you need custom software installed, please send mail to autolab-help@andrew.cmu.edu.\n\n\n\n\nIf you want a scoreboard, you should select \"Add Scoreboard,\" which will allow you to specify the number of columns and their names. The \"Add Scoreboard\" page contains a tutorial on how to do this. \n\n\nYou'll also need to define the names and point values for all the problems in this lab, including the autograded ones. \n\n\nEach student submission is a single file, either a text source file or an archive file containing multiple files and directories. You'll need to specify the \nbase name\n for the student submission files (e.g., \nmm.c\n, \nhandin.tar\n).\n\n\nStep 3: Add the required autograding files.\n\n\nFor an autograded lab, Autolab expects the following two \nautograding files\n in the lab directory: \n\n\n\n\nautograde-Makefile\n: runs the autograder on a student submission.\n\n\nautograde.tar\n: contains all of the files (except for the student handin file) that are needed for autograding. \n\n\n\n\nEach time a student submits their work or an instructor requests a regrade, Autolab \n\n\n\n\ncopies the student handin file, along with the two autograding files, to an empty directory on an \nautograding instance\n, \n\n\nrenames the student handin file to \nbase name\n (e.g., hello.c, handin.tar), \n\n\nrenames \nautograde-Makefile\n to \nMakefile\n, \n\n\nexecutes the command \nmake\n on the autograding instance, and finally \n\n\ncaptures the stdout generated by the autograder, and parses the resulting JSON autoresult to determine the autograded scores. \n\n\n\n\nImporting an Autograded Lab from a Previous Semester\n\n\nIf you've created a lab for a course in a previous semester and have access to the lab directory (as we do at CMU via AFS), you can import the lab into your current course by \n\n\n\n\ncopying the lab directory from the previous course to the current course, \n\n\ncleaning out the \nhandin\n directory, then \n\n\nvisiting the \"Install Assessment\" page and selecting \"Option 2: Import an existing assessment from the file system.\" Autolab will give you a list of all of the directories that appear to be uninstalled labs, from which you can select your particular lab. \n\n\n\n\nIf you don't have access to the lab directory, another option is to import a lab from a tarball that was created by running \"Export assessment\" in an instance of a lab from a previous semester. Visit the \"Install Assessment\" page and select \"Option 3: Import an existing assessment from tarball.\" This will upload the tarball, create a new lab directory by expanding the tarball, and then import the directory.\n\n\nExample: Hello Lab\n\n\nIn this section we'll look at the simplest possible autograded lab we could imagine, called, appropriately enough, the\n\nHello Lab\n \n(with \ntarball\n), which is stored in a lab directory called \nhello\n in the Autolab github repo. While it's trivial, it illustrates all of the aspects of developing an autograded lab, and provides a simple example that you can use for sanity testing on your Autolab installation.\n\n\nIn this lab, students are asked to write a version of the K\nR \"hello, world\" program, called \nhello.c\n. The autograder simply checks that the submitted \nhello.c\n program compiles and runs with an exit status of zero. If so, the submission gets 100 points. Otherwise it gets 0 points. \n\n\nDirectory Structure\n\n\nAutolab expects to find the \nautograde-Makefile\nand \nautograde.tar\n files in the \nhello\n lab directory, but otherwise places no constraints on the contents and organization of this directory. However, based on our experience, we strongly recommend a directory structure with the following form:\n\n\nhello/README\n:\n\n\n# Basic files created by the lab author\nMakefile                Builds the lab from src/\nREADME                  \nautograde-Makefile      Makefile that runs the autograder \nsrc/                    Contains all src files and solutions         \ntest-autograder/        For testing autograder offline\nwriteup/                Lab writeup that students view from Autolab    \n\n# Files created by running make\nhello-handout/          The directory that is handed out to students, created\n                        using files from src/. \nhello-handout.tar       Archive of hello-handout directory\nautograde.tar           File that is copied to the autograding instance \n                        (along with autograde-Makefile and student handin file)\n\n# Files created and managed by Autolab\nhandin/    All students handin files\nhello.rb   Config file\nhello.yml  Database properties that persist from semester to semester\nlog.txt    Log of autograded submissions\n\n\n\n\nThe key idea with this directory structure is to place \nall\n code for the lab in the \nsrc\n directory, including the autograding code and any starter code handed out to students in the handout directory (\nhello-handout.tar\n in this example). Keeping all hard state in the \nsrc\n directory helps limit inconsistencies. \n\n\nThe main makefile creates \nhello-handout\n by copying files from \nsrc\n, and then tars it up:\n\n\nhello/Makefile\n:\n\n\n#\n# Makefile to manage the example Hello Lab\n#\n\n# Get the name of the lab directory\nLAB = $(notdir $(PWD))\n\nall: handout handout-tarfile\n\nhandout: \n    # Rebuild the handout directory that students download\n    (rm -rf $(LAB)-handout; mkdir $(LAB)-handout)\n    cp -p src/Makefile-handout $(LAB)-handout/Makefile\n    cp -p src/README-handout $(LAB)-handout/README\n    cp -p src/hello.c-handout $(LAB)-handout/hello.c \n    cp -p src/driver.sh $(LAB)-handout\n\nhandout-tarfile: handout\n    # Build *-handout.tar and autograde.tar\n    tar cvf $(LAB)-handout.tar $(LAB)-handout\n    cp -p $(LAB)-handout.tar autograde.tar\n\nclean:\n    # Clean the entire lab directory tree.  Note that you can run\n    # \nmake clean; make\n at any time while the lab is live with no\n    # adverse effects.\n    rm -f *~ *.tar\n    (cd src; make clean)\n    (cd test-autograder; make clean)\n    rm -rf $(LAB)-handout\n    rm -f autograde.tar\n#\n# CAREFULL!!! This will delete all student records in the logfile and\n# in the handin directory. Don't run this once the lab has started.\n# Use it to clean the directory when you are starting a new version\n# of the lab from scratch, or when you are debugging the lab prior\n# to releasing it to the students.\n#\ncleanallfiles:\n    # Reset the lab from scratch.\n    make clean\n    rm -f log.txt\n    rm -rf handin/*\n\n\n\n\nFilenames are disambiguated by appending \n-handout\n, which is stripped when they are copied to the handout directory. For example, \nsrc/hello.c\n is the instructor's solution file, and \nsrc/hello.c-handout\n is the starter code that is given to the students in \nhello-handout/hello.c\n. And \nsrc/README\n is the README for the src directory and \nsrc/README-handout\n is the README that is handed out to students in \nhello-handout/README\n.\n\n\nTo build the lab, type \nmake clean; make\n. You can do this as often as you like while the lab is live with no adverse effects. However, be careful to never type \nmake cleanallfiles\n while the lab is live; this should only be done before the lab goes live; never during or after.\n\n\nSource Directory\n\n\nThe \nhello/src/\n directory \ncontains \nall\n of the code files for the Hello Lab, including the files that are handed out to students:\n\n\nhello/src/README\n:\n\n\n# Autograder and solution files\nMakefile                Makefile and ...\nREADME                  ... README for this directory\ndriver.sh*              Autograder\nhello.c                 Solution hello.c file\n\n# Files that are handed out to students\nMakefile-handout        Makefile and ...\nREADME-handout          ... README handed out to students\nhello.c-handout         Blank hello.c file handed out to students\n\n\n\n\nHandout Directory\n\n\nThe \nhello/hello-handout/\n directory\ncontains the files that the students will use to work on the lab. It contains no hard state, and is populated entirely with files from \nhello/src\n:\n\n\nhello/hello-handout/README\n:\n\n\nFor this lab, you should write a tiny C program, called \nhello.c\n,\nthat prints \nhello, world\n to stdout and then indicates success by\nexiting with a status of zero.\n\nTo test your work: \n$ make clean; make; ./hello\n\nTo run the same autograder that Autolab will use when you submit:\n$ ./driver.sh\n\nFiles:\nREADME          This file\nMakefile        Compiles hello.c\ndriver.sh       Autolab autograder\nhello.c         Empty C file that you will edit\n\n\n\n\nhello/hello-handout/Makefile\n contains the rules that compile the student source code:\n\n\n# Student makefile for the Hello Lab\nall: \n    gcc hello.c -o hello\n\nclean:\n    rm -rf *~ hello\n\n\n\n\nTo compile and run their code, students type:\n\n\n$ make clean; make\n$ ./hello\n\n\n\n\nAutograder\n\n\nThe autograder for the Hello Lab is a trivially simple bash script called \ndriver.sh\n that compiles and runs \nhello.c\n and verifies that it returns with an exit status of zero:\n\n\nhello/src/driver.sh\n \n\n\n#!/bin/bash\n\n# driver.sh - The simplest autograder we could think of. It checks\n#   that students can write a C program that compiles, and then\n#   executes with an exit status of zero.\n#   Usage: ./driver.sh\n\n# Compile the code\necho \nCompiling hello.c\n\n(make clean; make)\nstatus=$?\nif [ ${status} -ne 0 ]; then\n    echo \nFailure: Unable to compile hello.c (return status = ${status})\n\n    echo \n{\\\nscores\\\n: {\\\nCorrectness\\\n: 0}}\n\n    exit\nfi\n\n# Run the code\necho \nRunning ./hello\n\n./hello\nstatus=$?\nif [ ${status} -eq 0 ]; then\n    echo \nSuccess: ./hello runs with an exit status of 0\n\n    echo \n{\\\nscores\\\n: {\\\nCorrectness\\\n: 100}}\n\nelse\n    echo \nFailure: ./hello fails or returns nonzero exit status of ${status}\n\n    echo \n{\\\nscores\\\n: {\\\nCorrectness\\\n: 0}}\n\nfi\n\nexit\n\n\n\n\nFor example:\n\n\n$ ./driver.sh\n# Compiling hello.c\n# rm -rf *~ hello\n# gcc hello.c -o hello\n# Running ./hello\n# Hello, world\n# Success: ./hello runs with an exit status of 0\n# {\nscores\n: {\nCorrectness\n: 100}}\n\n\n\n\nNotice that the autograder expects the \nhello\n lab on the Autolab front-end to have been defined with a problem called \"Correctness\", with a maximum value of 100 points. If you forget to define the problems listed in the JSON autoresult, scores will still be logged, but they won't be posted to the database. \n\n\nRequired Autograding Files\n\n\nAutolab requires two \nautograding files\n called \nautograde.tar\n, which contains all of the code required by the autograder, and \nautograde-Makefile\n, which runs the autograder on the autograding image when each submission is graded.\n\n\nFor the Hello Lab, \nautograde.tar\n is simply a copy of the \nhello-handout.tar\n file that is handed out to students. And here is the corresponding \n\nhello/autograde-makefile\n:\n\n\nall:\n    tar xvf autograde.tar\n    cp hello.c hello-handout\n    (cd hello-handout; ./driver.sh)\n\nclean:\n    rm -rf *~ hello-handout\n\n\n\n\nThe makefile expands \nautograde.tar\n into \nhello-handout\n, copies \nhello.c\n (the submission file) into \nhello-handout\n, changes directory to \nhello-handout\n, builds the autograder, and then runs it. \n\n\nTest Directory\n\n\nFor our labs, we like to setup a test directory (called \ntest-autograder\n in this example), that allows us to test our \nautograde-Makefile\n and \nautograde-tar\n files by simulating Autolab's behavior on the autograding instance. The \ntest-autograder\n directory has the following form:\n\n\n$ cd test-autograder\n$ ls -l\n# total 3\n# lrwxr-xr-x 1 droh users  21 Aug  4 16:43 Makefile -\n ../autograde-Makefile\n# lrwxr-xr-x 1 droh users  16 Aug  4 16:43 autograde.tar -\n ../autograde.tar\n# -rw-rw-r-- 1 droh users 113 Aug  4 16:44 hello.c\n\n\n\n\nTo simulate Autolab's behavior on an autograding instance:\n\n\n$ cd test-autograder \n make clean \n make\n# Running ./hello\n# Hello, world\n# Success: ./hello runs with an exit status of 0\n# {\nscores\n: {\nCorrectness\n: 100}}\n\n\n\n\nWriteup directory\n\n\nThe \nhello/writeup\n contains the detailed lab writeup, either html or pdf file, that students can download from the Autolab front end.\n\n\nFAQ\n\n\nWhy is Autolab not displaying my stdout output?\n\n\nAutolab always shows the stdout output of running make, even when the program crashed or timed out. However, when it does crash and the expected autoresult json string is not appended to the output, parsing of the last line will fail. If this happens, any stdout output that is longer than 10,000 lines will be discarded (Note that this limit does not apply when the autoresult json is valid).", 
            "title": "Guide for Lab Authors"
        }, 
        {
            "location": "/lab/#guide-for-lab-authors", 
            "text": "This guide explains how to create autograded programming assignments (labs) for the Autolab system.", 
            "title": "Guide for Lab Authors"
        }, 
        {
            "location": "/lab/#writing-autograders", 
            "text": "An  autograder  is a program that takes a student's work as input, and generates some quantitative evaluation of that work as output. The student's work consists of one or more source files written in an arbitrary programming language. \nThe autograder processes these files and generates arbitrary text lines on stdout. The last text line on stdout must be a JSON string, called an  autoresult , that assigns an autograded score to one or more problems, and optionally, generates the scoreboard entries for this submission.  The JSON autoresult is a \"scores\" hash that assigns a numerical score to one or more problems, and an optional \"scoreboard\" array that provides the scoreboard entries for this submission. For example,   { scores : { Prob1 : 10,  Prob2 : 5}}  assigns 10 points to \"Prob1\" and 5 points to \"Prob2\" for this submission. The names of the problems must exactly match the names of the problems for this lab on the Autolab web site. Not all problems need to be autograded. For example, there might be a problem for this assessment called \"Style\" that you grade manually after the due date.   If you used the Autolab web site to configure a scoreboard for this lab with three columns called \"Prob1\", \"Prob2\", and \"Total\", then the autoresult might be:   { scores : { Prob1 : 10,  Prob2 : 5},  scoreboard : [10, 5, 15]}  By convention, an autograder accepts an optional  -A  command line argument that tells it to emit the JSON autoresult. So if you run the autograder outside of the context of Autolab, you can suppress the autoresult line by calling the autograder without the  -A  argument.  One of the nice properties of Autolab autograders is that they can be written and tested offline, without requiring any interaction with Autolab. Writing autograders is not easy, but the fact that they can be developed offline allows you to develop and test them in your own familiar computing environment.", 
            "title": "Writing Autograders"
        }, 
        {
            "location": "/lab/#installing-autograded-labs", 
            "text": "After you've written and tested the autograder, you then use the Autolab web site to create the autograded lab. Autolab supports creating new labs from scratch, or reusing labs from previous semesters. We'll describe each of these in turn.", 
            "title": "Installing Autograded Labs"
        }, 
        {
            "location": "/lab/#creating-an-autograded-lab-from-scratch", 
            "text": "", 
            "title": "Creating an Autograded Lab from Scratch"
        }, 
        {
            "location": "/lab/#step-1-create-the-new-lab", 
            "text": "Create a new lab by clicking the \"Install Assessment\" button and choosing \"Option 1: Create a New Assessment from Scratch.\" For course  course  and lab  lab ,  this will create a  lab directory  in the Autolab file hierarchy called  courses/ course / lab . This initial directory contains a couple of config files and a directory called  lab /handin  that will contain all of the student handin files. In general, you should never modify any of these.    Attention CMU Lab Authors  At CMU, the lab directory is called  /afs/cs/academic/class/ course /autolab/ lab . For example:  /afs/cs/academic/class/15213-f16/autolab/foo  is the lab directory for the lab named  foo  for the Fall 2016 instance of 15-213. All lab-related files must go in this  autolab  directory to avoid permissions issues.", 
            "title": "Step 1: Create the new lab."
        }, 
        {
            "location": "/lab/#step-2-configure-the-lab-for-autograding", 
            "text": "Using the \"Edit Assessment\" page, turn on autograding for this lab by selecting \"Add Autograder.\" You will be asked for the name of the image to be used for autograding this lab. The default image distributed with Autolab is an Ubuntu image called  autograding_image . If your class needs different software, then you or your facilities staff will need to update the default image or create a new one.    Attention CMU Lab Authors  The default autograding image at CMU is called  rhel.img  and is a copy of the software on the CMU Andrew machines ( linux.andrew.cmu.edu ). If you need custom software installed, please send mail to autolab-help@andrew.cmu.edu.   If you want a scoreboard, you should select \"Add Scoreboard,\" which will allow you to specify the number of columns and their names. The \"Add Scoreboard\" page contains a tutorial on how to do this.   You'll also need to define the names and point values for all the problems in this lab, including the autograded ones.   Each student submission is a single file, either a text source file or an archive file containing multiple files and directories. You'll need to specify the  base name  for the student submission files (e.g.,  mm.c ,  handin.tar ).", 
            "title": "Step 2: Configure the lab for autograding."
        }, 
        {
            "location": "/lab/#step-3-add-the-required-autograding-files", 
            "text": "For an autograded lab, Autolab expects the following two  autograding files  in the lab directory:    autograde-Makefile : runs the autograder on a student submission.  autograde.tar : contains all of the files (except for the student handin file) that are needed for autograding.    Each time a student submits their work or an instructor requests a regrade, Autolab    copies the student handin file, along with the two autograding files, to an empty directory on an  autograding instance ,   renames the student handin file to  base name  (e.g., hello.c, handin.tar),   renames  autograde-Makefile  to  Makefile ,   executes the command  make  on the autograding instance, and finally   captures the stdout generated by the autograder, and parses the resulting JSON autoresult to determine the autograded scores.", 
            "title": "Step 3: Add the required autograding files."
        }, 
        {
            "location": "/lab/#importing-an-autograded-lab-from-a-previous-semester", 
            "text": "If you've created a lab for a course in a previous semester and have access to the lab directory (as we do at CMU via AFS), you can import the lab into your current course by    copying the lab directory from the previous course to the current course,   cleaning out the  handin  directory, then   visiting the \"Install Assessment\" page and selecting \"Option 2: Import an existing assessment from the file system.\" Autolab will give you a list of all of the directories that appear to be uninstalled labs, from which you can select your particular lab.    If you don't have access to the lab directory, another option is to import a lab from a tarball that was created by running \"Export assessment\" in an instance of a lab from a previous semester. Visit the \"Install Assessment\" page and select \"Option 3: Import an existing assessment from tarball.\" This will upload the tarball, create a new lab directory by expanding the tarball, and then import the directory.", 
            "title": "Importing an Autograded Lab from a Previous Semester"
        }, 
        {
            "location": "/lab/#example-hello-lab", 
            "text": "In this section we'll look at the simplest possible autograded lab we could imagine, called, appropriately enough, the Hello Lab  \n(with  tarball ), which is stored in a lab directory called  hello  in the Autolab github repo. While it's trivial, it illustrates all of the aspects of developing an autograded lab, and provides a simple example that you can use for sanity testing on your Autolab installation.  In this lab, students are asked to write a version of the K R \"hello, world\" program, called  hello.c . The autograder simply checks that the submitted  hello.c  program compiles and runs with an exit status of zero. If so, the submission gets 100 points. Otherwise it gets 0 points.", 
            "title": "Example: Hello Lab"
        }, 
        {
            "location": "/lab/#directory-structure", 
            "text": "Autolab expects to find the  autograde-Makefile and  autograde.tar  files in the  hello  lab directory, but otherwise places no constraints on the contents and organization of this directory. However, based on our experience, we strongly recommend a directory structure with the following form:  hello/README :  # Basic files created by the lab author\nMakefile                Builds the lab from src/\nREADME                  \nautograde-Makefile      Makefile that runs the autograder \nsrc/                    Contains all src files and solutions         \ntest-autograder/        For testing autograder offline\nwriteup/                Lab writeup that students view from Autolab    \n\n# Files created by running make\nhello-handout/          The directory that is handed out to students, created\n                        using files from src/. \nhello-handout.tar       Archive of hello-handout directory\nautograde.tar           File that is copied to the autograding instance \n                        (along with autograde-Makefile and student handin file)\n\n# Files created and managed by Autolab\nhandin/    All students handin files\nhello.rb   Config file\nhello.yml  Database properties that persist from semester to semester\nlog.txt    Log of autograded submissions  The key idea with this directory structure is to place  all  code for the lab in the  src  directory, including the autograding code and any starter code handed out to students in the handout directory ( hello-handout.tar  in this example). Keeping all hard state in the  src  directory helps limit inconsistencies.   The main makefile creates  hello-handout  by copying files from  src , and then tars it up:  hello/Makefile :  #\n# Makefile to manage the example Hello Lab\n#\n\n# Get the name of the lab directory\nLAB = $(notdir $(PWD))\n\nall: handout handout-tarfile\n\nhandout: \n    # Rebuild the handout directory that students download\n    (rm -rf $(LAB)-handout; mkdir $(LAB)-handout)\n    cp -p src/Makefile-handout $(LAB)-handout/Makefile\n    cp -p src/README-handout $(LAB)-handout/README\n    cp -p src/hello.c-handout $(LAB)-handout/hello.c \n    cp -p src/driver.sh $(LAB)-handout\n\nhandout-tarfile: handout\n    # Build *-handout.tar and autograde.tar\n    tar cvf $(LAB)-handout.tar $(LAB)-handout\n    cp -p $(LAB)-handout.tar autograde.tar\n\nclean:\n    # Clean the entire lab directory tree.  Note that you can run\n    #  make clean; make  at any time while the lab is live with no\n    # adverse effects.\n    rm -f *~ *.tar\n    (cd src; make clean)\n    (cd test-autograder; make clean)\n    rm -rf $(LAB)-handout\n    rm -f autograde.tar\n#\n# CAREFULL!!! This will delete all student records in the logfile and\n# in the handin directory. Don't run this once the lab has started.\n# Use it to clean the directory when you are starting a new version\n# of the lab from scratch, or when you are debugging the lab prior\n# to releasing it to the students.\n#\ncleanallfiles:\n    # Reset the lab from scratch.\n    make clean\n    rm -f log.txt\n    rm -rf handin/*  Filenames are disambiguated by appending  -handout , which is stripped when they are copied to the handout directory. For example,  src/hello.c  is the instructor's solution file, and  src/hello.c-handout  is the starter code that is given to the students in  hello-handout/hello.c . And  src/README  is the README for the src directory and  src/README-handout  is the README that is handed out to students in  hello-handout/README .  To build the lab, type  make clean; make . You can do this as often as you like while the lab is live with no adverse effects. However, be careful to never type  make cleanallfiles  while the lab is live; this should only be done before the lab goes live; never during or after.", 
            "title": "Directory Structure"
        }, 
        {
            "location": "/lab/#source-directory", 
            "text": "The  hello/src/  directory \ncontains  all  of the code files for the Hello Lab, including the files that are handed out to students:  hello/src/README :  # Autograder and solution files\nMakefile                Makefile and ...\nREADME                  ... README for this directory\ndriver.sh*              Autograder\nhello.c                 Solution hello.c file\n\n# Files that are handed out to students\nMakefile-handout        Makefile and ...\nREADME-handout          ... README handed out to students\nhello.c-handout         Blank hello.c file handed out to students", 
            "title": "Source Directory"
        }, 
        {
            "location": "/lab/#handout-directory", 
            "text": "The  hello/hello-handout/  directory\ncontains the files that the students will use to work on the lab. It contains no hard state, and is populated entirely with files from  hello/src :  hello/hello-handout/README :  For this lab, you should write a tiny C program, called  hello.c ,\nthat prints  hello, world  to stdout and then indicates success by\nexiting with a status of zero.\n\nTo test your work: \n$ make clean; make; ./hello\n\nTo run the same autograder that Autolab will use when you submit:\n$ ./driver.sh\n\nFiles:\nREADME          This file\nMakefile        Compiles hello.c\ndriver.sh       Autolab autograder\nhello.c         Empty C file that you will edit  hello/hello-handout/Makefile  contains the rules that compile the student source code:  # Student makefile for the Hello Lab\nall: \n    gcc hello.c -o hello\n\nclean:\n    rm -rf *~ hello  To compile and run their code, students type:  $ make clean; make\n$ ./hello", 
            "title": "Handout Directory"
        }, 
        {
            "location": "/lab/#autograder", 
            "text": "The autograder for the Hello Lab is a trivially simple bash script called  driver.sh  that compiles and runs  hello.c  and verifies that it returns with an exit status of zero:  hello/src/driver.sh    #!/bin/bash\n\n# driver.sh - The simplest autograder we could think of. It checks\n#   that students can write a C program that compiles, and then\n#   executes with an exit status of zero.\n#   Usage: ./driver.sh\n\n# Compile the code\necho  Compiling hello.c \n(make clean; make)\nstatus=$?\nif [ ${status} -ne 0 ]; then\n    echo  Failure: Unable to compile hello.c (return status = ${status}) \n    echo  {\\ scores\\ : {\\ Correctness\\ : 0}} \n    exit\nfi\n\n# Run the code\necho  Running ./hello \n./hello\nstatus=$?\nif [ ${status} -eq 0 ]; then\n    echo  Success: ./hello runs with an exit status of 0 \n    echo  {\\ scores\\ : {\\ Correctness\\ : 100}} \nelse\n    echo  Failure: ./hello fails or returns nonzero exit status of ${status} \n    echo  {\\ scores\\ : {\\ Correctness\\ : 0}} \nfi\n\nexit  For example:  $ ./driver.sh\n# Compiling hello.c\n# rm -rf *~ hello\n# gcc hello.c -o hello\n# Running ./hello\n# Hello, world\n# Success: ./hello runs with an exit status of 0\n# { scores : { Correctness : 100}}  Notice that the autograder expects the  hello  lab on the Autolab front-end to have been defined with a problem called \"Correctness\", with a maximum value of 100 points. If you forget to define the problems listed in the JSON autoresult, scores will still be logged, but they won't be posted to the database.", 
            "title": "Autograder"
        }, 
        {
            "location": "/lab/#required-autograding-files", 
            "text": "Autolab requires two  autograding files  called  autograde.tar , which contains all of the code required by the autograder, and  autograde-Makefile , which runs the autograder on the autograding image when each submission is graded.  For the Hello Lab,  autograde.tar  is simply a copy of the  hello-handout.tar  file that is handed out to students. And here is the corresponding  hello/autograde-makefile :  all:\n    tar xvf autograde.tar\n    cp hello.c hello-handout\n    (cd hello-handout; ./driver.sh)\n\nclean:\n    rm -rf *~ hello-handout  The makefile expands  autograde.tar  into  hello-handout , copies  hello.c  (the submission file) into  hello-handout , changes directory to  hello-handout , builds the autograder, and then runs it.", 
            "title": "Required Autograding Files"
        }, 
        {
            "location": "/lab/#test-directory", 
            "text": "For our labs, we like to setup a test directory (called  test-autograder  in this example), that allows us to test our  autograde-Makefile  and  autograde-tar  files by simulating Autolab's behavior on the autograding instance. The  test-autograder  directory has the following form:  $ cd test-autograder\n$ ls -l\n# total 3\n# lrwxr-xr-x 1 droh users  21 Aug  4 16:43 Makefile -  ../autograde-Makefile\n# lrwxr-xr-x 1 droh users  16 Aug  4 16:43 autograde.tar -  ../autograde.tar\n# -rw-rw-r-- 1 droh users 113 Aug  4 16:44 hello.c  To simulate Autolab's behavior on an autograding instance:  $ cd test-autograder   make clean   make\n# Running ./hello\n# Hello, world\n# Success: ./hello runs with an exit status of 0\n# { scores : { Correctness : 100}}", 
            "title": "Test Directory"
        }, 
        {
            "location": "/lab/#writeup-directory", 
            "text": "The  hello/writeup  contains the detailed lab writeup, either html or pdf file, that students can download from the Autolab front end.", 
            "title": "Writeup directory"
        }, 
        {
            "location": "/lab/#faq", 
            "text": "", 
            "title": "FAQ"
        }, 
        {
            "location": "/lab/#why-is-autolab-not-displaying-my-stdout-output", 
            "text": "Autolab always shows the stdout output of running make, even when the program crashed or timed out. However, when it does crash and the expected autoresult json string is not appended to the output, parsing of the last line will fail. If this happens, any stdout output that is longer than 10,000 lines will be discarded (Note that this limit does not apply when the autoresult json is valid).", 
            "title": "Why is Autolab not displaying my stdout output?"
        }, 
        {
            "location": "/features/", 
            "text": "Autolab Features Documentation\n\n\nThis guide details the usage of features in Autolab.\n\n\nFeatures Documented (Work in Progress):\n\n\n\n\nScoreboards\n \n\n\nEmbedded Forms\n \n\n\n\n\nScoreboards\n\n\nScoreboards are a central concept of Autolab and are created by the output of \nAutograders\n. They anonomously rank students submitted assignments inspiring health competition and desire to improve. They are simple and highly customizable. Scoreboard's can be added/edited on the edit assessment screen (\n/courses/\ncourse\n/assessments/\nassessment\n/edit\n).\n\n\n\n\nIn general, scoreboards are configured using a JSON string.\n\n\nDefault Scoreboard\n\n\nThe default scoreboard displays the total problem scores, followed by each individual problem score, sorted in descending order by the total score.\n\n\nCustom Scoreboards\n\n\nAutograded assignments have the option of creating custom scoreboards. You can specify your own custom scoreboard using a JSON column specification.\n\n\nThe column spec consists of a \"scoreboard\" object, which is an array of JSON objects, where each object describes a column.\n\n\nExample:\n a scoreboard with one column, called \nScore\n.\n\n\n{\n    \nscoreboard\n: [ \n        {\nhdr\n:\nScore\n} \n    ] \n}\n\n\n\n\nA custom scoreboard sorts the first three columns, from left to right, in descending order. You can change the default sort order for a particular column by adding an optional \"asc:1\" element to its hash.\n\n\nExample:\n Scoreboard with two columns, \"Score\" and \"Ops\", with \"Score\" sorted descending, and then \"Ops\" ascending:\n\n\n{\n    \nscoreboard\n: [ \n        {\nhdr\n:\nScore\n}, \n        {\nhdr\n:\nOps\n, \nasc\n:1} \n    ] \n}\n\n\n\n\nScoreboard Entries\n\n\nThe values for each row in a custom scoreboard come directly from a \nscoreboard\n array object in the autoresult string produced by the Tango, the autograder.\n\n\nExample:\n Autoresult returning the score (97) for a single autograded problem called \nautograded\n, and a scoreboard entry with two columns: the autograded score (\nScore\n) and the number of operations (\nOps\n):\n\n\n{\n    \nscores\n: {\n        \nautograded\n:97\n    }, \n    \nscoreboard\n:[97, 128] \n}\n\n\n\n\nFor more information on how to use Autograders and Scoreboards together, visit the \nGuide for Lab Authors\n.\n\n\nEmbedded Forms\n\n\nThis feature allows an instructor to create an assessment which does not require a file submission on the part of the student. Instead, when an assessment is created, the hand-in page for that assessment will display an HTML form of the instructor\u2019s design. When the student submits the form, the information is sent directly in JSON format to the Tango grading server for evaluation.\n\n\n\n\nTango Required\n\n\nTango is needed to use this feature. Please install \nTango\n and connect it to Autolab before proceeding.\n\n\n\n\n\n\nCreating an Embedded Form\n\n\nCreate an HTML file with a combination of the following elements. The HTML file need only include form elements, because it will automatically be wrapped in a \nform\n/form\n block when it is rendered on the page.\n\n\nIn order for the JSON string (the information passed to the grader) to be constructed properly, your form elements must follow the following conventions:\n\n\n\n\nA unique name attribute\n\n\nA value attribute which corresponds to the correct answer to the question (unless it is a text field or text area)\n\n\n\n\nHTML Form Reference:\n\n\nText Field (For short responses)\n\n\ninput type=\u201ctext\u201d name=\u201cquestion-1\u201d/\n\n\n\n\nText Area (For coding questions)\n\n\ntextarea name=\u201cquestion-2\u201d style=\u201cwidth:100%\u201d/\n\n\n\n\nRadio Button (For multiple choice)\n\n\ndiv class = \u201crow\n\n    \ninput name=\u201cquestion-3\u201d type=\u201cradio\u201d value=\u201cobject\u201d id=\u201cq3-1\u201d/\n\n    \nlabel for=\nq3-1\nObject\n/label\n\n    \ninput name=\u201cquestion-3\u201d type=\u201cradio\u201d value=\u201cboolean\u201d id=\u201cq3-2\u201d/\n\n    \nlabel for=\u201cq3-2\u201d\nBoolean\n/label\n\n\n/div\n\n\n\n\nDropdown (For multiple choice or select all that apply)\n \n\n\nselect multiple name=\nquestion-4\n\n    \noption value=\n1\nOption 1\n/option\n\n    \noption value=\n2\nOption 2\n/option\n\n    \noption value=\n3\nOption 3\n/option\n\n\n/select\n\n\n\n\nExample Form (shown in screenshot above)\n\n\ninput type=\ntext\n name=\nquestion-1\n id=\nq1\n placeholder=\nWhat's your name?\n /\n\n\n\ndiv class = \nrow\n\n    \ninput name=\nquestion-2\n type=\nradio\n value=\nfreshman\n id=\nq3-1\n/\n\n    \nlabel for=\nq3-1\nFreshman\n/label\n\n\n    \ninput name=\nquestion-2\n type=\nradio\n value=\nsophomore\n id=\nq3-2\n/\n\n    \nlabel for=\nq3-2\nSophomore\n/label\n\n\n    \ninput name=\nquestion-2\n type=\nradio\n value=\njunior\n id=\nq3-3\n/\n\n    \nlabel for=\nq3-3\nJunior\n/label\n\n\n    \ninput name=\nquestion-2\n type=\nradio\n value=\nsenior\n id=\nq3-4\n/\n\n    \nlabel for=\nq3-4\nSenior\n/label\n\n\n/div\n\n\nlabel for=\nq4\nWhat's your favorite language?\n/label\n\n\nselect name=\nquestion-3\n id=\nq4\n\n    \noption value=\nC\nC\n/option\n\n    \noption value=\nPython\nPython\n/option\n\n    \noption value=\nJava\nJava\n/option\n\n\n/select\n\n\n\n\nNavigate to the Basic section of editing an assessment (\n/courses/\ncourse\n/assessments/\nassessment\n/edit\n), check the check box, and upload the HTML file. Ensure you submit the form by clicking \nSave\n at the bottom of the page.\n\n\n\n\nGrading an Embedded Form\n\n\nWhen a student submits a form, the form data is sent to \nTango\n in the form of a JSON string in the file \nout.txt.\n In your grading script, parse the contents of \nout.txt\n as a JSON object. The JSON object will be a key-value pair data structure, so you can access the students response string (\nvalue\n) by its unique key (the \nname\n attribute).\n\n\nFor the example form shown above, the JSON object will be as follows:\n\n\n{\n    \nutf8\n:\n\u2713\n,\n    \nauthenticity_token\n:\nLONGAUTHTOKEN\n,\n    \nsubmission[embedded_quiz_form_answer]\n:\n,\n    \nquestion-1\n:\nJohn Smith\n,\n    \nquestion-2\n:\njunior\n,\n    \nquestion-3\n:\nPython\n,\n    \nintegrity_checkbox\n:\n1\n\n}\n\n\n\n\nUse this information to do any processing you need in Tango.If you find any problems, please file an issue on the \nAutolab Github\n.", 
            "title": "Advanced Features"
        }, 
        {
            "location": "/features/#autolab-features-documentation", 
            "text": "This guide details the usage of features in Autolab.  Features Documented (Work in Progress):   Scoreboards    Embedded Forms", 
            "title": "Autolab Features Documentation"
        }, 
        {
            "location": "/features/#scoreboards", 
            "text": "Scoreboards are a central concept of Autolab and are created by the output of  Autograders . They anonomously rank students submitted assignments inspiring health competition and desire to improve. They are simple and highly customizable. Scoreboard's can be added/edited on the edit assessment screen ( /courses/ course /assessments/ assessment /edit ).   In general, scoreboards are configured using a JSON string.", 
            "title": "Scoreboards"
        }, 
        {
            "location": "/features/#default-scoreboard", 
            "text": "The default scoreboard displays the total problem scores, followed by each individual problem score, sorted in descending order by the total score.", 
            "title": "Default Scoreboard"
        }, 
        {
            "location": "/features/#custom-scoreboards", 
            "text": "Autograded assignments have the option of creating custom scoreboards. You can specify your own custom scoreboard using a JSON column specification.  The column spec consists of a \"scoreboard\" object, which is an array of JSON objects, where each object describes a column.  Example:  a scoreboard with one column, called  Score .  {\n     scoreboard : [ \n        { hdr : Score } \n    ] \n}  A custom scoreboard sorts the first three columns, from left to right, in descending order. You can change the default sort order for a particular column by adding an optional \"asc:1\" element to its hash.  Example:  Scoreboard with two columns, \"Score\" and \"Ops\", with \"Score\" sorted descending, and then \"Ops\" ascending:  {\n     scoreboard : [ \n        { hdr : Score }, \n        { hdr : Ops ,  asc :1} \n    ] \n}", 
            "title": "Custom Scoreboards"
        }, 
        {
            "location": "/features/#scoreboard-entries", 
            "text": "The values for each row in a custom scoreboard come directly from a  scoreboard  array object in the autoresult string produced by the Tango, the autograder.  Example:  Autoresult returning the score (97) for a single autograded problem called  autograded , and a scoreboard entry with two columns: the autograded score ( Score ) and the number of operations ( Ops ):  {\n     scores : {\n         autograded :97\n    }, \n     scoreboard :[97, 128] \n}  For more information on how to use Autograders and Scoreboards together, visit the  Guide for Lab Authors .", 
            "title": "Scoreboard Entries"
        }, 
        {
            "location": "/features/#embedded-forms", 
            "text": "This feature allows an instructor to create an assessment which does not require a file submission on the part of the student. Instead, when an assessment is created, the hand-in page for that assessment will display an HTML form of the instructor\u2019s design. When the student submits the form, the information is sent directly in JSON format to the Tango grading server for evaluation.   Tango Required  Tango is needed to use this feature. Please install  Tango  and connect it to Autolab before proceeding.", 
            "title": "Embedded Forms"
        }, 
        {
            "location": "/features/#creating-an-embedded-form", 
            "text": "Create an HTML file with a combination of the following elements. The HTML file need only include form elements, because it will automatically be wrapped in a  form /form  block when it is rendered on the page.  In order for the JSON string (the information passed to the grader) to be constructed properly, your form elements must follow the following conventions:   A unique name attribute  A value attribute which corresponds to the correct answer to the question (unless it is a text field or text area)   HTML Form Reference:  Text Field (For short responses)  input type=\u201ctext\u201d name=\u201cquestion-1\u201d/  Text Area (For coding questions)  textarea name=\u201cquestion-2\u201d style=\u201cwidth:100%\u201d/  Radio Button (For multiple choice)  div class = \u201crow \n     input name=\u201cquestion-3\u201d type=\u201cradio\u201d value=\u201cobject\u201d id=\u201cq3-1\u201d/ \n     label for= q3-1 Object /label \n     input name=\u201cquestion-3\u201d type=\u201cradio\u201d value=\u201cboolean\u201d id=\u201cq3-2\u201d/ \n     label for=\u201cq3-2\u201d Boolean /label  /div  Dropdown (For multiple choice or select all that apply)    select multiple name= question-4 \n     option value= 1 Option 1 /option \n     option value= 2 Option 2 /option \n     option value= 3 Option 3 /option  /select  Example Form (shown in screenshot above)  input type= text  name= question-1  id= q1  placeholder= What's your name?  /  div class =  row \n     input name= question-2  type= radio  value= freshman  id= q3-1 / \n     label for= q3-1 Freshman /label \n\n     input name= question-2  type= radio  value= sophomore  id= q3-2 / \n     label for= q3-2 Sophomore /label \n\n     input name= question-2  type= radio  value= junior  id= q3-3 / \n     label for= q3-3 Junior /label \n\n     input name= question-2  type= radio  value= senior  id= q3-4 / \n     label for= q3-4 Senior /label  /div  label for= q4 What's your favorite language? /label  select name= question-3  id= q4 \n     option value= C C /option \n     option value= Python Python /option \n     option value= Java Java /option  /select  Navigate to the Basic section of editing an assessment ( /courses/ course /assessments/ assessment /edit ), check the check box, and upload the HTML file. Ensure you submit the form by clicking  Save  at the bottom of the page.", 
            "title": "Creating an Embedded Form"
        }, 
        {
            "location": "/features/#grading-an-embedded-form", 
            "text": "When a student submits a form, the form data is sent to  Tango  in the form of a JSON string in the file  out.txt.  In your grading script, parse the contents of  out.txt  as a JSON object. The JSON object will be a key-value pair data structure, so you can access the students response string ( value ) by its unique key (the  name  attribute).  For the example form shown above, the JSON object will be as follows:  {\n     utf8 : \u2713 ,\n     authenticity_token : LONGAUTHTOKEN ,\n     submission[embedded_quiz_form_answer] : ,\n     question-1 : John Smith ,\n     question-2 : junior ,\n     question-3 : Python ,\n     integrity_checkbox : 1 \n}  Use this information to do any processing you need in Tango.If you find any problems, please file an issue on the  Autolab Github .", 
            "title": "Grading an Embedded Form"
        }, 
        {
            "location": "/api-overview/", 
            "text": "Overview\n\n\nThe web interface that has served us well for many years is no longer the only way to use Autolab. With the API, developers will be able to help make Autolab more versatile and convenient: Whether it be with a mobile app, a command line tool, a browser extension, or something we've never even thought of.\n\n\nFor students and instructors who only plan to use Autolab, try out the \nAutolab CLI\n.\n\n\nThe Autolab REST API allows developers to create clients that can access features of Autolab on behalf of Autolab users.\n\n\nV1 of the API allows clients to:\n\n\n\n\nAccess basic user info\n\n\nView courses and assessments\n\n\nSubmit to assessments\n\n\nView scores and feedback\n\n\nManage course enrollments\n\n\n\n\nAuthorization\n\n\nAll endpoints of the Autolab API requires client authentication in the form of an access token. To obtain this access token, clients must obtain authorization from the user.\n\n\nAutolab API uses the standard \nOAuth2\n \nAuthorization Code Grant\n for user authorization. For clients with no easy access to web browsers (e.g. console apps), an alternative \ndevice flow\n-based authorization method is provided as well.\n\n\nTo understand how to authorize and unauthorize clients as a user, go to \nManaging Authorized Apps\n\n\nAuthorization Code Grant Flow\n\n\nThe authorization code grant consists of 5 basic steps:\n\n\n\n\nClient directs the user to the authorization request endpoint via a web browser.\n\n\nAuthorization server (Autolab) authenticates the user.\n\n\nIf user grants access to the client, the authorization server provides an \"authorization code\" to the client.\n\n\nClient exchanges the authorization code for an access token from the access token endpoint.\n\n\nClient uses the access token for subsequent requests to the API.\n\n\n\n\nThe endpoint for obtaining user authorization is\n\n/oauth/authorize\n\n\nThe endpoint for obtaning access tokens and refresh tokens is\n\noauth/token\n\n\nSection 4.1 of RFC 6749\n details the parameters required and the response clients can expect from these endpoints.\n\n\nAutolab API provides a refresh token with every new access token. Once the access token has expired, the client can use the refresh token to obtain a new access token, refresh token pair. Details are also provided in RFC 6749 \nhere\n.\n\n\nDevice Flow\n\n\nFor devices that cannot use a web browser to obtain user authorization, the alternative device flow approach circumvents the first 3 steps in the authorization code grant flow. Instead of directing a user to the authorization page directly, the client obtains a user code that the user can enter on the Autolab website from any device. The website then takes the user through the authorization procedure, and returns the authorization code to the client. The client can then use this code to request an access token from the access token endpoint as usual.\n\n\nNote that this is different from the \"device flow\" described in the Internet Draft linked above.\n\n\nObtaining User Code\n\n\nRequest Endpoint: \nGET /device_flow_init\n\n\nParameters:\n\n\n\n\nclient_id: the client_id obtained when registering the client\n\n\n\n\nSuccess Response:\n\n\n\n\ndevice_code: the verification code used by the client (should be kept secret from the user).\n\n\nuser_code: the verification code that should be displayed to the user.\n\n\nverification_uri: the verification uri that the user should use to authorize the client. By default is \n/activate\n\n\n\n\nThe latter two should be displayed to the user.\n\n\nObtaining Authorization Code\n\n\nAfter asking the user to enter the user code on the verification site, the client should poll the device_flow_authorize endpoint to find out if the user has completed the authorization step.\n\n\nRequest Endpoint: \nGET /device_flow_authorize\n\n\nParameters:\n\n\n\n\nclient_id: the client_id obtained when registering the client\n\n\ndevice_code: the device_code obtained from the device_flow_init endpoint\n\n\n\n\nFailure Responses:\n\n\n\n\n400 Bad Request: {error: authorization_pending}\n\n  The user has not yet granted or denied the authorization request. Please try again in a while.\n\n\n429 Too Many Requests: {error: Retry later}\n\n  The client is polling too frequently. Please wait for a while before polling again.\n\n  The default rate limit is once every 5 seconds.\n\n\n\n\nSuccess Response:\n\n\n\n\ncode: the authorization code that should be used to obtain an access token.\n\n\n\n\nThe client could then perform steps 4 and 5 of the Authorization Code Grant Flow.\n\n\nGetting Started\n\n\nAutolab requires all client applications to be registered clients. Upon registration, a client_id and client_secret pair will be provided to the developers for use in the app as identification to the server. Please contact the administrators of your specific Autolab deployment for registration.\n\n\n\n\nSecurity Concerns\n\n\nPlease make sure to keep the client_secret secret. Leaking this code may allow third-parties to impersonate your app.\n\n\n\n\nScopes\n\n\nThe scopes of an API client specifies the permissions it has, and must be specified during client registration (can be modified later). Currently, Autolab offers the following scopes for third-party clients:\n\n\n\n\nuser_info: Access your basic info (e.g. name, email, school, year).\n\n\nuser_courses: Access your courses and assessments.\n\n\nuser_scores: Access your submissions, scores, and feedback.\n\n\nuser_submit: Submit to assessments on your behalf.\n\n\ninstructor_all: Access admin options of courses where you are an instructor.\n\n\n\n\nExample usages\n\n\n\n\nIf your app only wants to use the API for quick user authentication, you only need the 'user_info' scope.\n\n\nIf you want to develop a mobile client for Autolab that allows students to view their upcoming assessments, you may ask for 'user_info' and 'user_courses'.\n\n\nIf you want to write a full desktop client that users can use to submit to assessments and view their grades, you may ask for all 5 scopes.\n\n\n\n\nOf course, these are only examples. We can't wait to see what new usages of the API you may come up with! We just recommend that you only ask for the scopes you need as the users will be shown the required scopes during authorization, and it gives them peace of mind to know that an app doesn't ask for excessive permissions.", 
            "title": "Overview"
        }, 
        {
            "location": "/api-overview/#overview", 
            "text": "The web interface that has served us well for many years is no longer the only way to use Autolab. With the API, developers will be able to help make Autolab more versatile and convenient: Whether it be with a mobile app, a command line tool, a browser extension, or something we've never even thought of.  For students and instructors who only plan to use Autolab, try out the  Autolab CLI .  The Autolab REST API allows developers to create clients that can access features of Autolab on behalf of Autolab users.  V1 of the API allows clients to:   Access basic user info  View courses and assessments  Submit to assessments  View scores and feedback  Manage course enrollments", 
            "title": "Overview"
        }, 
        {
            "location": "/api-overview/#authorization", 
            "text": "All endpoints of the Autolab API requires client authentication in the form of an access token. To obtain this access token, clients must obtain authorization from the user.  Autolab API uses the standard  OAuth2   Authorization Code Grant  for user authorization. For clients with no easy access to web browsers (e.g. console apps), an alternative  device flow -based authorization method is provided as well.  To understand how to authorize and unauthorize clients as a user, go to  Managing Authorized Apps", 
            "title": "Authorization"
        }, 
        {
            "location": "/api-overview/#authorization-code-grant-flow", 
            "text": "The authorization code grant consists of 5 basic steps:   Client directs the user to the authorization request endpoint via a web browser.  Authorization server (Autolab) authenticates the user.  If user grants access to the client, the authorization server provides an \"authorization code\" to the client.  Client exchanges the authorization code for an access token from the access token endpoint.  Client uses the access token for subsequent requests to the API.   The endpoint for obtaining user authorization is /oauth/authorize  The endpoint for obtaning access tokens and refresh tokens is oauth/token  Section 4.1 of RFC 6749  details the parameters required and the response clients can expect from these endpoints.  Autolab API provides a refresh token with every new access token. Once the access token has expired, the client can use the refresh token to obtain a new access token, refresh token pair. Details are also provided in RFC 6749  here .", 
            "title": "Authorization Code Grant Flow"
        }, 
        {
            "location": "/api-overview/#device-flow", 
            "text": "For devices that cannot use a web browser to obtain user authorization, the alternative device flow approach circumvents the first 3 steps in the authorization code grant flow. Instead of directing a user to the authorization page directly, the client obtains a user code that the user can enter on the Autolab website from any device. The website then takes the user through the authorization procedure, and returns the authorization code to the client. The client can then use this code to request an access token from the access token endpoint as usual.  Note that this is different from the \"device flow\" described in the Internet Draft linked above.", 
            "title": "Device Flow"
        }, 
        {
            "location": "/api-overview/#obtaining-user-code", 
            "text": "Request Endpoint:  GET /device_flow_init  Parameters:   client_id: the client_id obtained when registering the client   Success Response:   device_code: the verification code used by the client (should be kept secret from the user).  user_code: the verification code that should be displayed to the user.  verification_uri: the verification uri that the user should use to authorize the client. By default is  /activate   The latter two should be displayed to the user.", 
            "title": "Obtaining User Code"
        }, 
        {
            "location": "/api-overview/#obtaining-authorization-code", 
            "text": "After asking the user to enter the user code on the verification site, the client should poll the device_flow_authorize endpoint to find out if the user has completed the authorization step.  Request Endpoint:  GET /device_flow_authorize  Parameters:   client_id: the client_id obtained when registering the client  device_code: the device_code obtained from the device_flow_init endpoint   Failure Responses:   400 Bad Request: {error: authorization_pending} \n  The user has not yet granted or denied the authorization request. Please try again in a while.  429 Too Many Requests: {error: Retry later} \n  The client is polling too frequently. Please wait for a while before polling again. \n  The default rate limit is once every 5 seconds.   Success Response:   code: the authorization code that should be used to obtain an access token.   The client could then perform steps 4 and 5 of the Authorization Code Grant Flow.", 
            "title": "Obtaining Authorization Code"
        }, 
        {
            "location": "/api-overview/#getting-started", 
            "text": "Autolab requires all client applications to be registered clients. Upon registration, a client_id and client_secret pair will be provided to the developers for use in the app as identification to the server. Please contact the administrators of your specific Autolab deployment for registration.   Security Concerns  Please make sure to keep the client_secret secret. Leaking this code may allow third-parties to impersonate your app.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/api-overview/#scopes", 
            "text": "The scopes of an API client specifies the permissions it has, and must be specified during client registration (can be modified later). Currently, Autolab offers the following scopes for third-party clients:   user_info: Access your basic info (e.g. name, email, school, year).  user_courses: Access your courses and assessments.  user_scores: Access your submissions, scores, and feedback.  user_submit: Submit to assessments on your behalf.  instructor_all: Access admin options of courses where you are an instructor.   Example usages   If your app only wants to use the API for quick user authentication, you only need the 'user_info' scope.  If you want to develop a mobile client for Autolab that allows students to view their upcoming assessments, you may ask for 'user_info' and 'user_courses'.  If you want to write a full desktop client that users can use to submit to assessments and view their grades, you may ask for all 5 scopes.   Of course, these are only examples. We can't wait to see what new usages of the API you may come up with! We just recommend that you only ask for the scopes you need as the users will be shown the required scopes during authorization, and it gives them peace of mind to know that an app doesn't ask for excessive permissions.", 
            "title": "Scopes"
        }, 
        {
            "location": "/api-interface/", 
            "text": "This page details all the endpoints of the Autolab REST API. \n\n\nThe client's access token should be included as a parameter to all endpoints. For details on obtaining access tokens, please see the \nAPI Overview\n\n\nFormat\n\n\nAll endpoints expect the HTTP GET method unless otherwise specified.\n\n\nAll parameters listed below are required unless denoted [OPTIONAL].\n\n\nAll responses are in JSON format.\n\n\n\n\nIf the request is completed successfully, the HTTP response code will be 200. The reference below details the keys and their respective value types that the client can expect from each endpoint.\n\n\nIf an error occurs, the response code will \nnot\n be 200. The returned JSON will be an object with the key 'error'. Its value will be a string that explains the error.\n\n\n\n\nNotes on return value types\n\n\nAll datetime formats are strings in the form of 'YYYY-MM-DDThh:mm:ss.sTZD'.\n\ne.g. '2017-10-23T04:17:41.000-04:00', which means 4:17:41 AM on October 23rd, 2017 US Eastern Time.\n\n\nJSON spec only has a 'number' type, but the spec below distinguishes between integers and floats for ease of use in certain languages.\n\n\nIf a field does not exist, the value is generally null. Please be sure to check if a value is null before using it.\n\n\nInterface\n\n\n\n\nuser\n\n\nGet basic user info.\n\n\nScope:\n 'user_info'\n\n\nEndpoint:\n \n/user\n\n\nParameters:\n [none]\n\n\nResponses:\n\n\n\n\n\n\n\n\nkey\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nfirst_name\n\n\nstring\n\n\nThe user's first name.\n\n\n\n\n\n\nlast_name\n\n\nstring\n\n\nThe user's last name.\n\n\n\n\n\n\nemail\n\n\nstring\n\n\nThe user's registered email.\n\n\n\n\n\n\nschool\n\n\nstring\n\n\nThe school the user belongs to.\n\n\n\n\n\n\nmajor\n\n\nstring\n\n\nThe user's major of study.\n\n\n\n\n\n\nyear\n\n\nstring\n\n\nThe user's year.\n\n\n\n\n\n\n\n\n\n\ncourses\n\n\nGet all courses currently taking or taken before.\n\n\nScope:\n 'user_courses'\n\n\nEndpoint:\n \n/courses\n\n\nParameters:\n\n\n\n\nstate\n\n  [OPTIONAL] filter the courses by the state of the course. Should be one of 'disabled', 'completed', 'current', or 'upcoming'. If no state is provided, all courses are returned.\n\n\n\n\nResponses:\n\n\nA list of courses. Each course contains:\n\n\n\n\n\n\n\n\nkey\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nstring\n\n\nThe unique url-safe name.\n\n\n\n\n\n\ndisplay_name\n\n\nstring\n\n\nThe full name of the course.\n\n\n\n\n\n\nsemester\n\n\nstring\n\n\nThe semester this course is being offered.\n\n\n\n\n\n\nlate_slack\n\n\ninteger\n\n\nThe number of seconds after a deadline that the server will still accept a submission and not count it as late.\n\n\n\n\n\n\ngrace_days\n\n\ninteger\n\n\nAKA late days. The total number of days (over the entire semester) a student is allowed to submit an assessment late.\n\n\n\n\n\n\nauth_level\n\n\nstring\n\n\nThe user's level of access for this course. One of 'student', 'course_assistant', or 'instructor'.\n\n\n\n\n\n\n\n\n\n\nassessments\n\n\nGet all the assessments of a course.\n\n\nScope:\n 'user_courses'\n\n\nEndpoint:\n \n/courses/{course_name}/assessments\n\n\nParameters:\n [none]\n\n\nResponses:\n\n\nA list of assessments. If the user is only a student of the course, only released assessments are available. Otherwise, all assessments are available. Each assessment contains:\n\n\n\n\n\n\n\n\nkey\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nstring\n\n\nThe unique url-safe name.\n\n\n\n\n\n\ndisplay_name\n\n\nstring\n\n\nThe full name of the assessments.\n\n\n\n\n\n\nstart_at\n\n\ndatetime\n\n\nThe time this assessment is released to students.\n\n\n\n\n\n\ndue_at\n\n\ndatetime\n\n\nStudents can submit before this time without being penalized or using grace days.\n\n\n\n\n\n\nend_at\n\n\ndatetime\n\n\nLast possible time that students can submit (except those granted extensions.)\n\n\n\n\n\n\ncategory_name\n\n\nstring\n\n\nName of the category this assessment belongs to.\n\n\n\n\n\n\ngrading_deadline\n\n\nstring\n\n\nNot available if the user is a student.\nTime after which final scores are included in the gradebook.\n\n\n\n\n\n\n\n\n\n\nassessment details\n\n\nShow detailed information of an assessment.\n\n\nScope:\n 'user_courses'\n\n\nEndpoint:\n \n/courses/{course_name}/assessments/{assessment_name}\n\n\nParameters:\n [none]\n\n\nResponse:\n\n\n\n\n\n\n\n\nkey\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nstring\n\n\nThe unique url-safe name.\n\n\n\n\n\n\ndisplay_name\n\n\nstring\n\n\nThe full name of the assessments.\n\n\n\n\n\n\ndescription\n\n\nstring\n\n\nA short description of the assessment.\n\n\n\n\n\n\nstart_at\n\n\ndatetime\n\n\nThe time this assessment is released to students.\n\n\n\n\n\n\ndue_at\n\n\ndatetime\n\n\nStudents can submit before this time without being penalized or using grace days.\n\n\n\n\n\n\nend_at\n\n\ndatetime\n\n\nLast possible time that students can submit (except those granted extensions.)\n\n\n\n\n\n\nupdated_at\n\n\ndatetime\n\n\nThe last time an update was made to the assessment.\n\n\n\n\n\n\nmax_grace_days\n\n\ninteger\n\n\nMaximum number of grace days that a student can spend on this assessment.\n\n\n\n\n\n\nmax_submissions\n\n\ninteger\n\n\nThe maximum number of times a student can submit the assessment.\n-1 means unlimited submissions.\n\n\n\n\n\n\ndisable_handins\n\n\nboolean\n\n\nAre handins disallowed by students?\n\n\n\n\n\n\ncategory_name\n\n\nstring\n\n\nName of the category this assessment belongs to.\n\n\n\n\n\n\ngroup_size\n\n\ninteger\n\n\nThe maximum size of groups for this assessment.\n\n\n\n\n\n\nwriteup_format\n\n\nstring\n\n\nThe format of this assessment's writeup.\nOne of 'none', 'url', or 'file'.\n\n\n\n\n\n\nhandout_format\n\n\nstring\n\n\nThe format of this assessment's handout.\nOne of 'none', 'url', or 'file'.\n\n\n\n\n\n\nhas_scoreboard\n\n\nboolean\n\n\nDoes this assessment have a scoreboard?\n\n\n\n\n\n\nhas_autograder\n\n\nboolean\n\n\nDoes this assessment use an autograder?\n\n\n\n\n\n\ngrading_deadline\n\n\nstring\n\n\nNot available if the user is a student.\nTime after which final scores are included in the gradebook.\n\n\n\n\n\n\n\n\n\n\nproblems\n\n\nGet all problems of an assessment.\n\n\nScope:\n 'user_courses'\n\n\nEndpoint \n/courses/{course_name}/assessments/{assessment_name}/problems\n\n\nParameters:\n [none]\n\n\nResponses:\n\n\nA list of problems. Each problem contains:\n\n\n\n\n\n\n\n\nkey\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nstring\n\n\nFull name of the problem.\n\n\n\n\n\n\ndescription\n\n\nstring\n\n\nBrief description of the problem.\n\n\n\n\n\n\nmax_score\n\n\nfloat\n\n\nMaximum possible score for this problem.\n\n\n\n\n\n\noptional\n\n\nboolean\n\n\nIs this problem optional?\n\n\n\n\n\n\n\n\n\n\nwriteup\n\n\nGet the writeup of an assessment.\n\n\nScope:\n 'user_courses'\n\n\nEndpoint:\n \n/courses/{course_name}/assessments/{assessment_name}/writeup\n\n\nParameters:\n [none]\n\n\nResponses:\n\n\n\n\nIf no writeup exists:\n\n\n\n\n\n\n\n\n\n\nkey\n\n\ntype\n\n\nvalue\n\n\n\n\n\n\n\n\n\n\nwriteup\n\n\nstring\n\n\n\"none\"\n\n\n\n\n\n\n\n\n\n\nIf writeup is a url:\n\n\n\n\n\n\n\n\n\n\nkey\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nurl\n\n\nstring\n\n\nThe url of the writeup.\n\n\n\n\n\n\n\n\n\n\nIf writeup is a file:\n\nThe file is returned.\n\n\n\n\n\n\nhandout\n\n\nGet the handout of an assessment.\n\n\nScope:\n 'user_courses'\n\n\nEndpoint:\n \n/courses/{course_name}/assessments/{assessment_name}/handout\n\n\nParameters:\n [none]\n\n\nResponses:\n [same as \nwriteup\n]\n\n\n\n\nsubmit\n\n\nMake a submission to an assessment.\n\n\nScope:\n 'user_submit'\n\n\nEndpoint:\n \nPOST /courses/{course_name}/assessments/{assessment_name}/submit\n\n\nParameters:\n\n\n\n\nsubmission[file]\n\n  The file to submit\n\n  \nNote: the name should be the string 'submission[file]'\n\n\n\n\nSuccess Response:\n\n\n\n\n\n\n\n\nkey\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nversion\n\n\ninteger\n\n\nThe version number of the newly submitted submission.\n\n\n\n\n\n\nfilename\n\n\nstring\n\n\nThe final filename the submitted file is referred to as.\n\n\n\n\n\n\n\n\nFailure Response:\n\n\nA valid submission request may still fail for many reasons, such as file too large, handins disabled by staff, deadline has passed, etc.\n\n\nWhen a submission fails, the HTTP response code will not be 200. The response body will include a json with the key 'error'. Its contents will be a user-friendly string that the client may display to the user to explain why the submission has failed. The client \nmust not\n repeat the request without any modifications. The client is \nnot\n expected to be able to handle the error automatically.\n\n\n\n\nsubmissions\n\n\nGet all submissions the user has made via this client. To protect the user's private scores and feedback, only the submissions made via your client is returned. i.e. submissions made through other clients or on the Autolab website are never returned.\n\n\nScope:\n 'user_scores'\n\n\nEndpoint:\n \n/courses/{course_name}/assessments/{assessment_name}/submissions\n\n\nParameters:\n [none]\n\n\nResponse:\n\n\nA list of submissions. Each submission includes:\n\n\n\n\n\n\n\n\nkey\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nversion\n\n\ninteger\n\n\nThe version number of this submission.\n\n\n\n\n\n\nfilename\n\n\nstring\n\n\nThe final filename the submitted file is referred to as.\n\n\n\n\n\n\ncreated_at\n\n\ndatetime\n\n\nThe time this submission was made.\n\n\n\n\n\n\nscores\n\n\nobject\n\n\nA dictionary containing the scores of each problem.\nThe keys are the names of the problems, and the value is either the score (a float), or the string 'unreleased' if the score for this problem is not yet released.\n\n\n\n\n\n\n\n\n\n\nfeedback\n\n\nGet the text feedback given to a problem of a submission.\n\n\nFor autograded assessments, the feedback will by default be the autograder feedback, and will be identical for all problems.\n\n\nScope:\n 'user_scores'\n\n\nEndpoint:\n \n/courses/{course_name}/assessments/{assessment_name}/submissions/{submission_version}/feedback\n\n\nParameters:\n\n\n\n\nproblem\n\n  The name of the problem that the feedback is given to.\n\n\n\n\nResponse:\n\n\n\n\n\n\n\n\nkey\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nfeedback\n\n\nstring\n\n\nThe full feedback text for this problem.\n\n\n\n\n\n\n\n\n\n\ncourse_user_data (enrollments)\n\n\nAutolab uses the term course_user_data to represent the users affiliated with a course. It includes all students, course assistants, and instructors of the course.\n\n\nA course_user_data object in the response will be formatted in this form:\n\n\n\n\n\n\n\n\nkey\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nfirst_name\n\n\nstring\n\n\nThe user's first name.\n\n\n\n\n\n\nlast_name\n\n\nstring\n\n\nThe user's last name.\n\n\n\n\n\n\nemail\n\n\nstring\n\n\nThe user's registered email.\n\n\n\n\n\n\nschool\n\n\nstring\n\n\nThe school the user belongs to.\n\n\n\n\n\n\nmajor\n\n\nstring\n\n\nThe user's major of study.\n\n\n\n\n\n\nyear\n\n\nstring\n\n\nThe user's year.\n\n\n\n\n\n\nlecture\n\n\nstring\n\n\nThe user's assigned lecture.\n\n\n\n\n\n\nsection\n\n\nstring\n\n\nThe user's assigned section.\n\n\n\n\n\n\ngrade_policy\n\n\nstring\n\n\nThe user's grade policy for this course.\n\n\n\n\n\n\nnickname\n\n\nstring\n\n\nThe user's nickname for this course.\n\n\n\n\n\n\ndropped\n\n\nboolean\n\n\nIs the user marked as dropped from this course?\n\n\n\n\n\n\nauth_level\n\n\nstring\n\n\nThe user's level of access for this course. One of 'student', 'course_assistant', or 'instructor'.\n\n\n\n\n\n\n\n\nThere are five endpoints related to course_user_data:\n\n\nIndex\n\n\nList all course_user_data of a course.\n\n\nScope:\n 'instructor_all'\n\n\nEndpoint:\n \nGET /courses/{course_name}/course_user_data\n\n\nParameters:\n [none]\n\n\nResponse:\n\n\nA list of course_user_data objects.\n\n\nShow\n\n\nShow the course_user_data of a particular student in a course.\n\n\nScope:\n 'instructor_all'\n\n\nEndpoint:\n \nGET /courses/{course_name}/course_user_data/{user_email}\n\n\nParameters:\n [none]\n\n\nResponse:\n\n\nThe requested user's course_user_data object.\n\n\nCreate\n\n\nCreate a new course_user_data for a course.\n\n\nThe user's email is used to uniquely identify the user on Autolab. If the user is not yet a user of Autolab, they need to be registered on Autolab before they can be enrolled in any courses.\n\n\nScope:\n 'instructor_all'\n\n\nEndpoint:\n \nPOST /courses/{course_name}/course_user_data\n\n\nParameters:\n\n\n\n\n\n\n\n\nkey\n\n\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nemail\n\n\nrequired\n\n\nstring\n\n\nThe email of the user (to uniquely identify the user).\n\n\n\n\n\n\nlecture\n\n\nrequired\n\n\nstring\n\n\nThe lecture to assign the user to.\n\n\n\n\n\n\nsection\n\n\nrequired\n\n\nstring\n\n\nThe section to assign the user to.\n\n\n\n\n\n\ngrade_policy\n\n\n\n\nstring\n\n\nThe user's grade policy (opaque to Autolab).\n\n\n\n\n\n\ndropped\n\n\n\n\nboolean\n\n\nShould the user be marked as dropped?\n\n\n\n\n\n\nnickname\n\n\n\n\nstring\n\n\nThe nickname to give the user.\n\n\n\n\n\n\nauth_level\n\n\nrequired\n\n\nstring\n\n\nThe level of access this user has for this course. One of 'student', 'course_assistant', or 'instructor'.\n\n\n\n\n\n\n\n\nResponse:\n\n\nThe newly created course_user_data object.\n\n\nUpdate\n\n\nUpdate an existing course_user_data.\n\n\nScope:\n 'instructor_all'\n\n\nEndpoint:\n \nPUT /courses/{course_name}/course_user_data/{user_email}\n\n\nParameters:\n\n\n\n\n\n\n\n\nkey\n\n\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nlecture\n\n\n\n\nstring\n\n\nThe lecture to assign the user to.\n\n\n\n\n\n\nsection\n\n\n\n\nstring\n\n\nThe section to assign the user to.\n\n\n\n\n\n\ngrade_policy\n\n\n\n\nstring\n\n\nThe user's grade policy (opaque to Autolab).\n\n\n\n\n\n\ndropped\n\n\n\n\nboolean\n\n\nShould the user be marked as dropped?\n\n\n\n\n\n\nnickname\n\n\n\n\nstring\n\n\nThe nickname to give the user.\n\n\n\n\n\n\nauth_level\n\n\n\n\nstring\n\n\nThe level of access this user has for this course. One of 'student', 'course_assistant', or 'instructor'.\n\n\n\n\n\n\n\n\nResponse:\n\n\nThe newly updated course_user_data object.\n\n\nDestroy\n\n\nDrop a user from a course. Since CUDs are never deleted from the course, this is just a shortcut for updating a user with the dropped attribute set to true.\n\n\nScope:\n 'instructor_all'\n\n\nEndpoint:\n \nDELETE /courses/{course_name}/course_user_data/{user_email}\n\n\nParameters:\n [none]\n\n\nResponse:\n\n\nThe newly updated course_user_data object.", 
            "title": "Reference"
        }, 
        {
            "location": "/api-interface/#format", 
            "text": "All endpoints expect the HTTP GET method unless otherwise specified.  All parameters listed below are required unless denoted [OPTIONAL].  All responses are in JSON format.   If the request is completed successfully, the HTTP response code will be 200. The reference below details the keys and their respective value types that the client can expect from each endpoint.  If an error occurs, the response code will  not  be 200. The returned JSON will be an object with the key 'error'. Its value will be a string that explains the error.   Notes on return value types  All datetime formats are strings in the form of 'YYYY-MM-DDThh:mm:ss.sTZD'. \ne.g. '2017-10-23T04:17:41.000-04:00', which means 4:17:41 AM on October 23rd, 2017 US Eastern Time.  JSON spec only has a 'number' type, but the spec below distinguishes between integers and floats for ease of use in certain languages.  If a field does not exist, the value is generally null. Please be sure to check if a value is null before using it.", 
            "title": "Format"
        }, 
        {
            "location": "/api-interface/#interface", 
            "text": "", 
            "title": "Interface"
        }, 
        {
            "location": "/api-interface/#user", 
            "text": "Get basic user info.  Scope:  'user_info'  Endpoint:   /user  Parameters:  [none]  Responses:     key  type  description      first_name  string  The user's first name.    last_name  string  The user's last name.    email  string  The user's registered email.    school  string  The school the user belongs to.    major  string  The user's major of study.    year  string  The user's year.", 
            "title": "user"
        }, 
        {
            "location": "/api-interface/#courses", 
            "text": "Get all courses currently taking or taken before.  Scope:  'user_courses'  Endpoint:   /courses  Parameters:   state \n  [OPTIONAL] filter the courses by the state of the course. Should be one of 'disabled', 'completed', 'current', or 'upcoming'. If no state is provided, all courses are returned.   Responses:  A list of courses. Each course contains:     key  type  description      name  string  The unique url-safe name.    display_name  string  The full name of the course.    semester  string  The semester this course is being offered.    late_slack  integer  The number of seconds after a deadline that the server will still accept a submission and not count it as late.    grace_days  integer  AKA late days. The total number of days (over the entire semester) a student is allowed to submit an assessment late.    auth_level  string  The user's level of access for this course. One of 'student', 'course_assistant', or 'instructor'.", 
            "title": "courses"
        }, 
        {
            "location": "/api-interface/#assessments", 
            "text": "Get all the assessments of a course.  Scope:  'user_courses'  Endpoint:   /courses/{course_name}/assessments  Parameters:  [none]  Responses:  A list of assessments. If the user is only a student of the course, only released assessments are available. Otherwise, all assessments are available. Each assessment contains:     key  type  description      name  string  The unique url-safe name.    display_name  string  The full name of the assessments.    start_at  datetime  The time this assessment is released to students.    due_at  datetime  Students can submit before this time without being penalized or using grace days.    end_at  datetime  Last possible time that students can submit (except those granted extensions.)    category_name  string  Name of the category this assessment belongs to.    grading_deadline  string  Not available if the user is a student. Time after which final scores are included in the gradebook.", 
            "title": "assessments"
        }, 
        {
            "location": "/api-interface/#assessment-details", 
            "text": "Show detailed information of an assessment.  Scope:  'user_courses'  Endpoint:   /courses/{course_name}/assessments/{assessment_name}  Parameters:  [none]  Response:     key  type  description      name  string  The unique url-safe name.    display_name  string  The full name of the assessments.    description  string  A short description of the assessment.    start_at  datetime  The time this assessment is released to students.    due_at  datetime  Students can submit before this time without being penalized or using grace days.    end_at  datetime  Last possible time that students can submit (except those granted extensions.)    updated_at  datetime  The last time an update was made to the assessment.    max_grace_days  integer  Maximum number of grace days that a student can spend on this assessment.    max_submissions  integer  The maximum number of times a student can submit the assessment. -1 means unlimited submissions.    disable_handins  boolean  Are handins disallowed by students?    category_name  string  Name of the category this assessment belongs to.    group_size  integer  The maximum size of groups for this assessment.    writeup_format  string  The format of this assessment's writeup. One of 'none', 'url', or 'file'.    handout_format  string  The format of this assessment's handout. One of 'none', 'url', or 'file'.    has_scoreboard  boolean  Does this assessment have a scoreboard?    has_autograder  boolean  Does this assessment use an autograder?    grading_deadline  string  Not available if the user is a student. Time after which final scores are included in the gradebook.", 
            "title": "assessment details"
        }, 
        {
            "location": "/api-interface/#problems", 
            "text": "Get all problems of an assessment.  Scope:  'user_courses'  Endpoint  /courses/{course_name}/assessments/{assessment_name}/problems  Parameters:  [none]  Responses:  A list of problems. Each problem contains:     key  type  description      name  string  Full name of the problem.    description  string  Brief description of the problem.    max_score  float  Maximum possible score for this problem.    optional  boolean  Is this problem optional?", 
            "title": "problems"
        }, 
        {
            "location": "/api-interface/#writeup", 
            "text": "Get the writeup of an assessment.  Scope:  'user_courses'  Endpoint:   /courses/{course_name}/assessments/{assessment_name}/writeup  Parameters:  [none]  Responses:   If no writeup exists:      key  type  value      writeup  string  \"none\"      If writeup is a url:      key  type  description      url  string  The url of the writeup.      If writeup is a file: \nThe file is returned.", 
            "title": "writeup"
        }, 
        {
            "location": "/api-interface/#handout", 
            "text": "Get the handout of an assessment.  Scope:  'user_courses'  Endpoint:   /courses/{course_name}/assessments/{assessment_name}/handout  Parameters:  [none]  Responses:  [same as  writeup ]", 
            "title": "handout"
        }, 
        {
            "location": "/api-interface/#submit", 
            "text": "Make a submission to an assessment.  Scope:  'user_submit'  Endpoint:   POST /courses/{course_name}/assessments/{assessment_name}/submit  Parameters:   submission[file] \n  The file to submit \n   Note: the name should be the string 'submission[file]'   Success Response:     key  type  description      version  integer  The version number of the newly submitted submission.    filename  string  The final filename the submitted file is referred to as.     Failure Response:  A valid submission request may still fail for many reasons, such as file too large, handins disabled by staff, deadline has passed, etc.  When a submission fails, the HTTP response code will not be 200. The response body will include a json with the key 'error'. Its contents will be a user-friendly string that the client may display to the user to explain why the submission has failed. The client  must not  repeat the request without any modifications. The client is  not  expected to be able to handle the error automatically.", 
            "title": "submit"
        }, 
        {
            "location": "/api-interface/#submissions", 
            "text": "Get all submissions the user has made via this client. To protect the user's private scores and feedback, only the submissions made via your client is returned. i.e. submissions made through other clients or on the Autolab website are never returned.  Scope:  'user_scores'  Endpoint:   /courses/{course_name}/assessments/{assessment_name}/submissions  Parameters:  [none]  Response:  A list of submissions. Each submission includes:     key  type  description      version  integer  The version number of this submission.    filename  string  The final filename the submitted file is referred to as.    created_at  datetime  The time this submission was made.    scores  object  A dictionary containing the scores of each problem. The keys are the names of the problems, and the value is either the score (a float), or the string 'unreleased' if the score for this problem is not yet released.", 
            "title": "submissions"
        }, 
        {
            "location": "/api-interface/#feedback", 
            "text": "Get the text feedback given to a problem of a submission.  For autograded assessments, the feedback will by default be the autograder feedback, and will be identical for all problems.  Scope:  'user_scores'  Endpoint:   /courses/{course_name}/assessments/{assessment_name}/submissions/{submission_version}/feedback  Parameters:   problem \n  The name of the problem that the feedback is given to.   Response:     key  type  description      feedback  string  The full feedback text for this problem.", 
            "title": "feedback"
        }, 
        {
            "location": "/api-interface/#course_user_data-enrollments", 
            "text": "Autolab uses the term course_user_data to represent the users affiliated with a course. It includes all students, course assistants, and instructors of the course.  A course_user_data object in the response will be formatted in this form:     key  type  description      first_name  string  The user's first name.    last_name  string  The user's last name.    email  string  The user's registered email.    school  string  The school the user belongs to.    major  string  The user's major of study.    year  string  The user's year.    lecture  string  The user's assigned lecture.    section  string  The user's assigned section.    grade_policy  string  The user's grade policy for this course.    nickname  string  The user's nickname for this course.    dropped  boolean  Is the user marked as dropped from this course?    auth_level  string  The user's level of access for this course. One of 'student', 'course_assistant', or 'instructor'.     There are five endpoints related to course_user_data:", 
            "title": "course_user_data (enrollments)"
        }, 
        {
            "location": "/api-interface/#index", 
            "text": "List all course_user_data of a course.  Scope:  'instructor_all'  Endpoint:   GET /courses/{course_name}/course_user_data  Parameters:  [none]  Response:  A list of course_user_data objects.", 
            "title": "Index"
        }, 
        {
            "location": "/api-interface/#show", 
            "text": "Show the course_user_data of a particular student in a course.  Scope:  'instructor_all'  Endpoint:   GET /courses/{course_name}/course_user_data/{user_email}  Parameters:  [none]  Response:  The requested user's course_user_data object.", 
            "title": "Show"
        }, 
        {
            "location": "/api-interface/#create", 
            "text": "Create a new course_user_data for a course.  The user's email is used to uniquely identify the user on Autolab. If the user is not yet a user of Autolab, they need to be registered on Autolab before they can be enrolled in any courses.  Scope:  'instructor_all'  Endpoint:   POST /courses/{course_name}/course_user_data  Parameters:     key   type  description      email  required  string  The email of the user (to uniquely identify the user).    lecture  required  string  The lecture to assign the user to.    section  required  string  The section to assign the user to.    grade_policy   string  The user's grade policy (opaque to Autolab).    dropped   boolean  Should the user be marked as dropped?    nickname   string  The nickname to give the user.    auth_level  required  string  The level of access this user has for this course. One of 'student', 'course_assistant', or 'instructor'.     Response:  The newly created course_user_data object.", 
            "title": "Create"
        }, 
        {
            "location": "/api-interface/#update", 
            "text": "Update an existing course_user_data.  Scope:  'instructor_all'  Endpoint:   PUT /courses/{course_name}/course_user_data/{user_email}  Parameters:     key   type  description      lecture   string  The lecture to assign the user to.    section   string  The section to assign the user to.    grade_policy   string  The user's grade policy (opaque to Autolab).    dropped   boolean  Should the user be marked as dropped?    nickname   string  The nickname to give the user.    auth_level   string  The level of access this user has for this course. One of 'student', 'course_assistant', or 'instructor'.     Response:  The newly updated course_user_data object.", 
            "title": "Update"
        }, 
        {
            "location": "/api-interface/#destroy", 
            "text": "Drop a user from a course. Since CUDs are never deleted from the course, this is just a shortcut for updating a user with the dropped attribute set to true.  Scope:  'instructor_all'  Endpoint:   DELETE /courses/{course_name}/course_user_data/{user_email}  Parameters:  [none]  Response:  The newly updated course_user_data object.", 
            "title": "Destroy"
        }, 
        {
            "location": "/command-line-interface/", 
            "text": "Autolab Command Line Interface\n\n\nTo help showcase the capabilities of the API, we developed autolab-cli: A first-party command line client that serves as both a practical tool for users of Autolab, as well as a reference design for developers intending to use the API in their own programs. The cli includes features like downloading and submitting assignments from the terminal, viewing problems, and getting submission feedback.\n\n\n\n\nNote to CMU Students:\n\n\nThis cli binary has already been installed on the andrew machines as \nautolab\n.\n\n\n\n\nObtaining authorization\n\n\nMake sure you have the cli installed by running \nautolab\n in your terminal. If you see the usage instructions you're good to go. Otherwise, ask your school admin to install the cli from the \nAutolab CLI Repository\n.\n\n\n\n\nTo setup autolab-cli with your Autolab account, run \nautolab setup\n. This will initiate a manual activation.\n\n\n\n\n\n\nWhat you'll see when you run \nautolab setup\n\n\nOnce you approve the client on the Autolab website, the client will respond telling you that authorization was successful. You should be able to use the client from now on. If at any point you want to reset the client, run \nautolab setup -f\n and you'll be asked to re-authorize the client from a clean state. To deauthorize any client that you've given permission to, look at how to \nManage Authorized Apps\n.\n\n\nViewing your courses and assessments\n\n\n\n\nTo view your current courses, run  \n\n\n\n\n$ autolab courses\n\n\n\n\n\n\nThis will show you a list of ongoing courses in the form \nunique_name (Display name)\n. You should use the 'unique_name' of each course when interacting with autolab-cli.\n\n\n\n\nTo view the assessments of a course, run  \n\n\n\n\n$ autolab asmts \ncourse_unique_name\n\n\n\n\nThis will show you a list of assessments in the same \nunique_name (Display name)\n format.\n\n\nDownloading an assessment\n\n\n\n\nTo start working on an assessment, go to a directory where you usually put your work, and run  \n\n\n\n\n$ autolab download \ncourse_unique_name\n:\nasmt_unique_name\n\n\n\n\n\n\nThis will create a directory with the assessment name in your current directory, and download the handout and writeup in it. This new directory is called an 'assessment directory'. Whenever you're inside an assessment directory, autolab-cli will respond according to the context.\n\n\nFor example, when you're inside an assessment directory, you can run  \n\n\n$ autolab problems\n\n\n\n\nThis will show you the problems of this assessment.\n\n\nSubmitting solutions\n\n\n\n\nTo submit to an assessment inside an assessment directory, run  \n\n\n\n\n$ autolab submit \nfilename\n\n\n\n\n\n\nYep, it's that easy.\n\n\nViewing scores\n\n\n\n\nTo view the scores you got, run  \n\n\n\n\n$ autolab scores\n\n\n\n\n\n\nThe scores command will only return scores for those submissions that are made via this client. This is a privacy constraint of the Autolab API.\n\n\n\n\nTo view the feedback you got, run  \n\n\n\n\n$ autolab feedback\n\n\n\n\nAdvanced features\n\n\n\n\nYou can learn more about each sub-command by running  \n\n\n\n\n$ autolab \nsub-command\n -h\n\n\n\n\nThis will reveal other flags you may be able to use with each command. For example, you can call all of the context-dependent commands outside of an assessment directory by providing the \ncourse_unique_name\n:\nasmt_unique_name\n pair.\n\n\nWe hope this speeds up your workflow! If you find any problems, please file an issue on the \nAutolab CLI Repository\n.", 
            "title": "Command Line Interface"
        }, 
        {
            "location": "/command-line-interface/#autolab-command-line-interface", 
            "text": "To help showcase the capabilities of the API, we developed autolab-cli: A first-party command line client that serves as both a practical tool for users of Autolab, as well as a reference design for developers intending to use the API in their own programs. The cli includes features like downloading and submitting assignments from the terminal, viewing problems, and getting submission feedback.   Note to CMU Students:  This cli binary has already been installed on the andrew machines as  autolab .", 
            "title": "Autolab Command Line Interface"
        }, 
        {
            "location": "/command-line-interface/#obtaining-authorization", 
            "text": "Make sure you have the cli installed by running  autolab  in your terminal. If you see the usage instructions you're good to go. Otherwise, ask your school admin to install the cli from the  Autolab CLI Repository .   To setup autolab-cli with your Autolab account, run  autolab setup . This will initiate a manual activation.    What you'll see when you run  autolab setup  Once you approve the client on the Autolab website, the client will respond telling you that authorization was successful. You should be able to use the client from now on. If at any point you want to reset the client, run  autolab setup -f  and you'll be asked to re-authorize the client from a clean state. To deauthorize any client that you've given permission to, look at how to  Manage Authorized Apps .", 
            "title": "Obtaining authorization"
        }, 
        {
            "location": "/command-line-interface/#viewing-your-courses-and-assessments", 
            "text": "To view your current courses, run     $ autolab courses   This will show you a list of ongoing courses in the form  unique_name (Display name) . You should use the 'unique_name' of each course when interacting with autolab-cli.   To view the assessments of a course, run     $ autolab asmts  course_unique_name  This will show you a list of assessments in the same  unique_name (Display name)  format.", 
            "title": "Viewing your courses and assessments"
        }, 
        {
            "location": "/command-line-interface/#downloading-an-assessment", 
            "text": "To start working on an assessment, go to a directory where you usually put your work, and run     $ autolab download  course_unique_name : asmt_unique_name   This will create a directory with the assessment name in your current directory, and download the handout and writeup in it. This new directory is called an 'assessment directory'. Whenever you're inside an assessment directory, autolab-cli will respond according to the context.  For example, when you're inside an assessment directory, you can run    $ autolab problems  This will show you the problems of this assessment.", 
            "title": "Downloading an assessment"
        }, 
        {
            "location": "/command-line-interface/#submitting-solutions", 
            "text": "To submit to an assessment inside an assessment directory, run     $ autolab submit  filename   Yep, it's that easy.", 
            "title": "Submitting solutions"
        }, 
        {
            "location": "/command-line-interface/#viewing-scores", 
            "text": "To view the scores you got, run     $ autolab scores   The scores command will only return scores for those submissions that are made via this client. This is a privacy constraint of the Autolab API.   To view the feedback you got, run     $ autolab feedback", 
            "title": "Viewing scores"
        }, 
        {
            "location": "/command-line-interface/#advanced-features", 
            "text": "You can learn more about each sub-command by running     $ autolab  sub-command  -h  This will reveal other flags you may be able to use with each command. For example, you can call all of the context-dependent commands outside of an assessment directory by providing the  course_unique_name : asmt_unique_name  pair.  We hope this speeds up your workflow! If you find any problems, please file an issue on the  Autolab CLI Repository .", 
            "title": "Advanced features"
        }, 
        {
            "location": "/api-managing-authorized-apps/", 
            "text": "Managing Authorized Apps\n\n\nWith the advent of the API, developers can now create new, more versatile and convenient ways of accessing Autolab. \n\n\nWhat this means for users is that you can now use third-party programs to access Autolab to view assignments, download handouts, and even submit your solutions. Rest assured that all developers and their clients will be manually vetted by our team to ensure quality and safety. However, it is still important that you understand how clients interact with your account.\n\n\nTerminology\n\n\n\n\nuser: a user of autolab (student/instructor)\n\n\nclient: a program that uses the autolab api\n\n\ndeveloper: a person that develops clients\n\n\n\n\nGranting access\n\n\nAs a user of Autolab, when you want to use a client for the first time, you need to grant access to the client so that it can interact with Autolab for you.\n\n\n\n\nEasy Activation\n: Clients that have access to a web browser (e.g. mobile apps, web apps) will \nredirect\n the user directly to the Grant Permissions page on Autolab.\n\n\nManual Activation\n: Clients that \ndon't\n have access to a web browser (e.g. command line programs) will present to the user a \n6-digit code\n (case sensitive) that should be entered on the Autolab website.\n\n\n\n\nNote\n: Third-party clients never ask for your Autolab username or password. Never enter them anywhere else except on the Autolab website (always check the page url before entering your credentials).\n\n\n\n\nManual activation page\n\n\nWhen you enter the code on the website and click \"Activate\", you will be taken to the Grant Permissions page.\n\n\n\n\nAPI Grant Permissions Page\n\n\nThis page shows you all the permissions the client requests. Click 'approve' to grant these permissions to this client.\n\n\nReviewing your authorized clients\n\n\nAs a user, you can review all the clients that you've granted access to on the Manage Authorized Clients page. Click on the menu at the upper right corner, then click on 'Account'. At the bottom of the page you'll find the 'Manage Authorized Clients' link.\n\n\n\n\nManage all the clients that currently have access to your account\n\n\nYou can view the permissions that each client has (hover over the icon to see a description of each permission). You can also click 'Revoke' at any time to revoke the access of a client immediately.", 
            "title": "Managing Authorized Apps"
        }, 
        {
            "location": "/api-managing-authorized-apps/#managing-authorized-apps", 
            "text": "With the advent of the API, developers can now create new, more versatile and convenient ways of accessing Autolab.   What this means for users is that you can now use third-party programs to access Autolab to view assignments, download handouts, and even submit your solutions. Rest assured that all developers and their clients will be manually vetted by our team to ensure quality and safety. However, it is still important that you understand how clients interact with your account.", 
            "title": "Managing Authorized Apps"
        }, 
        {
            "location": "/api-managing-authorized-apps/#terminology", 
            "text": "user: a user of autolab (student/instructor)  client: a program that uses the autolab api  developer: a person that develops clients", 
            "title": "Terminology"
        }, 
        {
            "location": "/api-managing-authorized-apps/#granting-access", 
            "text": "As a user of Autolab, when you want to use a client for the first time, you need to grant access to the client so that it can interact with Autolab for you.   Easy Activation : Clients that have access to a web browser (e.g. mobile apps, web apps) will  redirect  the user directly to the Grant Permissions page on Autolab.  Manual Activation : Clients that  don't  have access to a web browser (e.g. command line programs) will present to the user a  6-digit code  (case sensitive) that should be entered on the Autolab website.   Note : Third-party clients never ask for your Autolab username or password. Never enter them anywhere else except on the Autolab website (always check the page url before entering your credentials).   Manual activation page  When you enter the code on the website and click \"Activate\", you will be taken to the Grant Permissions page.   API Grant Permissions Page  This page shows you all the permissions the client requests. Click 'approve' to grant these permissions to this client.", 
            "title": "Granting access"
        }, 
        {
            "location": "/api-managing-authorized-apps/#reviewing-your-authorized-clients", 
            "text": "As a user, you can review all the clients that you've granted access to on the Manage Authorized Clients page. Click on the menu at the upper right corner, then click on 'Account'. At the bottom of the page you'll find the 'Manage Authorized Clients' link.   Manage all the clients that currently have access to your account  You can view the permissions that each client has (hover over the icon to see a description of each permission). You can also click 'Revoke' at any time to revoke the access of a client immediately.", 
            "title": "Reviewing your authorized clients"
        }, 
        {
            "location": "/tango/", 
            "text": "Tango\n\n\nTango is a standalone RESTful Web service that runs jobs in virtual machines or containers. It was developed as a distributed grading system for \nAutolab\n and has been extensively used for autograding programming assignments. It is also open source and hosted on \nGithub\n.\n\n\nGetting Started\n\n\nA brief overview of the Tango respository:\n\n\n\n\ntango.py\n - Main tango server\n\n\njobQueue.py\n - Manages the job queue\n\n\njobManager.py\n - Assigns jobs to free VMs\n\n\nworker.py\n - Shepherds a job through its execution\n\n\npreallocator.py\n - Manages pools of VMs\n\n\nvmms/\n - VMMS library implementations\n\n\nrestful-tango/\n - HTTP server layer on the main Tango\n\n\n\n\nTango runs jobs in VMs using a high level Virtual Memory Management System (VMMS) API. Tango currently has support for running jobs in \nDocker\n containers (\nrecommended\n), \nTashi VMs\n, or \nAmazon EC2\n.\n\n\nFor more information about the different Tango components, go to the following pages:\n\n\n\n\nREST API docs\n\n\nVMMS API docs\n\n\nTango Architecture Overview\n\n\nDeploying Tango\n\n\n\n\nInstallation\n\n\nThis guide shows how to setup Tango in a \ndevelopment environment\n. Use the \ndeploying Tango\n guide for installing in a \nproduction environment\n. \n\n\n\n\n\n\nObtain the source code.\n\n\ngit clone https://github.com/autolab/Tango.git; cd Tango\n\n\n\n\n\n\n\n\nInstall Redis following \nthis guide\n. By default, Tango uses Redis as a stateless job queue. Learn more \nhere\n.\n\n\n\n\n\n\nCreate a \nconfig.py\n file from the given template. \n\n\ncp config.template.py config.py\n\n\n\n\n\n\n\n\nCreate the course labs directory where job's output files will go, organized by key and lab name:\n\n\nmkdir courselabs\n\n\n\n\nBy default the \nCOURSELABS\n option in \nconfig.py\n points to the \ncourselabs\n directory in the Tango directory.\n   Change this to specify another path if you wish.\n\n\n\n\n\n\nSet up a VMMS for Tango to use. \n\n\n\n\nDocker\n (\nrecommended\n)\n\n\nAmazon EC2\n\n\nTashiVMMS (deprecated)\n\n\n\n\n\n\n\n\nRun the following commands to setup the Tango dev environment inside the Tango directory. \nInstall pip\n if needed.\n\n\n$ pip install virtualenv\n$ virtualenv .\n$ source bin/activate\n$ pip install -r requirements.txt\n$ mkdir volumes\n\n\n\n\n\n\n\n\nStart Redis by running the following command:\n\n\n$ redis-server\n\n\n\n\n\n\n\n\nRun the following command to start the server (producer). If no port is given, the server will run on the port specified in \nconfig.py\n (default: 3000):\n\n\npython restful-tango/server.py \nport\n\n\n\n\nOpen another terminal window and start the job manager (consumer):\n\n\npython jobManager.py\n\n\n\n\nFor more information on the job producer/consumer model check out our \nblog post\n\n\n\n\n\n\nEnsure Tango is running:\n\n\n$ curl localhost:\nport\n\n# Hello, world! RESTful Tango here!\n\n\n\n\n\n\n\n\nYou can test the Tango setup using the \ncommand line client\n.\n\n\n\n\n\n\nIf you are using Tango with Autolab, you have to configure Autolab to use Tango. Go to your Autolab directory and enter the following commands:\n\n\ncp config/autogradeConfig.rb.template config/autogradeConfig.rb\n\n\n\n\nFill in the correct info for your Tango deployment, mainly the following:\n\n\n# Hostname for Tango RESTful API\nRESTFUL_HOST = \nfoo.bar.edu\n\n\n# Port for Tango RESTful API\nRESTFUL_PORT = \n3000\n\n\n# Key for Tango RESTful API\nRESTFUL_KEY = \ntest\n\n\n\n\n\n\n\n\nTo deploy Tango in a standalone production environment, use this \nguide", 
            "title": "Getting Started"
        }, 
        {
            "location": "/tango/#tango", 
            "text": "Tango is a standalone RESTful Web service that runs jobs in virtual machines or containers. It was developed as a distributed grading system for  Autolab  and has been extensively used for autograding programming assignments. It is also open source and hosted on  Github .", 
            "title": "Tango"
        }, 
        {
            "location": "/tango/#getting-started", 
            "text": "A brief overview of the Tango respository:   tango.py  - Main tango server  jobQueue.py  - Manages the job queue  jobManager.py  - Assigns jobs to free VMs  worker.py  - Shepherds a job through its execution  preallocator.py  - Manages pools of VMs  vmms/  - VMMS library implementations  restful-tango/  - HTTP server layer on the main Tango   Tango runs jobs in VMs using a high level Virtual Memory Management System (VMMS) API. Tango currently has support for running jobs in  Docker  containers ( recommended ),  Tashi VMs , or  Amazon EC2 .  For more information about the different Tango components, go to the following pages:   REST API docs  VMMS API docs  Tango Architecture Overview  Deploying Tango", 
            "title": "Getting Started"
        }, 
        {
            "location": "/tango/#installation", 
            "text": "This guide shows how to setup Tango in a  development environment . Use the  deploying Tango  guide for installing in a  production environment .     Obtain the source code.  git clone https://github.com/autolab/Tango.git; cd Tango    Install Redis following  this guide . By default, Tango uses Redis as a stateless job queue. Learn more  here .    Create a  config.py  file from the given template.   cp config.template.py config.py    Create the course labs directory where job's output files will go, organized by key and lab name:  mkdir courselabs  By default the  COURSELABS  option in  config.py  points to the  courselabs  directory in the Tango directory.\n   Change this to specify another path if you wish.    Set up a VMMS for Tango to use.    Docker  ( recommended )  Amazon EC2  TashiVMMS (deprecated)     Run the following commands to setup the Tango dev environment inside the Tango directory.  Install pip  if needed.  $ pip install virtualenv\n$ virtualenv .\n$ source bin/activate\n$ pip install -r requirements.txt\n$ mkdir volumes    Start Redis by running the following command:  $ redis-server    Run the following command to start the server (producer). If no port is given, the server will run on the port specified in  config.py  (default: 3000):  python restful-tango/server.py  port  Open another terminal window and start the job manager (consumer):  python jobManager.py  For more information on the job producer/consumer model check out our  blog post    Ensure Tango is running:  $ curl localhost: port \n# Hello, world! RESTful Tango here!    You can test the Tango setup using the  command line client .    If you are using Tango with Autolab, you have to configure Autolab to use Tango. Go to your Autolab directory and enter the following commands:  cp config/autogradeConfig.rb.template config/autogradeConfig.rb  Fill in the correct info for your Tango deployment, mainly the following:  # Hostname for Tango RESTful API\nRESTFUL_HOST =  foo.bar.edu \n\n# Port for Tango RESTful API\nRESTFUL_PORT =  3000 \n\n# Key for Tango RESTful API\nRESTFUL_KEY =  test    To deploy Tango in a standalone production environment, use this  guide", 
            "title": "Installation"
        }, 
        {
            "location": "/tango-cli/", 
            "text": "Tango Command Line Client\n\n\nThis is a guide to use the command-line client (\nclients/tango-cli.py\n) to test and collect other valuable information from Tango. Please \nsetup Tango\n before moving forward. This guide assumes an instance of Tango is already up and running.\n\n\nRunning a Sample Job\n\n\nThe CLI supports two ways to run a sample job, \nindividual steps\n or in a \nsingle all-in-one command\n. The first option is better for debugging each individual API call, whereas the second option is best for quickly running a job. Other Tango CLI commands are also discussed \nbelow\n.\n\n\nThe Tango directory contains various different jobs in the \nclients/\n directory; \nclients/README.md\n discusses the function of each job.  \n\n\nFind out more information about the Tango REST API \nhere\n.\n\n\nSingle Command\n\n\nThe \n--runJob\n command simply runs a job from a directory of files by uploading all the files in the directory. You can use this to submit an autograding job by running\n\n\n$ python clients/tango-cli.py -P 3000 -k test -l assessment1 --runJob clients/job1/ --image autograding_image\n\n\n\n\nThe args are -P \nport>, -k \nkey>, -l \nunique_job_name> --runJob \njob_files_path> --image \nautograde_image>\n\n\nIndividual Steps\n\n\n\n\n\n\nOpen a \ncourselab\n on Tango. This will create a directory for tango to store the files for the job. \n\n\n$ python clients/tango-cli.py -P \nport\n -k \nkey\n -l \ncourselab\n --open\n\n\n\n\n\n\n\n\nUpload files necessary for the job.\n\n\n$ python clients/tango-cli.py -P \nport\n -k \nkey\n -l \ncourselab\n \\\n    --upload --filename \nclients/job1/hello.sh\n\n$ python clients/tango-cli.py -P \nport\n -k \nkey\n -l \ncourselab\n \\\n    --upload --filename \nclients/job1/autograde-Makefile\n\n\n\n\n\n\n\n\nAdd the job to the queue. Note: \nlocalFile\n is the name of the file that was uploaded and \ndestFile\n is the name of the file that will be on the VM. One of the \ndestFile\n attributes must be \nMakefile\n. Furthermore, \nimage\n references the name of the VM image you want the job to be run on. For Docker it is \nautograding_image\n.\n\n\n$ python clients/tango-cli.py -P \nport\n -k \nkey\n -l \ncourselab\n \\\n    --addJob --infiles \\\n    '{\nlocalFile\n : \nhello.sh\n, \ndestFile\n : \nhello.sh\n}' \\\n    '{\nlocalFile\n : \nautograde-Makefile\n, \ndestFile\n : \nMakefile\n}' \\\n    --image \nimage\n --outputFile \noutputFileName\n \\\n    --jobname \njobname\n --maxsize \nmaxOutputSize\n --timeout \njobTimeout\n\n\n\n\n\n\n\n\nGet the job output.\n\n\n$ python clients/tango-cli.py -P \nport\n -k \nkey\n -l \ncourselab\n \\\n    --poll --outputFile \noutputFileName\n\n\n\n\nThe output file will have the following header:\n\n\nAutograder [\ndate-time\n]: Received job \njobname\n:\njobid\n\nAutograder [\ndate-time\n]: Success: Autodriver returned normally\nAutograder [\ndate-time\n]: Here is the output from the autograder:\n---\n...\n\n\n\n\n\n\n\n\nMiscellaneous Commands\n\n\nThe CLI also implements a list of commands to invoke the \nTango REST API\n, including \n--info\n, \n--prealloc\n, and \n--jobs\n. For a full list of commands, run:\n\n\npython clients/tango-cli.py --help\n\n\n\n\nThe general form for each command is as follows:\n\n\npython clients/tango-cli.py -P \nport\n -k \nkey\n \ncommand", 
            "title": "Tango CLI"
        }, 
        {
            "location": "/tango-cli/#tango-command-line-client", 
            "text": "This is a guide to use the command-line client ( clients/tango-cli.py ) to test and collect other valuable information from Tango. Please  setup Tango  before moving forward. This guide assumes an instance of Tango is already up and running.", 
            "title": "Tango Command Line Client"
        }, 
        {
            "location": "/tango-cli/#running-a-sample-job", 
            "text": "The CLI supports two ways to run a sample job,  individual steps  or in a  single all-in-one command . The first option is better for debugging each individual API call, whereas the second option is best for quickly running a job. Other Tango CLI commands are also discussed  below .  The Tango directory contains various different jobs in the  clients/  directory;  clients/README.md  discusses the function of each job.    Find out more information about the Tango REST API  here .", 
            "title": "Running a Sample Job"
        }, 
        {
            "location": "/tango-cli/#single-command", 
            "text": "The  --runJob  command simply runs a job from a directory of files by uploading all the files in the directory. You can use this to submit an autograding job by running  $ python clients/tango-cli.py -P 3000 -k test -l assessment1 --runJob clients/job1/ --image autograding_image  The args are -P  port>, -k  key>, -l  unique_job_name> --runJob  job_files_path> --image  autograde_image>", 
            "title": "Single Command"
        }, 
        {
            "location": "/tango-cli/#individual-steps", 
            "text": "Open a  courselab  on Tango. This will create a directory for tango to store the files for the job.   $ python clients/tango-cli.py -P  port  -k  key  -l  courselab  --open    Upload files necessary for the job.  $ python clients/tango-cli.py -P  port  -k  key  -l  courselab  \\\n    --upload --filename  clients/job1/hello.sh \n$ python clients/tango-cli.py -P  port  -k  key  -l  courselab  \\\n    --upload --filename  clients/job1/autograde-Makefile    Add the job to the queue. Note:  localFile  is the name of the file that was uploaded and  destFile  is the name of the file that will be on the VM. One of the  destFile  attributes must be  Makefile . Furthermore,  image  references the name of the VM image you want the job to be run on. For Docker it is  autograding_image .  $ python clients/tango-cli.py -P  port  -k  key  -l  courselab  \\\n    --addJob --infiles \\\n    '{ localFile  :  hello.sh ,  destFile  :  hello.sh }' \\\n    '{ localFile  :  autograde-Makefile ,  destFile  :  Makefile }' \\\n    --image  image  --outputFile  outputFileName  \\\n    --jobname  jobname  --maxsize  maxOutputSize  --timeout  jobTimeout    Get the job output.  $ python clients/tango-cli.py -P  port  -k  key  -l  courselab  \\\n    --poll --outputFile  outputFileName  The output file will have the following header:  Autograder [ date-time ]: Received job  jobname : jobid \nAutograder [ date-time ]: Success: Autodriver returned normally\nAutograder [ date-time ]: Here is the output from the autograder:\n---\n...", 
            "title": "Individual Steps"
        }, 
        {
            "location": "/tango-cli/#miscellaneous-commands", 
            "text": "The CLI also implements a list of commands to invoke the  Tango REST API , including  --info ,  --prealloc , and  --jobs . For a full list of commands, run:  python clients/tango-cli.py --help  The general form for each command is as follows:  python clients/tango-cli.py -P  port  -k  key   command", 
            "title": "Miscellaneous Commands"
        }, 
        {
            "location": "/tango-rest/", 
            "text": "This page documents the REST API for submitting jobs to Tango. \n\n\nAuthentication\n\n\nIn order to have access to the REST interface of the Tango server, clients will first have to obtain a key from the Tango server. This key is a unique identifier of the client and it must be supplied with every HTTP request to the Tango server. If the Tango server fails to recognize the key, it does not entertain the request and returns an error message as part of the response body. \n\n\nJob Requests\n\n\nHere is a description of the requests that clients use to submit jobs:\n\n\nopen\n\n\nA request to \nopen\n consists of the client's key and an identifier for every lab, which is likely to be a combination of the course name and the lab name (i.e. \ncourselab\n for autograding jobs). \nopen\n checks if a directory for \ncourselab\n exists. If a directory for \ncourselab\n exists, a dict of MD5 hashes corresponding to every file in that directory is returned. If the directory does not exist, it is created and a folder for output files is also created within the \ncourselab\n directory. Since no files exist in the newly created directory, an empty dict of MD5 hashes is returned. \n\n\nRequest header: \nGET /open/key/courselab/\n\nRequest body: empty\n\nResponse body:\n\n\n{ \n  \nstatusMsg\n: \nstring\n, \n  \nstatusId\n: \nint\n,\n  \nfiles\n: { \nfileName1\n : \nmd5hash1\n, \nfileName2\n : \nmd5hash2\n ... },\n}\n\n\n\n\nupload\n\n\nAfter receiving a list of MD5 hashes of files that exist on the Tango server, the client can choose to upload files that are different from the ones on the Tango server via successive \nupload\n commands. For each upload, the client must supply a \nfilename\n header that gives the name of the file (on the local machine) to be uploaded to Tango. One of these files must be a Makefile, which needs to contain a rule called \nautograde\n (command to drive the autograding process). \n\n\nRequest header: \nPOST /upload/key/courselab/\n\nRequest body: \nfile\n\nResponse body: \n\n\n{ \n  \nstatusMsg\n: \nstring\n, \n  \nstatusId\n: \nint\n \n}\n\n\n\n\naddJob\n\n\nAfter uploading the appropriate files, the client uses this command to run the job for the files specified as \nfiles\n in the \ncourselab\n and on an instance of a particular VM \nimage\n. Each file has \nlocalFile\n and \ndestFile\n attributes which specify what the file is called on the Tango server and what it should be called when copied over to a VM (for autograding) respectively. Exactly one of the specified \nfiles\n should have the \ndestFile\n attribute set to \nMakefile\n, and the Makefile must contain a rule called \nautograde\n. Clients can also specify an optional timeout value (\ntimeout\n) and maximum output file size (\nmax_kb\n). This command is non-blocking and returns immediately with a status message. Additionally, the command accepts an optional parameter, \ncallback_url\n. If the \ncallback_url\n is specified, then the Tango server sends a \nPOST\n request to the \ncallback_url\n with the output file once the job is terminated. If the \ncallback_url\n is not specified, the client can then send a \npoll\n request for the \noutput_file\n to check the status of that job and retrieve the output file from the Tango server if autograding is complete.\n\n\nRequest header: \nPOST /addJob/key/courselab/\n\nRequest body:  \n\n\n{\n  \nimage\n: \nstring\n,                            # required VM image (e.g. \nrhel.img\n)\n  \nfiles\n: [ { \nlocalFile\n: \nstring\n, \n               \ndestFile\n: \nstring\n }, ...],    # required list of files to be used for autograding\n  \njobName\n: \nstring\n,                          # required name of job\n  \noutput_file\n: \nstring\n,                      # required name of output file\n  \ntimeout\n: \nint\n,                             # optional timeout value (secs)\n  \nmax_kb\n: \nint\n,                              # optional max output file size (KB)\n  \ncallback_url\n: \nstring\n                      # optional URL for POST callback from server to client\n}\n\n\n\n\nResponse body: \n\n\n{ \n  \nstatusMsg\n: \nstring\n, \n  \nstatusId\n: \nint\n, \n  \njobId\n: \nint\n \n}\n\n\n\n\npoll\n\n\nCheck if the job for \noutputFile\n has completed. If not, return \n404: Not Found\n and a status message, otherwise return the file in the response body, and free all resources held by the job. \n\n\nRequest header: \nGET /poll/key/courselab/outputFile/\n\nRequest body: \n\n\n{\n  \nempty\n\n}\n\n\n\n\nResponse body:\n\n\nautograder output file\n if autograding successful otherwise: \n\n\n{\n  \nstatusMsg\n: \nstring\n, \n  \nstatusId\n: \nint\n \n}\n\n\n\n\nAdministrative Requests\n\n\nHere are the requests that administrators use to manage the Tango service, typically from a command line client. \n\n\n/info\n\n\nThis is the \"hello, world\" request for the service. It returns a JSON object with some basic stats about the service, such as uptime, number of jobs, etc. \n\n\nRequest header: \nGET /info/\nKEY\n/\nCOURSE_LAB\n/\n\nRequest body:\n\n\n{\n  \nempty\n\n}\n\n\n\n\nResponse body:\n\n\n{\n  \ninfo\n: {\n            \nnum_threads\n: \nint\n, \n            \njob_requests\n: \nint\n, \n            \nwaitvm_timeouts\n: \nint\n, \n            \nrunjob_timeouts\n: \nint\n, \n            \nelapsed_secs\n: \nfloat\n, \n            \nrunjob_errors\n: \nint\n, \n            \njob_retries\n: \nint\n, \n            \ncopyin_errors\n: \nint\n, \n            \ncopyout_errors\n: \nint\n\n          }, \n  \nstatusMsg\n: \nFound info successfully\n, \n  \nstatusId\n: 0\n}\n\n\n\n\n/jobs\n\n\nReturn a list of jobs. If deadjobs is set to 1, then return a list of recently completed jobs. Otherwise, return the list of currently running jobs. Note: This isn't strictly an admin request, since clients might find it useful to display jobs status, as we do in the Autolab front end. \n\n\nRequest header: \nPOST autograde.me/jobs/key/deadjobs/\n\nRequest body: empty \nResponse body: JSON \njobs\n object  \n\n\npool\n\n\nReturns a JSON object that provides info about the current state of a pool of instances spawned from some \nimage\n. The response gives the total number of instances in the pool, and the number of free instances not currently allocated to any job. \n\n\nRequest header: \nGET /pool/key/image/\n\nResponse body: JSON \npool\n object  \n\n\nprealloc\n\n\nCreates a pool of \nnum\n identical instances spawned from \nimage\n (e.g. \"rhel.img). \n\n\nRequest header: \nPOST /prealloc/key/image/num/\n\nRequest body:\n\n\n{\n    \nvmms\n: \nstring\n,     # vmms to use (e.g. \nlocalSSH\n)\n    \ncores\n: \nint\n,       # number of cores per VM\n    \nmemory\n: \nint\n,      # amount of memory per VM\n}\n\n\n\n\nResponse body: \n{ \"status\": \nstring\n }\n \n\n\nImplementation Notes\n\n\nTango will maintain a directory for each of the labs in a course, which is created by \nopen\n. All output files are stored within a specified output folder in this directory. Besides the runtime job queue, no other state is necessary.\n\n\nAt job execution time, Tango will copy files specified by the \nfiles\n parameter in \naddJob\n to the VM. When the VM finishes, it will copy the output file back to the lab directory.", 
            "title": "REST API"
        }, 
        {
            "location": "/tango-rest/#authentication", 
            "text": "In order to have access to the REST interface of the Tango server, clients will first have to obtain a key from the Tango server. This key is a unique identifier of the client and it must be supplied with every HTTP request to the Tango server. If the Tango server fails to recognize the key, it does not entertain the request and returns an error message as part of the response body.", 
            "title": "Authentication"
        }, 
        {
            "location": "/tango-rest/#job-requests", 
            "text": "Here is a description of the requests that clients use to submit jobs:", 
            "title": "Job Requests"
        }, 
        {
            "location": "/tango-rest/#open", 
            "text": "A request to  open  consists of the client's key and an identifier for every lab, which is likely to be a combination of the course name and the lab name (i.e.  courselab  for autograding jobs).  open  checks if a directory for  courselab  exists. If a directory for  courselab  exists, a dict of MD5 hashes corresponding to every file in that directory is returned. If the directory does not exist, it is created and a folder for output files is also created within the  courselab  directory. Since no files exist in the newly created directory, an empty dict of MD5 hashes is returned.   Request header:  GET /open/key/courselab/ \nRequest body: empty \nResponse body:  { \n   statusMsg :  string , \n   statusId :  int ,\n   files : {  fileName1  :  md5hash1 ,  fileName2  :  md5hash2  ... },\n}", 
            "title": "open"
        }, 
        {
            "location": "/tango-rest/#upload", 
            "text": "After receiving a list of MD5 hashes of files that exist on the Tango server, the client can choose to upload files that are different from the ones on the Tango server via successive  upload  commands. For each upload, the client must supply a  filename  header that gives the name of the file (on the local machine) to be uploaded to Tango. One of these files must be a Makefile, which needs to contain a rule called  autograde  (command to drive the autograding process).   Request header:  POST /upload/key/courselab/ \nRequest body:  file \nResponse body:   { \n   statusMsg :  string , \n   statusId :  int  \n}", 
            "title": "upload"
        }, 
        {
            "location": "/tango-rest/#addjob", 
            "text": "After uploading the appropriate files, the client uses this command to run the job for the files specified as  files  in the  courselab  and on an instance of a particular VM  image . Each file has  localFile  and  destFile  attributes which specify what the file is called on the Tango server and what it should be called when copied over to a VM (for autograding) respectively. Exactly one of the specified  files  should have the  destFile  attribute set to  Makefile , and the Makefile must contain a rule called  autograde . Clients can also specify an optional timeout value ( timeout ) and maximum output file size ( max_kb ). This command is non-blocking and returns immediately with a status message. Additionally, the command accepts an optional parameter,  callback_url . If the  callback_url  is specified, then the Tango server sends a  POST  request to the  callback_url  with the output file once the job is terminated. If the  callback_url  is not specified, the client can then send a  poll  request for the  output_file  to check the status of that job and retrieve the output file from the Tango server if autograding is complete.  Request header:  POST /addJob/key/courselab/ \nRequest body:    {\n   image :  string ,                            # required VM image (e.g.  rhel.img )\n   files : [ {  localFile :  string , \n                destFile :  string  }, ...],    # required list of files to be used for autograding\n   jobName :  string ,                          # required name of job\n   output_file :  string ,                      # required name of output file\n   timeout :  int ,                             # optional timeout value (secs)\n   max_kb :  int ,                              # optional max output file size (KB)\n   callback_url :  string                       # optional URL for POST callback from server to client\n}  Response body:   { \n   statusMsg :  string , \n   statusId :  int , \n   jobId :  int  \n}", 
            "title": "addJob"
        }, 
        {
            "location": "/tango-rest/#poll", 
            "text": "Check if the job for  outputFile  has completed. If not, return  404: Not Found  and a status message, otherwise return the file in the response body, and free all resources held by the job.   Request header:  GET /poll/key/courselab/outputFile/ \nRequest body:   {\n   empty \n}  Response body:  autograder output file  if autograding successful otherwise:   {\n   statusMsg :  string , \n   statusId :  int  \n}", 
            "title": "poll"
        }, 
        {
            "location": "/tango-rest/#administrative-requests", 
            "text": "Here are the requests that administrators use to manage the Tango service, typically from a command line client.", 
            "title": "Administrative Requests"
        }, 
        {
            "location": "/tango-rest/#info", 
            "text": "This is the \"hello, world\" request for the service. It returns a JSON object with some basic stats about the service, such as uptime, number of jobs, etc.   Request header:  GET /info/ KEY / COURSE_LAB / \nRequest body:  {\n   empty \n}  Response body:  {\n   info : {\n             num_threads :  int , \n             job_requests :  int , \n             waitvm_timeouts :  int , \n             runjob_timeouts :  int , \n             elapsed_secs :  float , \n             runjob_errors :  int , \n             job_retries :  int , \n             copyin_errors :  int , \n             copyout_errors :  int \n          }, \n   statusMsg :  Found info successfully , \n   statusId : 0\n}", 
            "title": "/info"
        }, 
        {
            "location": "/tango-rest/#jobs", 
            "text": "Return a list of jobs. If deadjobs is set to 1, then return a list of recently completed jobs. Otherwise, return the list of currently running jobs. Note: This isn't strictly an admin request, since clients might find it useful to display jobs status, as we do in the Autolab front end.   Request header:  POST autograde.me/jobs/key/deadjobs/ \nRequest body: empty \nResponse body: JSON  jobs  object", 
            "title": "/jobs"
        }, 
        {
            "location": "/tango-rest/#pool", 
            "text": "Returns a JSON object that provides info about the current state of a pool of instances spawned from some  image . The response gives the total number of instances in the pool, and the number of free instances not currently allocated to any job.   Request header:  GET /pool/key/image/ \nResponse body: JSON  pool  object", 
            "title": "pool"
        }, 
        {
            "location": "/tango-rest/#prealloc", 
            "text": "Creates a pool of  num  identical instances spawned from  image  (e.g. \"rhel.img).   Request header:  POST /prealloc/key/image/num/ \nRequest body:  {\n     vmms :  string ,     # vmms to use (e.g.  localSSH )\n     cores :  int ,       # number of cores per VM\n     memory :  int ,      # amount of memory per VM\n}  Response body:  { \"status\":  string  }", 
            "title": "prealloc"
        }, 
        {
            "location": "/tango-rest/#implementation-notes", 
            "text": "Tango will maintain a directory for each of the labs in a course, which is created by  open . All output files are stored within a specified output folder in this directory. Besides the runtime job queue, no other state is necessary.  At job execution time, Tango will copy files specified by the  files  parameter in  addJob  to the VM. When the VM finishes, it will copy the output file back to the lab directory.", 
            "title": "Implementation Notes"
        }, 
        {
            "location": "/tango-vmms/", 
            "text": "This page documents the interface for Tango's Virtual Machine Management Systems' (VMMSs) API and instructions for setting up VMMSs. See \nthe vmms directory\n in Tango for example implementations. \n\n\nAPI\n\n\nThe functions necessary to implement the API are documented here. Note that for certain implementations, some of these methods will be no-ops since the VMMS doesn't require any particular instructions to perform the specified actions. Furthermore, throughout this document, we use the term \"VM\" liberally to represent any container-like object on which Tango jobs may be run. \n\n\ninitializeVM\n\n\ninitializeVM(self, vm)\n\n\n\n\nCreates a new VM instance for the VMMS based on the fields of \nvm\n, which is a \nTangoMachine\n object defined in \ntangoObjects.py\n.\n\n\nwaitVM\n\n\nwaitVM(self, vm, max_secs)\n\n\n\n\nWaits at most \nmax_secs\n for a VM to be ready to run jobs. Returns an error if the VM is not ready after \nmax_secs\n.\n\n\ncopyIn\n\n\ncopyIn(self, vm, inputFiles)\n\n\n\n\nCopies the input files for a job into the VM. \ninputFiles\n is a list of \nInputFile\n objects defined in \ntangoObjects.py\n. For each \nInputFile\n object, \nfile.localFile\n is the name of the file on the Tango host machine and \nfile.destFile\n is what the name of the file should be on the VM. \n\n\nrunJob\n\n\nrunJob(self, vm, runTimeout, maxOutputFileSize)\n\n\n\n\nRuns the autodriver binary on the VM. The autodriver runs \nmake\n on the VM (which in turn runs the job via the \nMakefile\n that was provided as a part of the input files for the job). The output from the autodriver most likely should be redirected to some feedback file to be used in the next method of the API. \n\n\ncopyOut\n\n\ncopyOut(self, vm, destFile)\n\n\n\n\nCopies the output file for the job out of the VM into \ndestFile\n on the Tango host machine. \n\n\ndestroyVM\n\n\ndestroyVM(self, vm)\n\n\n\n\nRemoves a VM from the Tango system. \n\n\nsafeDestroyVM\n\n\nsafeDestroyVM(self, vm)\n\n\n\n\nRemoves a VM from the Tango system and makes sure that it has been removed. \n\n\ngetVMs\n\n\ngetVMs(self)\n\n\n\n\nReturns a complete list of VMs associated with this Tango system. \n\n\nDocker VMMS Setup\n\n\nThis is a guide to set up Tango to run jobs inside Docker containers. \n\n\n\n\n\n\nInstall docker on host machine by following instructions on the \ndocker installation page\n. Ensure docker is running:\n\n\n$ docker ps\n# CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n\n\n\n\n\n\n\n\nBuild base Docker image from root Tango directory.\n\n\ncd path/to/Tango\ndocker build -t autograding_image vmms/\ndocker images autograding_image    # Check if image built\n\n\n\n\n\n\n\n\nUpdate \nVMMS_NAME\n in \nconfig.py\n.\n\n\n# in config.py\nVMMS_NAME = \nlocalDocker\n\n\n\n\n\n\n\n\nAmazon EC2 VMMS Setup\n\n\nThis is a guide to set up Tango to run jobs on an Amazon EC2 VM. \n\n\n\n\n\n\nCreate an \nAWS Account\n or use an existing one.\n\n\n\n\n\n\nObtain your \naccess_key_id\n and \nsecret_access_key\n by following the instructions \nhere\n.\n\n\n\n\n\n\nAdd AWS Credentials to a file called \n~/.boto\n using the following format:\n\n\n[Credentials]\naws_access_key_id = MYAMAZONTESTKEY12345\naws_secret_access_key = myawssecretaccesskey12345\n\n\n\n\nTango uses the \nBoto\n Python package to interface with Amazon Web Services\n\n\n\n\n\n\nIn the AWS EC2 console, create an Ubuntu 14.04+ EC2 instance and save the \n.pem\n file in a safe location. \n\n\n\n\n\n\nCopy the directory and contents of \nautodriver/\n in the Tango repo into the EC2 VM. For more help connecting to the EC2 instance follow \nthis guide\n\n\nchmod 400 /path/my-key-pair.pem\nscp -i /path/my-key-pair.pem -r autodriver/ ubuntu@\nec2-host-name\n.compute-1.amazonaws.com:~/\n\n\n\n\nThe autodriver is used as a sandbox environment to run the job inside the VM. It limits Disk I/O, Disk Usage, monitors security, and controls other valuable \nsudo\n level resources.\n\n\n\n\n\n\nIn the EC2 VM, compile the autodriver.\n\n\n$ cd autodriver/\n$ make clean; make\n$ cp -p autodriver /usr/bin/autodriver\n\n\n\n\n\n\n\n\nCreate the \nautograde\n Linux user and directory. All jobs will be run under this user.\n\n\n$ useradd autograde\n$ mkdir autograde\n$ chown autograde autograde\n$ chown :autograde autograde\n\n\n\n\n\n\n\n\nIn the AWS EC2 console, create an AMI image from your EC2 VM. Use \nthis guide\n to create a custom AMI.\n\n\n\n\n\n\nExit the EC2 instance and edit the following values in \nconfig.py\n in the Tango directory.\n\n\n# VMMS to use. Must be set to a VMMS implemented in vmms/ before\n# starting Tango.  Options are: \nlocalDocker\n, \ndistDocker\n,\n# \ntashiSSH\n, and \nec2SSH\n\nVMMS_NAME = \nec2SSH\n\n######\n# Part 5: EC2 Constants\n#\nEC2_REGION = 'us-east-1'             # EC2 Region\nEC2_USER_NAME = 'ubuntu'             # EC2 username\nDEFAULT_AMI = 'ami-4c99c35b'         # Custom AMI Id \nDEFAULT_INST_TYPE = 't2.micro'       # Instance Type\nDEFAULT_SECURITY_GROUP = 'autolab-autograde-ec2'  # Security Group with full access to EC2\nSECURITY_KEY_PATH = '/path/to/my-key-pair.pem'    # Absolute path to my-key-pair.pem\nDYNAMIC_SECURITY_KEY_PATH = ''       # Leave blank\nSECURITY_KEY_NAME = 'my-key-pair'    # Name of the key file. Ex: if file name is 'my-key-pair.pem', fill value with 'my-key-pair'\nTANGO_RESERVATION_ID = '1'           # Leave as 1\nINSTANCE_RUNNING = 16                # Status code of a running instance, leave as 16\n\n\n\n\n\n\n\n\nYou should now be ready to run Tango jobs on EC2! Use the \nTango CLI\n to test your setup.", 
            "title": "VMMS Docs"
        }, 
        {
            "location": "/tango-vmms/#api", 
            "text": "The functions necessary to implement the API are documented here. Note that for certain implementations, some of these methods will be no-ops since the VMMS doesn't require any particular instructions to perform the specified actions. Furthermore, throughout this document, we use the term \"VM\" liberally to represent any container-like object on which Tango jobs may be run.", 
            "title": "API"
        }, 
        {
            "location": "/tango-vmms/#initializevm", 
            "text": "initializeVM(self, vm)  Creates a new VM instance for the VMMS based on the fields of  vm , which is a  TangoMachine  object defined in  tangoObjects.py .", 
            "title": "initializeVM"
        }, 
        {
            "location": "/tango-vmms/#waitvm", 
            "text": "waitVM(self, vm, max_secs)  Waits at most  max_secs  for a VM to be ready to run jobs. Returns an error if the VM is not ready after  max_secs .", 
            "title": "waitVM"
        }, 
        {
            "location": "/tango-vmms/#copyin", 
            "text": "copyIn(self, vm, inputFiles)  Copies the input files for a job into the VM.  inputFiles  is a list of  InputFile  objects defined in  tangoObjects.py . For each  InputFile  object,  file.localFile  is the name of the file on the Tango host machine and  file.destFile  is what the name of the file should be on the VM.", 
            "title": "copyIn"
        }, 
        {
            "location": "/tango-vmms/#runjob", 
            "text": "runJob(self, vm, runTimeout, maxOutputFileSize)  Runs the autodriver binary on the VM. The autodriver runs  make  on the VM (which in turn runs the job via the  Makefile  that was provided as a part of the input files for the job). The output from the autodriver most likely should be redirected to some feedback file to be used in the next method of the API.", 
            "title": "runJob"
        }, 
        {
            "location": "/tango-vmms/#copyout", 
            "text": "copyOut(self, vm, destFile)  Copies the output file for the job out of the VM into  destFile  on the Tango host machine.", 
            "title": "copyOut"
        }, 
        {
            "location": "/tango-vmms/#destroyvm", 
            "text": "destroyVM(self, vm)  Removes a VM from the Tango system.", 
            "title": "destroyVM"
        }, 
        {
            "location": "/tango-vmms/#safedestroyvm", 
            "text": "safeDestroyVM(self, vm)  Removes a VM from the Tango system and makes sure that it has been removed.", 
            "title": "safeDestroyVM"
        }, 
        {
            "location": "/tango-vmms/#getvms", 
            "text": "getVMs(self)  Returns a complete list of VMs associated with this Tango system.", 
            "title": "getVMs"
        }, 
        {
            "location": "/tango-vmms/#docker-vmms-setup", 
            "text": "This is a guide to set up Tango to run jobs inside Docker containers.     Install docker on host machine by following instructions on the  docker installation page . Ensure docker is running:  $ docker ps\n# CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES    Build base Docker image from root Tango directory.  cd path/to/Tango\ndocker build -t autograding_image vmms/\ndocker images autograding_image    # Check if image built    Update  VMMS_NAME  in  config.py .  # in config.py\nVMMS_NAME =  localDocker", 
            "title": "Docker VMMS Setup"
        }, 
        {
            "location": "/tango-vmms/#amazon-ec2-vmms-setup", 
            "text": "This is a guide to set up Tango to run jobs on an Amazon EC2 VM.     Create an  AWS Account  or use an existing one.    Obtain your  access_key_id  and  secret_access_key  by following the instructions  here .    Add AWS Credentials to a file called  ~/.boto  using the following format:  [Credentials]\naws_access_key_id = MYAMAZONTESTKEY12345\naws_secret_access_key = myawssecretaccesskey12345  Tango uses the  Boto  Python package to interface with Amazon Web Services    In the AWS EC2 console, create an Ubuntu 14.04+ EC2 instance and save the  .pem  file in a safe location.     Copy the directory and contents of  autodriver/  in the Tango repo into the EC2 VM. For more help connecting to the EC2 instance follow  this guide  chmod 400 /path/my-key-pair.pem\nscp -i /path/my-key-pair.pem -r autodriver/ ubuntu@ ec2-host-name .compute-1.amazonaws.com:~/  The autodriver is used as a sandbox environment to run the job inside the VM. It limits Disk I/O, Disk Usage, monitors security, and controls other valuable  sudo  level resources.    In the EC2 VM, compile the autodriver.  $ cd autodriver/\n$ make clean; make\n$ cp -p autodriver /usr/bin/autodriver    Create the  autograde  Linux user and directory. All jobs will be run under this user.  $ useradd autograde\n$ mkdir autograde\n$ chown autograde autograde\n$ chown :autograde autograde    In the AWS EC2 console, create an AMI image from your EC2 VM. Use  this guide  to create a custom AMI.    Exit the EC2 instance and edit the following values in  config.py  in the Tango directory.  # VMMS to use. Must be set to a VMMS implemented in vmms/ before\n# starting Tango.  Options are:  localDocker ,  distDocker ,\n#  tashiSSH , and  ec2SSH \nVMMS_NAME =  ec2SSH \n######\n# Part 5: EC2 Constants\n#\nEC2_REGION = 'us-east-1'             # EC2 Region\nEC2_USER_NAME = 'ubuntu'             # EC2 username\nDEFAULT_AMI = 'ami-4c99c35b'         # Custom AMI Id \nDEFAULT_INST_TYPE = 't2.micro'       # Instance Type\nDEFAULT_SECURITY_GROUP = 'autolab-autograde-ec2'  # Security Group with full access to EC2\nSECURITY_KEY_PATH = '/path/to/my-key-pair.pem'    # Absolute path to my-key-pair.pem\nDYNAMIC_SECURITY_KEY_PATH = ''       # Leave blank\nSECURITY_KEY_NAME = 'my-key-pair'    # Name of the key file. Ex: if file name is 'my-key-pair.pem', fill value with 'my-key-pair'\nTANGO_RESERVATION_ID = '1'           # Leave as 1\nINSTANCE_RUNNING = 16                # Status code of a running instance, leave as 16    You should now be ready to run Tango jobs on EC2! Use the  Tango CLI  to test your setup.", 
            "title": "Amazon EC2 VMMS Setup"
        }, 
        {
            "location": "/tango-deploy/", 
            "text": "Deploying Standalone Tango\n\n\nThis is a guide to setup a fully self-sufficient Tango deployment environment out-of-the-box using Docker. The suggested deployment pattern for Tango uses Nginx as a proxy and Supervisor as a process manager for Tango and all its dependencies. All requests to Nginx are rerouted to a Tango process. \n\n\nDetails\n\n\n\n\nNginx default port - 8600\n\n\nTango ports - 8610, 8611\n\n\nRedis port - 6379\n\n\nYou can change any of these in the respective config files in \ndeployment/config/\n before you build the \ntango_deployment\n image.\n\n\n\n\nSteps\n\n\n\n\n\n\nClone the Tango repo\n\n\n$ git clone https://github.com/autolab/Tango.git; cd Tango\n\n\n\n\n\n\n\n\nCreate a \nconfig.py\n file from the given template.  \n\n\n$ cp config.template.py config.py\n\n\n\n\n\n\n\n\nInstall docker on the host machine by following instructions on the \ndocker installation page\n. Ensure docker is running:\n\n\n$ docker ps\n# CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                    NAMES\n\n\n\n\n\n\n\n\nRun the following command to build the Tango deployment image. \n\n\n$ docker build --tag=\ntango_deployment\n .\n\n\n\n\n\n\n\n\nEnsure the image was built by running.\n\n\n$ docker images\n# REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n# tango_deployment    latest              3c0d4f4b4958        2 minutes ago       742.6 MB\n# ubuntu              15.04               d1b55fd07600        4 minutes ago       131.3 MB\n\n\n\n\n\n\n\n\nRun the following command to access the image in a container with a bash shell. The \n-p\n flag will map \nnginxPort\n on the docker container to \nlocalPort\n (8610 recommended) on your local machine (or on the VM that docker is running in on the local machine) so that Tango is accessible from outside the docker container.\n\n\n$ docker run --privileged -p \nlocalPort\n:\nnginxPort\n -it tango_deployment /bin/bash\n\n\n\n\n\n\n\n\nSet up a VMMS for Tango within the Docker container.\n\n\n\n\nDocker\n (\nrecommended\n)\n\n\nAmazon EC2\n\n\n\n\n\n\n\n\nRun the following command to start supervisor, which will then start Tango and all its dependencies.\n\n\n$ service supervisor start\n\n\n\n\n\n\n\n\nCheck to see if Tango is responding to requests\n\n\n$ curl localhost:8610\n# Hello, world! RESTful Tango here!\n\n\n\n\n\n\n\n\nOnce you have a VMMS set up, leave the tango_deployment container by typing \nexit\n and once back in the host shell run the following command to get the name of your production container.\n\n\n$ docker ps -as\n# CONTAINER ID        IMAGE               COMMAND               NAMES               SIZE\n# c704d45c3737    tango_deployment       \n/bin/bash\n            erwin             40.26 MB\n\nThe container created in this example has the name `erwin`.\n\n\n\n\n\n\n\n\nThe name of the production container can be changed by running the following command and will be used to run the container and create services.\n\n\n$ docker rename \nold_name\n \nnew_name\n\n\n\n\n\n\n\n\nTo reopen the container once it has been built use the following command. This will reopen the interactive shell within the container and allow for configuration of the container after its initial run.\n\n\n$ docker start erwin\n$ docker attach erwin\n\n\n\n\n\n\n\n\nOnce the container is set up with the autograding image, and the VMMS configured with any necessary software/environments needed for autograding (java, perl, etc), some configurations need to be changed to make the container daemon ready. Using the \nCONTAINER ID\n above, use the following commands to modify that containers \nconfig.v2.json\n file.\n\n\n$ sudo ls  /var/lib/docker/containers\nc704d45c37372a034cb97761d99f6f3f362707cc23d689734895e017eda3e55b\n$ sudo vim /var/lib/docker/containers/c704d45c37372a034cb97761d99f6f3f362707cc23d689734895e017eda3e55b/config.v2.json\n\n\n\n\n\n\n\n\nEdit the \"Path\" field in the config.v2.json file from \"/bin/bash\" to \"/usr/bin/supervisord\" and save the file. Run the following commands to verify the changes were successful. The COMMAND field should now be \"/usr/bin/supervisord\" \n\n\n$ service docker restart\n$ docker ps -as\n# CONTAINER ID        IMAGE               COMMAND               NAMES               SIZE\n# c704d45c3737    tango_deployment   \n/usr/bin/supervisord\n     erwin             40.26 MB\n\n\n\n\n\n\n\n\nAt this point when the container is started, the environment is fully set up and will no longer be an interactive shell. Instead, it will be the supervisor service that starts Tango and all its dependencies. Test this with the following commands and ensure Tango is functioning properly.\n\n\n$ docker start erwin\n# (Test tango environment)\n$ docker stop erwin\n\n\n\n\n\n\n\n\nTest the setup by running sample jobs using \nthe testing guide\n.\n\n\n\n\n\n\nThe following steps are optional and should only be used if you would like the Tango container to start on system boot.\n\n\n\n\n\n\nTo ensure Tango starts with the system in the production environment, the container needs to be configured as a service. Below is a sample service config file that needs to be changed to suit your environment and placed in \n/etc/systemd/system/\n. The file should be named \nname\n.service\n. For this example, it is \nerwin.service\n.\n\n\n[Unit]\nDescription=Docker Service Managing Tango Container\nRequires=docker.service\nAfter=docker.service\n\n[Service]\nRestart=always\nExecStart=/usr/bin/docker start -a erwin\nExecStop=/usr/bin/docker stop -t 2 erwin\n\n[Install]\nWantedBy=default.target\n\n\n\n\n\n\n\n\nTest and ensure the service was set up correctly. The service should start successfully and remain running.\n\n\n$ systemctl daemon-reload\n$ service erwin start\n$ service erwin status\n\n\n\n\n\n\n\n\nEnable the service at system startup and reboot and ensure it starts with the host.\n\n\n$ systemctl enable erwin.service\n$ sudo reboot\n# (Server Reboots)\n$ service erwin status", 
            "title": "Deploying Tango"
        }, 
        {
            "location": "/tango-deploy/#deploying-standalone-tango", 
            "text": "This is a guide to setup a fully self-sufficient Tango deployment environment out-of-the-box using Docker. The suggested deployment pattern for Tango uses Nginx as a proxy and Supervisor as a process manager for Tango and all its dependencies. All requests to Nginx are rerouted to a Tango process.", 
            "title": "Deploying Standalone Tango"
        }, 
        {
            "location": "/tango-deploy/#details", 
            "text": "Nginx default port - 8600  Tango ports - 8610, 8611  Redis port - 6379  You can change any of these in the respective config files in  deployment/config/  before you build the  tango_deployment  image.", 
            "title": "Details"
        }, 
        {
            "location": "/tango-deploy/#steps", 
            "text": "Clone the Tango repo  $ git clone https://github.com/autolab/Tango.git; cd Tango    Create a  config.py  file from the given template.    $ cp config.template.py config.py    Install docker on the host machine by following instructions on the  docker installation page . Ensure docker is running:  $ docker ps\n# CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                    NAMES    Run the following command to build the Tango deployment image.   $ docker build --tag= tango_deployment  .    Ensure the image was built by running.  $ docker images\n# REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n# tango_deployment    latest              3c0d4f4b4958        2 minutes ago       742.6 MB\n# ubuntu              15.04               d1b55fd07600        4 minutes ago       131.3 MB    Run the following command to access the image in a container with a bash shell. The  -p  flag will map  nginxPort  on the docker container to  localPort  (8610 recommended) on your local machine (or on the VM that docker is running in on the local machine) so that Tango is accessible from outside the docker container.  $ docker run --privileged -p  localPort : nginxPort  -it tango_deployment /bin/bash    Set up a VMMS for Tango within the Docker container.   Docker  ( recommended )  Amazon EC2     Run the following command to start supervisor, which will then start Tango and all its dependencies.  $ service supervisor start    Check to see if Tango is responding to requests  $ curl localhost:8610\n# Hello, world! RESTful Tango here!    Once you have a VMMS set up, leave the tango_deployment container by typing  exit  and once back in the host shell run the following command to get the name of your production container.  $ docker ps -as\n# CONTAINER ID        IMAGE               COMMAND               NAMES               SIZE\n# c704d45c3737    tango_deployment        /bin/bash             erwin             40.26 MB\n\nThe container created in this example has the name `erwin`.    The name of the production container can be changed by running the following command and will be used to run the container and create services.  $ docker rename  old_name   new_name    To reopen the container once it has been built use the following command. This will reopen the interactive shell within the container and allow for configuration of the container after its initial run.  $ docker start erwin\n$ docker attach erwin    Once the container is set up with the autograding image, and the VMMS configured with any necessary software/environments needed for autograding (java, perl, etc), some configurations need to be changed to make the container daemon ready. Using the  CONTAINER ID  above, use the following commands to modify that containers  config.v2.json  file.  $ sudo ls  /var/lib/docker/containers\nc704d45c37372a034cb97761d99f6f3f362707cc23d689734895e017eda3e55b\n$ sudo vim /var/lib/docker/containers/c704d45c37372a034cb97761d99f6f3f362707cc23d689734895e017eda3e55b/config.v2.json    Edit the \"Path\" field in the config.v2.json file from \"/bin/bash\" to \"/usr/bin/supervisord\" and save the file. Run the following commands to verify the changes were successful. The COMMAND field should now be \"/usr/bin/supervisord\"   $ service docker restart\n$ docker ps -as\n# CONTAINER ID        IMAGE               COMMAND               NAMES               SIZE\n# c704d45c3737    tango_deployment    /usr/bin/supervisord      erwin             40.26 MB    At this point when the container is started, the environment is fully set up and will no longer be an interactive shell. Instead, it will be the supervisor service that starts Tango and all its dependencies. Test this with the following commands and ensure Tango is functioning properly.  $ docker start erwin\n# (Test tango environment)\n$ docker stop erwin    Test the setup by running sample jobs using  the testing guide .    The following steps are optional and should only be used if you would like the Tango container to start on system boot.    To ensure Tango starts with the system in the production environment, the container needs to be configured as a service. Below is a sample service config file that needs to be changed to suit your environment and placed in  /etc/systemd/system/ . The file should be named  name .service . For this example, it is  erwin.service .  [Unit]\nDescription=Docker Service Managing Tango Container\nRequires=docker.service\nAfter=docker.service\n\n[Service]\nRestart=always\nExecStart=/usr/bin/docker start -a erwin\nExecStop=/usr/bin/docker stop -t 2 erwin\n\n[Install]\nWantedBy=default.target    Test and ensure the service was set up correctly. The service should start successfully and remain running.  $ systemctl daemon-reload\n$ service erwin start\n$ service erwin status    Enable the service at system startup and reboot and ensure it starts with the host.  $ systemctl enable erwin.service\n$ sudo reboot\n# (Server Reboots)\n$ service erwin status", 
            "title": "Steps"
        }, 
        {
            "location": "/one-click/", 
            "text": "Autolab + Tango OneClick Installation\n\n\nOneClick is the fastest way to install Autolab and Tango on an Ubuntu VM. The installation uses packages Autolab, MySQL, and Tango into seperate Docker containers with specific exposed ports for communication.\n\n\nThere are two types of installations. A local development setup and a real-world ready setup that requires SSL certificates, email service configuration, and domain name registration. Use the local setup for experimentation before deploying in a real-world scenario on such apps like Heroku, EC2, or DigitalOcean, among others.\n\n\nLocal OneClick Setup\n\n\n1. Prepare an Ubuntu VM\n\n\nThese installation instructions are for Ubuntu. If you're on other operating system, we recommend you set up an Ubuntu virtual machine first with \nVirtual Box\n.\n\n\nAbout the System Configuration:\n\n\n\n\nUbuntu 14.04( or higher) 64bit\n\n\n2GB memory + 20GB disk\n\n\n\n\nTo set up, \nInstall Ubuntu on Virtualbox\n may help you.\n\n\nOptional:\n\n\nFor better experience, we also recommend you to \"insert guest additional CD image\" for your virtual machine to enable full screen. \n(If you installed Ubuntu 16+, you can skip this)\n\n\nDevices \n Insert guest additional CD image\n\n\n\n\nAlso enable clipboard share for easier copy and paste between host and VM.\n\n\nSettings \n Advanced \n Shared Clipboard \n Bidrectional\n\n\n\n\nYou need to restart your virtual machine to validate these optional changes.\n\n\n2. Download\n\n\nRoot is required to install Autolab:\n\n\nsudo -i\n\n\n\n\nClone repo:\n\n\ngit clone https://github.com/autolab/autolab-oneclick.git; cd autolab-oneclick\n\n\n\n\n3. Installation\n\n\nRun the following in the autolab-oneclick folder\n\n\n./install.sh -l\n\n\n\n\nThis will take a few minutes. Once you see \nAutolab Installation Finished\n, ensure all docker containers are running:\n\n\ndocker ps\n# CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                    PORTS                     NAMES\n# c8679844bbfa        local_web           \n/sbin/my_init\n          3 months ago        Exited (0) 3 months ago                             local_web_1         721 kB (virtual 821 MB)\n# 45a9e30241ea        mysql               \ndocker-entrypoint...\n   3 months ago        Exited (0) 3 months ago   0.0.0.0:32768-\n3306/tcp   local_db_1          0 B (virtual 383 MB)\n# 1ef089e2dca4        local_tango         \nsh start.sh\n            3 months ago        Exited (0) 3 months ago   0.0.0.0:8600-\n8600/tcp    local_tango_1       91.1 kB (virtual 743 MB)\n\n\n\n\nNow Autolab is successfully installed and running on your virtual machine.\nOpen your browser and visit \nlocalhost:3000\n, you will see the landing page of Autolab.\n\n\nFollow the instructions \nhere\n to test out your set up.\n\n\nServer/Production OneClick Setup\n\n\n1. Provision a Server\n\n\nServer\n\n\nIf you don't already have a server, we recommend a VPS (virtual private server). Here are a couple popular VPS providers:\n\n\n\n\nDigitalOcean\n (recommended)\n\n\nAmazon Lightsail\n\n\nGoogle Cloud Platform\n\n\n\n\nDomain name\n\n\n(A domain name is both required by SSL and email service.)\n\n\nIn your DNS provider:\n\n\n\n\nAdd www and @ records pointing to the ip address of your server.\n\n\nAdd DKIM and SFF records by creating TXT records after you finish the email service part.\n\n\n\n\nSSL\n\n\nYou can run Autolab with or without HTTPS encryption. We strongly recommend you run it with HTTPS.\n\n\nHere are a few options to get the SSL certificate and key:\n\n\n\n\n\n\nGo through your school/organization\n\n\nMany universities have a program whereby they'll grant SSL certificates to students and faculty for free. Some of these programs require you to be using a school-related domain name, but some don't. You should be able to find out more information from your school's IT department.\n\n\n\n\n\n\nUse paid service: SSLmate\n\n\nYou can follow this \nsimple guide\n to get your paid SSL with \nSSLMate\n in the simplest way.\n\n\n\n\n\n\nEmail Service\n\n\nAutolab uses email for various features, include sending out user confirmation emails and instructor-to-student bulk emails. You can use MailChimp + Mandrill to configure transactional email. \n\n\n\n\n\n\nCreate a MailChimp account \nhere\n\n\n\n\n\n\nAdd Mandrill using \nthese instructions\n\n\n\n\n\n\nGo to the settings page and create a new API key\n\n\n\n\n\n\nFrom the Mailchimp/Mandrill Domains settings page, add your domain \n\n\n\n\n\n\nConfigure the DKIM and SFF settings by creating TXT records with your DNS provider (they link to some instructions for how to do this, but the process will differ depending on which DNS provider you are using. Try Google!).\n\n\n\n\n\n\n2. Download and Configuration\n\n\n\n\n\n\nUse root to install Autolab\n\n\nsudo -i\n\n\n\n\n\n\n\n\nClone the installation package\n\n\ngit clone https://github.com/autolab/autolab-oneclick.git; cd autolab-oneclick\n\n\n\n\n\n\n\n\nGenerate a new secret key for Devise Auth Configuration:\n\n\npython -c \nimport random; print hex(random.getrandbits(512))[2:-1]\n\n\n\n\nUpdate the values in \nserver/configs/devise.rb\n\n\nconfig.secret_key = \nGENERATED_SECRET_KEY\n\nconfig.mailer_sender = \nEMAIL_ADDRESS_WITH_YOUR_HOSTNAME\n\n\n\n\n\n\n\n\nCopy your SSL certificate and key file into the \nserver/ssl\n directory.\n\n\n\n\n\n\nConfigure Nginx in \nserver/configs/nginx.conf\n\n\nserver_name \nYOUR_SERVER_DOMAIN\n\nssl_certificate /path/to/ssl_certificate/file\nssl_certificate_key /path/to/ssl_certificate_key/file\n\n\n\n\n\n\n\n\nConfigure Email in \nserver/configs/production.rb\n. Update the address, port, user_name, password and domain with your email service informations. For Mandrill, go to \"SMTP \n API Info\" to see the informations.\n\n\n\n\n\n\n3. Installation\n\n\n\n\n\n\nStart Installation\n\n\ncd autolab-oneclick\n./install.sh -s\n\n\n\n\nAnswer the prompts and wait until you see \nAutolab Installation Finished\n.\n\n\n\n\n\n\nEnsure docker containers are running\n\n\ndocker ps\n# CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                    PORTS                     NAMES\n# c8679844bbfa        local_web           \n/sbin/my_init\n          3 months ago        Exited (0) 3 months ago                             local_web_1         721 kB (virtual 821 MB)\n# 45a9e30241ea        mysql               \ndocker-entrypoint...\n   3 months ago        Exited (0) 3 months ago   0.0.0.0:32768-\n3306/tcp   local_db_1          0 B (virtual 383 MB)\n# 1ef089e2dca4        local_tango         \nsh start.sh\n            3 months ago        Exited (0) 3 months ago   0.0.0.0:8600-\n8600/tcp    local_tango_1       91.1 kB (virtual 743 MB)\n\n\n\n\n\n\n\n\nNow Autolab is successfully installed and running on your virtual machine.\nOpen your browser and visit \nhttps://yourdomainname\n, to see the landing page of Autolab.\n\n\nFollow the instructions \nhere\n to test out your set up.\n\n\nTesting\n\n\nLogin with the following credentials:\n\n\nemail: admin@foo.bar\npassword: adminfoobar\n\n\n\n\nWe have populated dummy data for you to test with.\n\n\nRun the following commands to cleanup the dummy data:\n\n\ncd local\ndocker-compose run --rm -e RAILS_ENV=production web rake autolab:depopulate", 
            "title": "OneClick Install"
        }, 
        {
            "location": "/one-click/#autolab-tango-oneclick-installation", 
            "text": "OneClick is the fastest way to install Autolab and Tango on an Ubuntu VM. The installation uses packages Autolab, MySQL, and Tango into seperate Docker containers with specific exposed ports for communication.  There are two types of installations. A local development setup and a real-world ready setup that requires SSL certificates, email service configuration, and domain name registration. Use the local setup for experimentation before deploying in a real-world scenario on such apps like Heroku, EC2, or DigitalOcean, among others.", 
            "title": "Autolab + Tango OneClick Installation"
        }, 
        {
            "location": "/one-click/#local-oneclick-setup", 
            "text": "", 
            "title": "Local OneClick Setup"
        }, 
        {
            "location": "/one-click/#1-prepare-an-ubuntu-vm", 
            "text": "These installation instructions are for Ubuntu. If you're on other operating system, we recommend you set up an Ubuntu virtual machine first with  Virtual Box .  About the System Configuration:   Ubuntu 14.04( or higher) 64bit  2GB memory + 20GB disk   To set up,  Install Ubuntu on Virtualbox  may help you.  Optional:  For better experience, we also recommend you to \"insert guest additional CD image\" for your virtual machine to enable full screen. \n(If you installed Ubuntu 16+, you can skip this)  Devices   Insert guest additional CD image  Also enable clipboard share for easier copy and paste between host and VM.  Settings   Advanced   Shared Clipboard   Bidrectional  You need to restart your virtual machine to validate these optional changes.", 
            "title": "1. Prepare an Ubuntu VM"
        }, 
        {
            "location": "/one-click/#2-download", 
            "text": "Root is required to install Autolab:  sudo -i  Clone repo:  git clone https://github.com/autolab/autolab-oneclick.git; cd autolab-oneclick", 
            "title": "2. Download"
        }, 
        {
            "location": "/one-click/#3-installation", 
            "text": "Run the following in the autolab-oneclick folder  ./install.sh -l  This will take a few minutes. Once you see  Autolab Installation Finished , ensure all docker containers are running:  docker ps\n# CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                    PORTS                     NAMES\n# c8679844bbfa        local_web            /sbin/my_init           3 months ago        Exited (0) 3 months ago                             local_web_1         721 kB (virtual 821 MB)\n# 45a9e30241ea        mysql                docker-entrypoint...    3 months ago        Exited (0) 3 months ago   0.0.0.0:32768- 3306/tcp   local_db_1          0 B (virtual 383 MB)\n# 1ef089e2dca4        local_tango          sh start.sh             3 months ago        Exited (0) 3 months ago   0.0.0.0:8600- 8600/tcp    local_tango_1       91.1 kB (virtual 743 MB)  Now Autolab is successfully installed and running on your virtual machine.\nOpen your browser and visit  localhost:3000 , you will see the landing page of Autolab.  Follow the instructions  here  to test out your set up.", 
            "title": "3. Installation"
        }, 
        {
            "location": "/one-click/#serverproduction-oneclick-setup", 
            "text": "", 
            "title": "Server/Production OneClick Setup"
        }, 
        {
            "location": "/one-click/#1-provision-a-server", 
            "text": "Server  If you don't already have a server, we recommend a VPS (virtual private server). Here are a couple popular VPS providers:   DigitalOcean  (recommended)  Amazon Lightsail  Google Cloud Platform   Domain name  (A domain name is both required by SSL and email service.)  In your DNS provider:   Add www and @ records pointing to the ip address of your server.  Add DKIM and SFF records by creating TXT records after you finish the email service part.   SSL  You can run Autolab with or without HTTPS encryption. We strongly recommend you run it with HTTPS.  Here are a few options to get the SSL certificate and key:    Go through your school/organization  Many universities have a program whereby they'll grant SSL certificates to students and faculty for free. Some of these programs require you to be using a school-related domain name, but some don't. You should be able to find out more information from your school's IT department.    Use paid service: SSLmate  You can follow this  simple guide  to get your paid SSL with  SSLMate  in the simplest way.", 
            "title": "1. Provision a Server"
        }, 
        {
            "location": "/one-click/#email-service", 
            "text": "Autolab uses email for various features, include sending out user confirmation emails and instructor-to-student bulk emails. You can use MailChimp + Mandrill to configure transactional email.     Create a MailChimp account  here    Add Mandrill using  these instructions    Go to the settings page and create a new API key    From the Mailchimp/Mandrill Domains settings page, add your domain     Configure the DKIM and SFF settings by creating TXT records with your DNS provider (they link to some instructions for how to do this, but the process will differ depending on which DNS provider you are using. Try Google!).", 
            "title": "Email Service"
        }, 
        {
            "location": "/one-click/#2-download-and-configuration", 
            "text": "Use root to install Autolab  sudo -i    Clone the installation package  git clone https://github.com/autolab/autolab-oneclick.git; cd autolab-oneclick    Generate a new secret key for Devise Auth Configuration:  python -c  import random; print hex(random.getrandbits(512))[2:-1]  Update the values in  server/configs/devise.rb  config.secret_key =  GENERATED_SECRET_KEY \nconfig.mailer_sender =  EMAIL_ADDRESS_WITH_YOUR_HOSTNAME    Copy your SSL certificate and key file into the  server/ssl  directory.    Configure Nginx in  server/configs/nginx.conf  server_name  YOUR_SERVER_DOMAIN \nssl_certificate /path/to/ssl_certificate/file\nssl_certificate_key /path/to/ssl_certificate_key/file    Configure Email in  server/configs/production.rb . Update the address, port, user_name, password and domain with your email service informations. For Mandrill, go to \"SMTP   API Info\" to see the informations.", 
            "title": "2. Download and Configuration"
        }, 
        {
            "location": "/one-click/#3-installation_1", 
            "text": "Start Installation  cd autolab-oneclick\n./install.sh -s  Answer the prompts and wait until you see  Autolab Installation Finished .    Ensure docker containers are running  docker ps\n# CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                    PORTS                     NAMES\n# c8679844bbfa        local_web            /sbin/my_init           3 months ago        Exited (0) 3 months ago                             local_web_1         721 kB (virtual 821 MB)\n# 45a9e30241ea        mysql                docker-entrypoint...    3 months ago        Exited (0) 3 months ago   0.0.0.0:32768- 3306/tcp   local_db_1          0 B (virtual 383 MB)\n# 1ef089e2dca4        local_tango          sh start.sh             3 months ago        Exited (0) 3 months ago   0.0.0.0:8600- 8600/tcp    local_tango_1       91.1 kB (virtual 743 MB)    Now Autolab is successfully installed and running on your virtual machine.\nOpen your browser and visit  https://yourdomainname , to see the landing page of Autolab.  Follow the instructions  here  to test out your set up.", 
            "title": "3. Installation"
        }, 
        {
            "location": "/one-click/#testing", 
            "text": "Login with the following credentials:  email: admin@foo.bar\npassword: adminfoobar  We have populated dummy data for you to test with.  Run the following commands to cleanup the dummy data:  cd local\ndocker-compose run --rm -e RAILS_ENV=production web rake autolab:depopulate", 
            "title": "Testing"
        }
    ]
}